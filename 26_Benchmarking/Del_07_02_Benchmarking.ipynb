{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viri: \n",
    "- [Python 101: An Intro to Benchmarking Your Code](https://dzone.com/articles/python-101-an-intro-to-benchmarking-your-code)\n",
    "- [Profiling Python using cProfile: a concrete case](https://julien.danjou.info/guide-to-python-profiling-cprofile-concrete-case-carbonara/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean to benchmark one's code? The main idea behind benchmarking or profiling is to figure out how fast your code executes and where the bottlenecks are. The main reason to do this sort of thing is for optimization. You will run into situations where you need your code to run faster because your business needs have changed. When this happens, you will need to figure out what parts of your code are slowing it down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Your Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unix time command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you simply want to time your whole program, it’s usually easy enough to use something\n",
    "like the Unix time command. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    time python3 someprogram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#someprogram.py\n",
    "import math\n",
    "\n",
    "def f1(degrees):\n",
    "    return math.cos(degrees)\n",
    "\n",
    "def f2(degrees):\n",
    "    e = 2.718281828459045\n",
    "    return ((e**(degrees * 1j) + e**-(degrees * 1j)) / 2).real\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Starting')\n",
    "    for i in range(500000): \n",
    "        f1(i)\n",
    "        f2(i)\n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"post-text\" itemprop=\"text\">\n",
    "        <p><strong>Real, User and Sys process time statistics</strong></p>\n",
    "\n",
    "<p>One of these things is not like the other.  Real refers to actual elapsed time; User and Sys refer to CPU time used <em>only by the process.</em></p>\n",
    "\n",
    "<ul>\n",
    "<li><p><strong>Real</strong> is wall clock time - time from start to finish of the call.  This is all elapsed time including time slices used by other processes and time the process spends blocked (for example if it is waiting for I/O to complete).</p></li>\n",
    "<li><p><strong>User</strong> is the amount of CPU time spent in user-mode code (outside the kernel) <em>within</em> the process.  This is only actual CPU time used in executing the process.  Other processes and time the process spends blocked do not count towards this figure.</p></li>\n",
    "<li><p><strong>Sys</strong> is the amount of CPU time spent in the kernel within the process.  This means executing CPU time spent in system calls <em>within the kernel,</em> as opposed to library code, which is still running in user-space.  Like 'user', this is only CPU time used by the process.  See below for a brief description of kernel mode (also known as 'supervisor' mode) and the system call mechanism.</p></li>\n",
    "</ul>\n",
    "\n",
    "<p><code>User+Sys</code> will tell you how much actual CPU time your process used.  Note that this is across all CPUs, so if the process has multiple threads (and this process is running on a computer with more than one processor) it could potentially exceed the wall clock time reported by <code>Real</code> (which usually occurs).  Note that in the output these figures include the <code>User</code> and <code>Sys</code> time of all child processes (and their descendants) as well when they could have been collected, e.g. by <code>wait(2)</code> or <code>waitpid(2)</code>, although the underlying system calls return the statistics for the process and its children separately.</p>\n",
    "\n",
    "<p><strong>Origins of the statistics reported by <code>time (1)</code></strong></p>\n",
    "\n",
    "<p>The statistics reported by <code>time</code> are gathered from various system calls.  'User' and 'Sys' come from <a href=\"https://docs.oracle.com/cd/E23823_01/html/816-5168/wait-3c.html#scrolltoc\" rel=\"noreferrer\"><code>wait (2)</code></a> (<a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/functions/wait.html\" rel=\"noreferrer\">POSIX</a>) or <a href=\"https://linux.die.net/man/2/times\" rel=\"noreferrer\"><code>times (2)</code></a> (<a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/functions/times.html\" rel=\"noreferrer\">POSIX</a>), depending on the particular system.  'Real' is calculated from a start and end time gathered from the <a href=\"http://pubs.opengroup.org/onlinepubs/9699919799/functions/gettimeofday.html\" rel=\"noreferrer\"><code>gettimeofday (2)</code></a> call.  Depending on the version of the system, various other statistics such as the number of context switches may also be gathered by <code>time</code>.</p>\n",
    "\n",
    "<p>On a multi-processor machine, a multi-threaded process or a process forking children could have an elapsed time smaller than the total CPU time - as different threads or processes may run in parallel.  Also, the time statistics reported come from different origins, so times recorded for very short running tasks may be subject to rounding errors, as the example given by the original poster shows.</p>\n",
    "\n",
    "<p><strong>A brief primer on Kernel vs. User mode</strong></p>\n",
    "\n",
    "<p>On Unix, or any protected-memory operating system, <a href=\"https://en.wikipedia.org/wiki/Kernel_mode#Supervisor_mode\" rel=\"noreferrer\">'Kernel' or 'Supervisor'</a> mode refers to a <a href=\"https://en.wikipedia.org/wiki/Process_management_(computing)#Processor_modes\" rel=\"noreferrer\">privileged mode</a> that the CPU can operate in.  Certain privileged actions that could affect security or stability can only be done when the CPU is operating in this mode; these actions are not available to application code.  An example of such an action might be manipulation of the <a href=\"https://en.wikipedia.org/wiki/Memory_management_unit\" rel=\"noreferrer\">MMU</a> to gain access to the address space of another process.  Normally, <a href=\"https://en.wikipedia.org/wiki/User_space\" rel=\"noreferrer\">user-mode</a> code cannot do this (with good reason), although it can request <a href=\"https://en.wikipedia.org/wiki/Shared_memory\" rel=\"noreferrer\">shared memory</a> from the kernel, which <em>could</em> be read or written by more than one process.  In this case, the shared memory is explicitly requested from the kernel through a secure mechanism and both processes have to explicitly attach to it in order to use it.</p>\n",
    "\n",
    "<p>The privileged mode is usually referred to as 'kernel' mode because the kernel is executed by the CPU running in this mode.  In order to switch to kernel mode you have to issue a specific instruction (often called a <a href=\"https://en.wikipedia.org/wiki/Trap_(computing)\" rel=\"noreferrer\"><em>trap</em></a>) that switches the CPU to running in kernel mode <em>and runs code from a specific location held in a jump table.</em>  For security reasons, you cannot switch to kernel mode and execute arbitrary code - the traps are managed through a table of addresses that cannot be written to unless the CPU is running in supervisor mode.  You trap with an explicit trap number and the address is looked up in the jump table; the kernel has a finite number of controlled entry points.</p>\n",
    "\n",
    "<p>The 'system' calls in the C library (particularly those described in Section 2 of the man pages) have a user-mode component, which is what you actually call from your C program.  Behind the scenes, they may issue one or more system calls to the kernel to do specific services such as I/O, but they still also have code running in user-mode.  It is also quite possible to directly issue a trap to kernel mode from any user space code if desired, although you may need to write a snippet of assembly language to set up the registers correctly for the call.</p>\n",
    "\n",
    "<p><strong>More about 'sys'</strong></p>\n",
    "\n",
    "<p>There are things that your code cannot do from user mode - things like allocating memory or accessing hardware (HDD, network, etc.). These are under the supervision of the kernel, and it alone can do them. Some operations like <code>malloc</code> or<code>fread</code>/<code>fwrite</code> will invoke these kernel functions and that then will count as 'sys' time. Unfortunately it's not as simple as \"every call to malloc will be counted in 'sys' time\". The call to <code>malloc</code> will do some processing of its own (still counted in 'user' time) and then somewhere along the way it may call the function in kernel (counted in 'sys' time). After returning from the kernel call, there will be some more time in 'user' and then <code>malloc</code> will return to your code. As for when the switch happens, and how much of it is spent in kernel mode... you cannot say. It depends on the implementation of the library. Also, other seemingly innocent functions might also use <code>malloc</code> and the like in the background, which will again have some time in 'sys' then.</p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python comes with a module called timeit. You can use it to time small code snippets. The timeit module uses platform-specific time functions so that you will get the most accurate timings possible.\n",
    "\n",
    "The timeit module has a command-line interface, but it can also be imported. We will start out by looking at how to use timeit from the command line. Open up a terminal and try the following examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python3 -m timeit -s \"[ord(x) for x in 'abcdfghi']\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python3 -m timeit -s \"[chr(int(x)) for x in '123456789']\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s going on here? Well, when you call Python on the command line and pass it the “-m” option, you are telling it to look up a module and use it as the main program. The “-s” tells the timeit module to run setup once. Then it runs the code for n number of loops 5 times and returns the best average of the 5 runs. For these silly examples, you won’t see much difference.\n",
    "\n",
    "Your output will likely be slightly different as it is dependent on your computer’s specifications.\n",
    "\n",
    "Let’s write a silly function and see if we can time it from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_func.py\n",
    "def my_function():\n",
    "    try:\n",
    "        1 / 0\n",
    "    except ZeroDivisionError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this function does is cause an error that is promptly ignored. Yes, it’s another silly example. To get timeit to run this code on the command line, we will need to import the code into its namespace, so make sure you have changed your current working directory to be in the same folder that this script is in. Then run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python3 -m timeit \"import simple_func; simple_func.my_function()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the function and then call it. Note that we separate the import and the function call with semi-colons and that the Python code is in quotes. Now we’re ready to learn how to use timeit inside an actual Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing timeit for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [timeit — Measure execution time of small code snippets](https://docs.python.org/3/library/timeit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the timeit module inside your code is also pretty easy. We’ll use the same silly script from before and show you how below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014531105000060052\n"
     ]
    }
   ],
   "source": [
    "def my_function():\n",
    "    try:\n",
    "        1 / 0\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import timeit\n",
    "    # To give the timeit module access to functions you define, \n",
    "    # you can pass a setup parameter which contains an import statement\n",
    "    setup = \"from __main__ import my_function\"\n",
    "    print(timeit.timeit(\"my_function()\", setup=setup, number=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check to see if the script is being run directly (i.e. not imported). If it is, then we import timeit, create a setup string to import the function into timeit’s namespace and then we call timeit.timeit. You will note that we pass a call to the function in quotes, then the setup string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that timeit() will automatically determine the number of repetitions only when the command-line interface is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to pass globals() to the globals parameter, which will cause the code to be executed within your current global namespace. This can be more convenient than individually specifying imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "def g(x):\n",
    "    return x**4\n",
    "def h(x):\n",
    "    return x**8\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import timeit\n",
    "    print(timeit.timeit('[func(42) for func in (f,g,h)]', globals=globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a Decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing your own timer is a lot of fun too, although it may not be as accurate as just using timeit depending on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you may already know that your code spends most of its time in a few\n",
    "selected functions. For selected profiling of functions, a short decorator can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timethis.py\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timethis(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        r = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f'{func.__module__}.{func.__name__} : {end - start}')\n",
    "        return r\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this decorator, you simply place it in front of a function definition to get timings\n",
    "from it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator_test.py\n",
    "\n",
    "from timethis import timethis\n",
    "\n",
    "@timethis\n",
    "def countdown(n):\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    countdown(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that it accepts a function and has another function inside of it. The nested function will grab the time before calling the passed in function. Then it waits for the function to return and grabs the end time. Now we know how long the function took to run, so we print it out. Of course, the decorator also needs to return the result of the function call and the function itself, so that’s what the last two statements are all about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would actually want to time functions that connect to databases (or run large queries), websites, run threads or do other things that take a while to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making performance measurements, be aware that any results you get are approximations.\n",
    "The time.perf_counter() function used in the solution provides the\n",
    "highest-resolution timer possible on a given platform. However, it still measures wallclock\n",
    "time, and can be impacted by many different factors, such as machine load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in process time as opposed to wall-clock time, use time.process_time() instead. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__.countdown : 1.0159407149999993\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def timethis_process_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.process_time()\n",
    "        r = func(*args, **kwargs)\n",
    "        end = time.process_time()\n",
    "        print(f'{func.__module__}.{func.__name__} : {end - start}')\n",
    "        return r\n",
    "    return wrapper\n",
    "\n",
    "@timethis_process_time\n",
    "def countdown(n):\n",
    "    #time.sleep(2)\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    countdown(10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but not least, if you’re going to perform detailed timing analysis, make sure to read\n",
    "the documentation for the time, timeit, and other associated modules, so that you have\n",
    "an understanding of important platform-related differences and other pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **perf_counter()**: Return the value (in fractional seconds) of a performance counter, i.e. a clock with the highest available resolution to measure a short duration. It does include time elapsed during sleep and is system-wide. The reference point of the returned value is undefined, so that only the difference between the results of consecutive calls is valid.\n",
    "- **process_time()**: Return the value (in fractional seconds) of the sum of the system and user CPU time of the current process. It does not include time elapsed during sleep. It is process-wide by definition. The reference point of the returned value is undefined, so that only the difference between the results of consecutive calls is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Timing Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To time a block of statements, you can define a context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contextmaneger_time.py\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timeblock(label):\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end = time.perf_counter()\n",
    "        print(f'{label} : {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how the context manager works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting : 2.248141140677035\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with timeblock('counting'):\n",
    "        n = 10000000\n",
    "        while n > 0:\n",
    "            n -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Stopwatch Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to be able to record the time it takes to perform various tasks. The time module contains various functions for performing timing-related functions.\n",
    "However, it’s often useful to put a higher-level interface on them that mimics a stop\n",
    "watch. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwatch_timer.py\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, func=time.perf_counter):\n",
    "        self.elapsed = 0.0\n",
    "        self._func = func\n",
    "        self._start = None\n",
    "\n",
    "    def start(self):\n",
    "        if self._start is not None:\n",
    "            raise RuntimeError('Already started')\n",
    "        self._start = self._func()\n",
    "\n",
    "    def stop(self):\n",
    "        if self._start is None:\n",
    "            raise RuntimeError('Not started')\n",
    "        end = self._func()\n",
    "        self.elapsed += end - self._start\n",
    "        self._start = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.elapsed = 0.0\n",
    "\n",
    "    @property\n",
    "    def running(self):\n",
    "        return self._start is not None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class defines a timer that can be started, stopped, and reset as needed by the user.\n",
    "It keeps track of the total elapsed time in the elapsed attribute. Here is an example that\n",
    "shows how it can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15078234393149614\n",
      "0.2844322547316551\n",
      "0.10340904537588358\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def countdown(n):\n",
    "        while n > 0:\n",
    "            n -= 1\n",
    "\n",
    "    # Use 1: Explicit start/stop\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    countdown(1000000)\n",
    "    t.stop()\n",
    "    print(t.elapsed)\n",
    "    \n",
    "    # Use 2: As a context manager\n",
    "    with t:\n",
    "        countdown(1000000)\n",
    "\n",
    "    print(t.elapsed)\n",
    "\n",
    "    with Timer() as t2:\n",
    "        countdown(1000000)\n",
    "        \n",
    "    print(t2.elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe provides a simple yet very useful class for making timing measurements and\n",
    "tracking elapsed time. It’s also a nice illustration of how to support the contextmanagement\n",
    "protocol and the with statement.\n",
    "\n",
    "One issue in making timing measurements concerns the underlying time function used\n",
    "to do it. As a general rule, the accuracy of timing measurements made with functions\n",
    "such as time.time() or time.clock() varies according to the operating system. In\n",
    "contrast, the time.perf_counter() function always uses the highest-resolution timer\n",
    "available on the system.\n",
    "\n",
    "As shown, the time recorded by the Timer class is made according to wall-clock time,\n",
    "and includes all time spent sleeping. If you only want the amount of CPU time used by\n",
    "the process, use time.process_time() instead. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Timer(time.process_time)\n",
    "with t:\n",
    "    countdown(1000000)\n",
    "print(t.elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the time.perf_counter() and time.process_time() return a “time” in fractional\n",
    "seconds. However, the actual value of the time doesn’t have any particular meaning. To\n",
    "make sense of the results, you have to call the functions twice and compute a time\n",
    "difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Your Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A profile is a set of statistics that describes how often and for how long various parts of the program executed. So if you are trying to optimize a script runtime, or you are having a particular function that is taking too much time to process, you can profile the script to narrow down the issue. These statistics can be formatted into reports via the pstats module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The profiler modules are designed to provide an execution profile for a given program, not for benchmarking purposes (for that, there is timeit for reasonably accurate results). This particularly applies to benchmarking Python code against C code: the profilers introduce overhead for Python code, but not for C-level functions, and so the C code would seem faster than any Python one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Tracks time spent in functions.\n",
    "- Not for production -> overcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python standard library provides two different implementations of the same profiling interface:\n",
    "1. cProfile is recommended for most users; it’s a C extension with reasonable overhead that makes it suitable for profiling long-running programs. Based on lsprof, contributed by Brett Rosen and Ted Czotter.\n",
    "2. profile, a pure Python module whose interface is imitated by cProfile, but which adds significant overhead to profiled programs. If you’re trying to extend the profiler in some way, the task might be easier with this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Python Profilers](https://docs.python.org/3.8/library/profile.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python comes with its own code profilers built-in. There is the profile module and the cProfile module. The profile module is pure Python, but it will add a lot of overhead to anything you profile, so it’s usually recommended that you go with cProfile, which has a similar interface but is much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call cProfile on the command line in much the same way as we did with the timeit module. The main difference is that you would pass a Python script to it instead of just passing a snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python3 -m cProfile -o someprogram_results.cprof someprogram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "\n",
    "data = pstats.Stats('skripte/someprogram_results.cprof')\n",
    "data.sort_stats('cumulative').print_stats('someprogram') \n",
    "#cumtime -> tolko časa kot je porabla ta funkcija pa vse ki jih je poklicala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 12 09:06:49 2020    skripte/someprogram_results.cprof\n",
      "\n",
      "         1500067 function calls in 1.121 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 51 to 4 due to restriction <'someprogram'>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   500000    0.638    0.000    0.638    0.000 someprogram.py:6(f2)\n",
      "        1    0.239    0.239    1.121    1.121 someprogram.py:14(main)\n",
      "   500000    0.153    0.000    0.244    0.000 someprogram.py:3(f1)\n",
      "        1    0.000    0.000    1.121    1.121 someprogram.py:1(<module>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7fbc49e60d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_stats('tottime').print_stats('someprogram') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[snakeviz](https://jiffyclub.github.io/snakeviz) -> grafično prikazovanjem cprofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         4 function calls in 0.023 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.020    0.020    0.020    0.020 <string>:1(<listcomp>)\n",
      "        1    0.003    0.003    0.023    0.023 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.023    0.023 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# profiler_test.py\n",
    "import cProfile\n",
    "cProfile.run(\"[x for x in range(150000)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s break this down a bit. The first line shows that there were four function calls. The next line tells us how the results are ordered. According to the documentation, standard name refers to the far right column. There are a number of columns here.\n",
    "- **ncalls** is the number of calls made.\n",
    "- **tottime** is a total of the time spent in the given function.\n",
    "- **percall** refers to the quotient of tottime divided by ncalls\n",
    "- **cumtime** is the cumulative time spent in this and all subfunctions. It’s even accurate for recursive functions!\n",
    "- The **econd percall** column is the quotient of cumtime divided by primitive calls\n",
    "- **filename:lineno(function)** provides the respective data of each function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s a neat 3rd party project called line_profiler that is designed to profile the time each individual line takes to execute. It also includes a script called kernprof for profiling Python applications and scripts using line_profiler. Just use pip to install the package. Here’s how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip install line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually use the line_profiler, we will need some code to profile. But first, I need to explain how line_profiler works when you call it on the command line. You will actually be calling line_profiler by calling the kernprof script. I thought that was a bit confusing the first time I used it, but that’s just the way it works. Here’s the normal way to use it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kernprof -l silly_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print out the following message when it finishes: Wrote profile results to silly_functions.py.lprof. This is a binary file that we can’t view directly. When we run kernprof though, it will actually inject an instance of LineProfiler into your script’s `__builtins__` namespace. The instance will be named profile and is meant to be used as a decorator. With that in mind, we can actually write our script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silly_functions.py\n",
    "import time\n",
    "@profile\n",
    "def fast_function():\n",
    "    print(\"I'm a fast function!\")\n",
    "@profile\n",
    "def slow_function():\n",
    "    time.sleep(2)\n",
    "    print(\"I'm a slow function\")\n",
    "if __name__ == '__main__':\n",
    "    fast_function()\n",
    "    slow_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have two decorated functions that are decorated with something that isn’t imported. If you actually try to run this script as is, you will get a NameError because “profile” is not defined. So always remember to remove your decorators after you have profiled your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kernprof -l silly_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s back up and learn how to actually view the results of our profiler. There are two methods we can use. The first is to use the line_profiler module to read our results file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python3 -m line_profiler silly_functions.py.lprof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternate method is to just use kernprof in verbose mode by passing is -v:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kernprof -l -v silly_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless which method you use, you should end up seeing something like the following get printed to your screen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    I'm a fast function!\n",
    "    I'm a slow function\n",
    "    Wrote profile results to silly_functions.py.lprof\n",
    "    Timer unit: 1e-06 s\n",
    "    Total time: 3.4e-05 s\n",
    "    File: silly_functions.py\n",
    "    Function: fast_function at line 3\n",
    "    Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "    ==============================================================\n",
    "         3                                           @profile\n",
    "         4                                           def fast_function():\n",
    "         5         1           34     34.0    100.0      print(\"I'm a fast function!\")\n",
    "    Total time: 2.001 s\n",
    "    File: silly_functions.py\n",
    "    Function: slow_function at line 7\n",
    "    Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "    ==============================================================\n",
    "         7                                           @profile\n",
    "         8                                           def slow_function():\n",
    "         9         1      2000942 2000942.0    100.0      time.sleep(2)\n",
    "        10         1           59     59.0      0.0      print(\"I'm a slow function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the source code is printed out with the timing information for each line. There are six columns of information here. Let’s find out what each one means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    " <li><strong>Line #</strong> – The line number of the code that was profiled</li>\n",
    " <li><strong>Hits</strong> – The number of times that particular line was executed</li>\n",
    " <li><strong>Time</strong> – The total amount of time the line took to execute (in the timer’s unit). The timer unit can be seen at the beginning of the output</li>\n",
    " <li><strong>Per Hit</strong> – The average amount of time that line of code took to execute (in timer units)</li>\n",
    " <li><strong>% Time</strong> – The percentage of time spent on the line relative to the total amount of time spent in said function</li>\n",
    " <li>Line Contents – The actual source code that was executed</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you happen to be an IPython user, then you might want to know that IPython has a magic command (%lprun) that allows you to specify functions to profile and even which statement to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great 3rd party profiling package is memory_profiler. The memory_profiler module can be used for monitoring memory consumption in a process, or you can use it for a line-by-line analysis of the memory consumption of your code. Since it’s not included with Python, we’ll have to install it. You can use pip for this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    pip install memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it’s installed, we need some code to run it against. The memory_profiler actually works in much the same way as line_profiler in that when you run it, memory_profiler will inject an instance of itself into `__builtins__` named profile that you are supposed to use as a decorator on the function you are profiling. Here’s a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memo_prof.py \n",
    "@profile\n",
    "def mem_func():\n",
    "    lots_of_numbers = list(range(1500))\n",
    "    x = ['letters'] * (5 ** 10)\n",
    "    del lots_of_numbers\n",
    "    return None\n",
    "if __name__ == '__main__':\n",
    "    mem_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a list that contains 1500 integers. Then we create a list with 9765625 (5 to the 10 power) instances of a string. Finally we delete the first list and return. The memory_profiler doesn’t have another script you need to run to do the actual profiling like line_profiler did. Instead you can just run Python and use its **-m** parameter on the command line to load the module and run it against our script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python3 -m memory_profiler memo_prof.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are pretty self-explanatory this time around. We have our line numbers and then the amount of memory used after said line was executed. Next we have an increment field which tells us the difference in memory of the current line versus the line previous. The very last column is for the code itself.\n",
    "\n",
    "The memory_profiler also includes mprof which can be used to create full memory usage reports over time instead of line-by-line. It’s very easy to use; just take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    mprof run memo_prof.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mprof can also create a graph that shows you how your application consumed memory over time. To get the graph, all you need to do is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    mprof plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### profilehooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last third party package that we will look at in this chapter is called profilehooks. It is a collection of decorators specifically designed for profiling functions. To install profilehooks, just do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip install profilehooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it installed, let’s re-use the example from the last section and modify it slightly to use profilehooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profhooks.py\n",
    "from profilehooks import profile\n",
    "\n",
    "@profile\n",
    "def mem_func():\n",
    "    lots_of_numbers = list(range(1500))\n",
    "    x = ['letters'] * (5 ** 10)\n",
    "    del lots_of_numbers\n",
    "    return None\n",
    "if __name__ == '__main__':\n",
    "    mem_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need to do to use profilehooks is import it and then decorate the function that you want to profile. If you run the code above, you will get output similar to the following sent to stdout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *** PROFILER RESULTS ***\n",
    "    mem_func (c:\\Users\\mike\\Dropbox\\Scripts\\py3\\profhooks.py:3)\n",
    "    function called 1 times\n",
    "             3 function calls in 0.096 seconds\n",
    "       Ordered by: cumulative time, internal time, call count\n",
    "       ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "            1    0.096    0.096    0.096    0.096 profhooks.py:3(mem_func)\n",
    "            1    0.000    0.000    0.000    0.000 {range}\n",
    "            1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
    "            0    0.000             0.000          profile:0(profiler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for this package appears to follow that of the cProfile module from Python’s standard library. You can refer to the descriptions of the columns earlier in this chapter to see what these mean. The profilehooks package has two more decorators. The first one we will look at is called timecall. It gives us the course run time of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profhooks2.py\n",
    "from profilehooks import timecall\n",
    "\n",
    "@timecall\n",
    "def mem_func():\n",
    "    lots_of_numbers = list(range(1500))\n",
    "    x = ['letters'] * (5 ** 10)\n",
    "    del lots_of_numbers\n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mem_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I just want to mention that you can also run profilehooks on the command line using Python’s -m flag:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    python -m profilehooks mymodule.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Your Programs Run Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your program runs too slow and you’d like to speed it up without the assistance of more\n",
    "extreme solutions, such as C extensions or a just-in-time (JIT) compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the first rule of optimization might be to “not do it,” the second rule is almost\n",
    "certainly “don’t optimize the unimportant.” To that end, if your program is running slow,\n",
    "you might start by profiling your code. \n",
    "\n",
    "More often than not, you’ll find that your program spends its time in a few hotspots,\n",
    "such as inner data processing loops. Once you’ve identified those locations, you can use\n",
    "the no-nonsense techniques presented in the following sections to make your program\n",
    "run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of programmers start using Python as a language for writing simple scripts. When\n",
    "writing scripts, it is easy to fall into a practice of simply writing code with very little\n",
    "structure. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somescript.py\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "with open(sys.argv[1]) as f:\n",
    "    for row in csv.reader(f):\n",
    "        # Some kind of processing\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little-known fact is that code defined in the global scope like this runs slower than\n",
    "code defined in a function. The speed difference has to do with the implementation of\n",
    "local versus global variables (operations involving locals are faster). So, if you want to\n",
    "make the program run faster, simply put the scripting statements in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somescript.py\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "def main(filename):\n",
    "    with open(filename) as f:\n",
    "        for row in csv.reader(f):\n",
    "            # Some kind of processing\n",
    "            pass\n",
    "        \n",
    "main(sys.argv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed difference depends heavily on the processing being performed, but in our\n",
    "experience, speedups of 15-30% are not uncommon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_function_no.py\n",
    "n = 1000000\n",
    "\n",
    "while n > 0:\n",
    "    n -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_function_yes.py\n",
    "def countdown(n):\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "\n",
    "countdown(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    time python 01_function_no.py\n",
    "    time python 01_function_yes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selectively eliminate attribute access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every use of the dot (.) operator to access attributes comes with a cost. Under the covers,\n",
    "this triggers special methods, such as `__getattribute__()` and `_getattr__()`, which\n",
    "often lead to dictionary lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can often avoid attribute lookups by using the from module import name form of\n",
    "import as well as making selected use of bound methods. To illustrate, consider the\n",
    "following code fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02_access.py\n",
    "import math\n",
    "\n",
    "def compute_roots(nums):\n",
    "    result = []\n",
    "    for n in nums:\n",
    "        result.append(math.sqrt(n))\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "nums = range(1000000)\n",
    "for n in range(10):\n",
    "    r = compute_roots(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tested on our machine, this program runs in about 40 seconds. Now change the\n",
    "compute_roots() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02_noaccess.py\n",
    "from math import sqrt\n",
    "\n",
    "def compute_roots(nums):\n",
    "    result = []\n",
    "    result_append = result.append\n",
    "    for n in nums:\n",
    "        result_append(sqrt(n))\n",
    "    return result\n",
    "    \n",
    "# Test\n",
    "nums = range(1000000)\n",
    "for n in range(10):\n",
    "    r = compute_roots(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version runs in about 29 seconds. The only difference between the two versions of\n",
    "code is the elimination of attribute access. Instead of using math.sqrt(), the code uses\n",
    "sqrt(). The result.append() method is additionally placed into a local variable result_append and reused in the inner loop.\n",
    "\n",
    "However, it must be emphasized that these changes only make sense in frequently executed\n",
    "code, such as loops. So, this optimization really only makes sense in carefully\n",
    "selected places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand locality of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously noted, local variables are faster than global variables. For frequently accessed\n",
    "names, speedups can be obtained by making those names as local as possible.\n",
    "For example, consider this modified version of the compute_roots() function just\n",
    "discussed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_roots(nums):\n",
    "    sqrt = math.sqrt\n",
    "    result = []\n",
    "    result_append = result.append\n",
    "    for n in nums:\n",
    "        result_append(sqrt(n))\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "nums = range(1000000)\n",
    "for n in range(10):\n",
    "    r = compute_roots(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, sqrt has been lifted from the math module and placed into a local\n",
    "variable. If you run this code, it now runs in about 25 seconds (an improvement over\n",
    "the previous version, which took 29 seconds). That additional speedup is due to a local\n",
    "lookup of sqrt being a bit faster than a global lookup of sqrt.\n",
    "\n",
    "Locality arguments also apply when working in classes. In general, looking up a value\n",
    "such as self.name will be considerably slower than accessing a local variable. In inner\n",
    "loops, it might pay to lift commonly accessed attributes into a local variable. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slower\n",
    "class SomeClass:\n",
    "    # neki\n",
    "    def method(self):\n",
    "        for x in s:\n",
    "            op(self.value)\n",
    "\n",
    "# Faster\n",
    "class SomeClass:\n",
    "    # neki\n",
    "    def method(self):\n",
    "        value = self.value\n",
    "        for x in s:\n",
    "            op(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid gratuitous abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time you wrap up code with extra layers of processing, such as decorators, properties,\n",
    "or descriptors, you’re going to make it slower. As an example, consider this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "    \n",
    "    @y.setter\n",
    "    def y(self, value):\n",
    "        self._y = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try a simple timing test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "a = A(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11272381200024029"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit('a.x', 'from __main__ import a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2618356349998976"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit('a.y', 'from __main__ import a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, accessing the property y is not just slightly slower than a simple\n",
    "attribute x, it’s about 4.5 times slower. If this difference matters, you should ask yourself\n",
    "if the definition of y as a property was really necessary. If not, simply get rid of it and\n",
    "go back to using a simple attribute instead. Just because it might be common for programs\n",
    "in another programming language to use getter/setter functions, that doesn’t\n",
    "mean you should adopt that programming style for Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the built-in containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built-in data types such as strings, tuples, lists, sets, and dicts are all implemented in C,\n",
    "and are rather fast. If you’re inclined to make your own data structures as a replacement\n",
    "(e.g., linked lists, balanced trees, etc.), it may be rather difficult if not impossible to match\n",
    "the speed of the built-ins. Thus, you’re often better off just using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[collections — Container datatypes](https://docs.python.org/3.8/library/collections.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid making unnecessary data structures or copies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes programmers get carried away with making unnecessary data structures\n",
    "when they just don’t have to. For example, someone might write code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tuple(range(50))\n",
    "\n",
    "values = [x for x in sequence]\n",
    "squares = [x*x for x in values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the thinking here is to first collect a bunch of values into a list and then to start\n",
    "applying operations such as list comprehensions to it. However, the first list is completely\n",
    "unnecessary. Simply write the code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = [x*x for x in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related to this, be on the lookout for code written by programmers who are overly\n",
    "paranoid about Python’s sharing of values. Overuse of functions such as copy.deep\n",
    "copy() may be a sign of code that’s been written by someone who doesn’t fully understand\n",
    "or trust Python’s memory model. In such code, it may be safe to eliminate many\n",
    "of the copies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before optimizing, it’s usually worthwhile to study the algorithms that you’re using first.\n",
    "You’ll get a much bigger speedup by switching to an O(n log n) algorithm than by\n",
    "trying to tweak the implementation of an an O(n**2) algorithm.\n",
    "\n",
    "If you’ve decided that you still must optimize, it pays to consider the big picture. As a\n",
    "general rule, you don’t want to apply optimizations to every part of your program,\n",
    "because such changes are going to make the code hard to read and understand. Instead,\n",
    "focus only on known performance bottlenecks, such as inner loops.\n",
    "\n",
    "You need to be especially wary interpreting the results of micro-optimizations. For\n",
    "example, consider these two techniques for creating a dictionary:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "'name' : 'AAPL',\n",
    "'shares' : 100,\n",
    "'price' : 534.22\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dict(name='AAPL', shares=100, price=534.22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter choice has the benefit of less typing (you don’t need to quote the key names).\n",
    "However, if you put the two code fragments in a head-to-head performance battle, you’ll\n",
    "find that using dict() runs three times slower! With this knowledge, you might be\n",
    "inclined to scan your code and replace every use of dict() with its more verbose alternative.\n",
    "However, a smart programmer will only focus on parts of a program where\n",
    "it might actually matter, such as an inner loop. In other places, the speed difference just\n",
    "isn’t going to matter at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, on the other hand, your performance needs go far beyond the simple techniques in\n",
    "this recipe, you might investigate the use of tools based on just-in-time (JIT) compilation\n",
    "techniques. For example, the PyPy project is an alternate implementation of the Python interpreter \n",
    "that analyzes the execution of your program and generates native machine\n",
    "code for frequently executed parts. It can sometimes make Python programs run an\n",
    "order of magnitude faster, often approaching (or even exceeding) the speed of code\n",
    "written in C. Unfortunately, as of this writing, PyPy does not yet fully support Python3. \n",
    "So, that is something to look for in the future. You might also consider the Numba\n",
    "project. Numba is a dynamic compiler where you annotate selected Python functions\n",
    "that you want to optimize with a decorator. Those functions are then compiled into\n",
    "native machine code through the use of LLVM. It too can produce signficant performance\n",
    "gains. However, like PyPy, support for Python 3 should be viewed as somewhat\n",
    "experimental.\n",
    "\n",
    "Last, but not least, the words of John Ousterhout come to mind: “The best performance\n",
    "improvement is the transition from the nonworking to the working state.” Don’t worry\n",
    "about optimization until you need to. Making sure your program works correctly is\n",
    "usually more important than making it run fast (at least initially)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
