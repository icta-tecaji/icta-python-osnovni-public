{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Del 03: Web Communication and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Viri**:\n",
    "- [Building a Web Scraper from start to finish](https://hackernoon.com/building-a-web-scraper-from-start-to-finish-bb6b95388184)\n",
    "- [Introduction to Web Scraping with BeautifulSoup](https://towardsdatascience.com/introduction-to-web-scraping-with-beautifulsoup-e87a06c2b857)\n",
    "- [Web Scraping using Selenium and BeautifulSoup](https://towardsdatascience.com/web-scraping-using-selenium-and-beautifulsoup-99195cd70a58)\n",
    "- [Web scraping with Python — A to Z](https://towardsdatascience.com/web-scraping-with-python-a-to-copy-z-277a445d64c7)\n",
    "- [Web scraping for web developers: a concise summary](https://medium.freecodecamp.org/web-scraping-for-web-developers-a-concise-summary-3af3d0ca4069)\n",
    "- [Web Scraping Walkthrough with Python](https://dev.to/awwsmm/web-scraping-walkthrough-with-python-85c)\n",
    "- [Web Scraping Using BeautifulSoup](https://towardsdatascience.com/web-scraping-using-beautifulsoup-edd9441ba734)\n",
    "- [Web Scraping Mountain Weather Forecasts using Python and a Raspberry Pi](https://towardsdatascience.com/web-scraping-mountain-weather-forecasts-using-python-and-a-raspberry-pi-f215fdf82c6b)\n",
    "- [Data Science Skills: Web scraping using python](https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Web Scraping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web “scraping” (also called “web harvesting,” “web data extraction,” or even “web data\n",
    "mining”), can be defined as “the construction of an agent to download, parse, and\n",
    "organize data from the web in an automated manner.” Or, in other words: instead of\n",
    "a human end user clicking away in a web browser and copy-pasting interesting parts\n",
    "into, say, a spreadsheet, web scraping offloads this task to a computer program that can\n",
    "execute it much faster, and more correctly, than a human can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"progressiveMedia-image js-progressiveMedia-image\" data-src=\"https://cdn-images-1.medium.com/max/1600/1*GOyqaID2x1N5lD_rhTDKVQ.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*GOyqaID2x1N5lD_rhTDKVQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Web Scraping for Data Science?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When surfing the web using a normal web browser, you’ve probably encountered\n",
    "multiple sites where you considered the possibility of gathering, storing, and analyzing\n",
    "the data presented on the site’s pages. Especially for data scientists, whose “raw\n",
    "material” is data, the web exposes a lot of interesting opportunities:\n",
    "- There might be an interesting table on a Wikipedia page (or pages) you want to retrieve to perform some statistical analysis.\n",
    "- Perhaps you want to get a list of reviews from a movie site to perform text mining, create a recommendation engine, or build a predictive model to spot fake reviews.\n",
    "- You might wish to get a listing of properties on a real-estate site to build an appealing geo-visualization.\n",
    "- You’d like to gather additional features to enrich your data set based on information found on the web, say, weather information to forecast, for example, soft drink sales.\n",
    "- You might be wondering about doing social network analytics using profile data found on a web forum.\n",
    "- It might be interesting to monitor a news site for trending new stories on a particular topic of interest.\n",
    "\n",
    "The web contains lots of interesting data sources that provide a treasure trove for all\n",
    "sorts of interesting things. Sadly, the current unstructured nature of the web does not\n",
    "always make it easy to gather or export this data in an easy manner. Web browsers are\n",
    "very good at showing images, displaying animations, and laying out websites in a way\n",
    "that is visually appealing to humans, but they do not expose a simple way to export their\n",
    "data, at least not in most cases. Instead of viewing the web page by page through your\n",
    "web browser’s window, wouldn’t it be nice to be able to automatically gather a rich data\n",
    "set? This is exactly where web scraping enters the picture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, the web has become so integrated into our day-to-day activities that we\n",
    "rarely consider its complexity. Whenever you surf the web, a whole series of networking\n",
    "protocols is being kicked into gear to set up connections to computers all over the world\n",
    "and retrieve data, all in a matter of seconds. Consider, for instance, the following series\n",
    "of steps that gets executed by your web browser once you navigate to a website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve now seen how your web browser communicates with a server on the World\n",
    "Wide Web. The core component in the exchange of messages consists of a HyperText\n",
    "Transfer Protocol (HTTP) request message to a web server, followed by an HTTP\n",
    "response (also oftentimes called an HTTP reply), which can be rendered by the browser.\n",
    "\n",
    "Since all of our web scraping will build upon HTTP, we do need to take a closer look at\n",
    "HTTP messages to learn what they look like.\n",
    "\n",
    "HTTP is, in fact, a rather simple networking protocol. It is text based, which at\n",
    "least makes its messages somewhat readable to end users (compared to raw binary\n",
    "messages that have no textual structure at all) and follow a simple request-reply\n",
    "based communication scheme. That is, contacting a web server and receiving a reply\n",
    "simply involves two HTTP messages: a request and a reply. In case your browser wants\n",
    "to download or fetch additional resources (such as images), this will simply entail\n",
    "additional request-reply messages being sent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML and CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have discussed the basics of HTTP and how you can perform HTTP requests\n",
    "in Python using the requests library. However, since most web pages are formatted\n",
    "using the Hypertext Markup Language (HTML), we need to understand how to extract\n",
    "information from such pages. As such, this chapter introduces you to HTML, as well\n",
    "as another core building block that is used to format and stylize modern web pages:\n",
    "Cascading Style Sheets (CSS). This chapter then discusses the Beautiful Soup library,\n",
    "which will help us to make sense of the HTML and CSS “soup.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"progressiveMedia-image js-progressiveMedia-image\" data-src=\"https://cdn-images-1.medium.com/max/1600/1*x9mxFBXnLU05iPy19dGj7g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*x9mxFBXnLU05iPy19dGj7g.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertext Markup Language: HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we introduced the basics of HTTP and saw how to perform\n",
    "HTTP requests in Python using the requests library, but now we need to figure out a\n",
    "way to parse HTML contents. Recall our small Wikipedia example we ended with in the\n",
    "previous chapter and the soup of HTML we got back from it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link strani: https://en.wikipedia.org/w/index.php?title=List_of_Game_of_Thrones_episodes&oldid=802553687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# url_got = 'https://en.wikipedia.org/w/index.php?title=List_of_Game_of_Thrones_episodes&oldid=802553687'\n",
    "url_got = 'https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes'\n",
    "r = requests.get(url_got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the example above, you’ll see the following being printed onscreen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Hypertext Markup Language (HTML), the standard markup language for\n",
    "creating web pages. Although some will call HTML a “programming language,” “markup\n",
    "language” is a more appropriate term as it specifies how a document is structured and\n",
    "formatted. There is no strict need to use HTML to format web pages — in fact, all the\n",
    "examples we’ve dealt with in the previous chapter just returned simple, textual pages.\n",
    "However, if you want to create visually appealing pages that actually look good in a\n",
    "browser (even if it’s just putting some color on a page), HTML is the way to go.\n",
    "\n",
    "HTML provides the building blocks to provide structure and formatting to\n",
    "documents. This is provided by means of a series of “tags.” HTML tags often come in\n",
    "pairs and are enclosed in angled brackets, with `<tagname>` being the opening tag and\n",
    "`</tagname>` indicating the closing tag. Some tags come in an unpaired form, and do\n",
    "not require a closing tag. Some commonly used tags are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `<p>...</p>` to enclose a paragraph;\n",
    "- `<br>` to set a line break;\n",
    "- `<table>...</table>` to start a table block, inside; `<tr>...<tr/>` is used for the rows; and `<td>...</td>` cells;\n",
    "- `<img>` for images;\n",
    "- `<h1>...</h1> to <h6>...</h6>` for headers;\n",
    "- `<div>...</div>` to indicate a “division” in an HTML document, basically used to group a set of elements;\n",
    "- `<a>...</a>` for hyperlinks;\n",
    "- `<ul>...</ul>, <ol>...</ol>` for unordered and ordered lists respectively; inside of these, `<li>...</li>` is used for each list item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Your Browser as a Development Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most modern web browsers nowadays include a toolkit of powerful tools you can\n",
    "use to get an idea of what’s going on regarding HTML, and HTTP too. Navigate to the\n",
    "Wikipedia page over at https://en.wikipedia.org/w/index.php?title=List_of_Game_of_Thrones_episodes&oldid=802553687 again in your browser — we assume\n",
    "you’re using Google Chrome for what follows. First of all, it is helpful to know how you\n",
    "can take a look at the underlying HTML of this page. To do so, you can right-click on the\n",
    "page and press `View source` or simply press Control+U in Google Chrome. A new page\n",
    "will open containing the raw HTML contents for the current page (the same content as\n",
    "what we got back using r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can open up Chrome’s “Developer Tools.” To do so, either select the\n",
    "Chrome Menu at the top right of your browser window, then select “Tools,” “Developer\n",
    "Tools,” or press Control+Shift+I. Alternatively, you can also right-click on any page\n",
    "element and select `Inspect Element`. Other browsers such as Firefox and Microsoft\n",
    "Edge have similar tools built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Beautiful Soup Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re now ready to start working with HTML pages using Python. Recall the following\n",
    "lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#url_got = 'https://en.wikipedia.org/w/index.php?title=List_of_Game_of_Thrones_episodes&oldid=802553687'\n",
    "r = requests.get(url_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_contents = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**: Beautiful Soup tries to organize complexity: it helps to parse, structure and organize the oftentimes very messy web by fixing bad HTML and presenting us with an easy-to-work-with Python structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Beautiful Soup starts with the creation of a BeautifulSoup object. If you\n",
    "already have an HTML page contained in a string (as we have), this is straightforward.\n",
    "Don’t forget to add the new import line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = BeautifulSoup(html_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beautiful Soup library itself depends on an HTML\n",
    "parser to perform most of the bulk parsing work.\n",
    "\n",
    "In Python, multiple parsers exist to do so:\n",
    "- `html.parser`: a built-in Python parser that is decent (especially when using recent versions of Python 3) and requires no extra installation.\n",
    "- `lxml`: which is very fast but requires an extra installation.\n",
    "- `html5lib`: which aims to parse web page in exactly the same way as a web browser does, but is a bit slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are small differences between these parsers, Beautiful Soup warns you if\n",
    "you don’t explicitly provide one, this might cause your code to behave slightly different\n",
    "when executing the same script on different machines. To solve this, we simply specify a\n",
    "parser ourselves — we’ll stick with the default Python parser here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = BeautifulSoup(html_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup’s main task is to take HTML content and transform it into a tree-based representation. Once you’ve created a BeautifulSoup object, there are two\n",
    "methods you’ll be using to fetch data from the page:\n",
    "- `find(name, attrs, recursive, string, **keywords)`\n",
    "- `find_all(name, attrs, recursive, string, limit, **keywords)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods look very similar indeed, with the exception that find_all takes an\n",
    "extra limit argument. To test these methods, add the following lines to your script and\n",
    "run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">List of <i>Game of Thrones</i> episodes</h1>\n"
     ]
    }
   ],
   "source": [
    "print(html_soup.find('h1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"p-logo\" role=\"banner\">\n",
      "<a class=\"mw-wiki-logo\" href=\"/wiki/Main_Page\" title=\"Visit the main page\"></a>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "print(html_soup.find('', {'id': 'p-logo'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">List of <i>Game of Thrones</i> episodes</h1>\n",
      "<h2 id=\"mw-toc-heading\">Contents</h2>\n",
      "<h2><span class=\"mw-headline\" id=\"Series_overview\">Series overview</span></h2>\n",
      "<h2><span class=\"mw-headline\" id=\"Episodes\">Episodes</span></h2>\n",
      "<h2><span class=\"mw-headline\" id=\"Home_media_releases\">Home media releases</span></h2>\n",
      "<h2><span class=\"mw-headline\" id=\"Ratings\">Ratings</span></h2>\n",
      "<h2><span class=\"mw-headline\" id=\"References\">References</span></h2>\n",
      "<h2><span class=\"mw-headline\" id=\"External_links\">External links</span></h2>\n",
      "<h2>Navigation menu</h2>\n"
     ]
    }
   ],
   "source": [
    "for found in html_soup.find_all(['h1', 'h2']):\n",
    "    print(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind these two methods should be relatively clear: they’re used\n",
    "to find elements inside the HTML tree. Let’s discuss the arguments of these two methods\n",
    "step by step:\n",
    "- The `name` argument defines the tag names you wish to “find” on the page. You can pass a string, or a list of tags. Leaving this argument as an empty string simply selects all elements.\n",
    "- The `attrs` argument takes a Python dictionary of attributes and matches HTML elements that match those attributes.\n",
    "- The `recursive` argument is a Boolean and governs the depth of the search. If set to True - the default value, the find and find_all methods will look into children, children’s children, and so on... for elements that match your query. If it is False, it will only look at direct child elements.\n",
    "- The `string` argument is used to perform matching based on the text content of elements.\n",
    "- The `limit` argument is only used in the find_all method and can be used to limit the number of elements that are retrieved. Note that find is functionally equivalent to calling find_all with the limit set to 1, with the exception that the former returns the retrieved element directly, and that the latter will always return a list of items, even if it just contains a single element. Also important to know is that, when find_all cannot find anything, it returns an empty list, whereas if find cannot find anything, it returns None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both find and find_all return Tag objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first h1 tag\n",
    "first_h1 = html_soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">List of <i>Game of Thrones</i> episodes</h1>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the name attribute to retrieve the tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h1'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the contents attribute to get a Python list containing the tag’s\n",
    "children (its direct descendant tags) as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['List of ', <i>Game of Thrones</i>, ' episodes']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the Tag object to a string shows both the tag and its HTML\n",
    "content as a string. This is what happens if you call print out the Tag\n",
    "object, for instance, or wrap such an object in the str function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">List of <i>Game of Thrones</i> episodes</h1>\n"
     ]
    }
   ],
   "source": [
    "print(str(first_h1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the text attribute to get the contents of the Tag object as clear\n",
    "text (without HTML tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'List of Game of Thrones episodes'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the get_text method as well, to which a\n",
    "strip Boolean argument can be given so that get_text(strip=True)\n",
    "is equivalent to text.strip(). It’s also possible to specify a string to\n",
    "be used to join the bits of text enclosed in the element together, for\n",
    "example, get_text('--').\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'List of Game of Thrones episodes'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'firstHeading', 'class': ['firstHeading'], 'lang': 'en'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firstHeading'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1.attrs['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firstHeading'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1['id']     # Does the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'firstHeading'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_h1.get('id')   # Does the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First of all, you cannot use class as a keyword, as this is a reserved\n",
    "Python keyword. This is a pity, as this will be one of the most frequently used\n",
    "attributes when hunting for content inside HTML. Luckily, Beautiful Soup has\n",
    "provided a workaround. Instead of using class, just write class_ as follows:\n",
    "“find(class_='myclass')”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first four cite elements with a citation class\n",
    "cites = html_soup.find_all('cite', class_='citation', limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Fowler, Matt (April 8, 2011). \"Game of Thrones: \"Winter is Coming\" Review\". IGN. Archived from the original on August 17, 2012. Retrieved September 22, 2016.\n",
      "https://web.archive.org/web/20120817073932/http://tv.ign.com/articles/116/1160215p1.html\n",
      "\n",
      "--> Fleming, Michael (January 16, 2007). \"HBO turns Fire into fantasy series\". Variety. Archived from the original on May 16, 2012. Retrieved September 3, 2016.\n",
      "https://web.archive.org/web/20120516224747/http://www.variety.com/article/VR1117957532?refCatId=14\n",
      "\n",
      "--> \"Game of Thrones\". Emmys.com. Retrieved September 17, 2016.\n",
      "http://www.emmys.com/shows/game-thrones\n",
      "\n",
      "--> Roberts, Josh (April 1, 2012). \"Where HBO's hit 'Game of Thrones' was filmed\". USA Today. Archived from the original on April 1, 2012. Retrieved March 8, 2013.\n",
      "https://web.archive.org/web/20120401123724/http://travel.usatoday.com/destinations/story/2012-04-01/Where-the-HBO-hit-Game-of-Thrones-was-filmed/53876876/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for citation in cites:\n",
    "    print(\"-->\", citation.get_text())\n",
    "    # Inside of this cite element, find the first a tag\n",
    "    link = citation.find('a')\n",
    "    # ... and show its URL\n",
    "    print(link.get('href'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to work out the following use case. You’ll note that our Game of\n",
    "Thrones Wikipedia page has a number of well-maintained tables listing the episodes with their directors, writers, air date, and number of viewers. Let’s try to fetch all of this\n",
    "data at once using what we have learned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# url = 'https://en.wikipedia.org/w/index.php?title=List_of_Game_of_Thrones_episodes&oldid=802553687'\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_contents = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = BeautifulSoup(html_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a list to store our episode list\n",
    "episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidimo da je vsem tabelam skupen element class:'wikiepisodetable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_tables = html_soup.find_all('table', class_='wikiepisodetable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ep_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in ep_tables:\n",
    "    headers = []\n",
    "    rows = table.find_all('tr')\n",
    "    # Start by fetching the header cells from the first row to determine\n",
    "    # the field names\n",
    "    for header in table.find('tr').find_all('th'):\n",
    "        headers.append(header.text)\n",
    "    # Then go through all the rows except the first one\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        values = []\n",
    "        # And get the column cells, the first one being inside a th-tag\n",
    "        for col in row.find_all(['th', 'td']):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            episode_dict = {headers[i]: values[i] for i in range(len(values))}\n",
    "            episodes.append(episode_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No.overall': '1', 'No. inseason': '1', 'Title': '\"Winter Is Coming\"', 'Directed by': 'Tim Van Patten', 'Written by': 'David Benioff & D. B. Weiss', 'Original air date\\u200a[20]': 'April\\xa017,\\xa02011\\xa0(2011-04-17)', 'U.S. viewers(millions)': '2.22[21]'}\n",
      "{'No.overall': '2', 'No. inseason': '2', 'Title': '\"The Kingsroad\"', 'Directed by': 'Tim Van Patten', 'Written by': 'David Benioff & D. B. Weiss', 'Original air date\\u200a[20]': 'April\\xa024,\\xa02011\\xa0(2011-04-24)', 'U.S. viewers(millions)': '2.20[22]'}\n",
      "{'No.overall': '3', 'No. inseason': '3', 'Title': '\"Lord Snow\"', 'Directed by': 'Brian Kirk', 'Written by': 'David Benioff & D. B. Weiss', 'Original air date\\u200a[20]': 'May\\xa01,\\xa02011\\xa0(2011-05-01)', 'U.S. viewers(millions)': '2.44[23]'}\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "for episode in episodes[:3]:\n",
    "    print(episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.overall</th>\n",
       "      <th>No. inseason</th>\n",
       "      <th>Title</th>\n",
       "      <th>Directed by</th>\n",
       "      <th>Written by</th>\n",
       "      <th>Original air date [20]</th>\n",
       "      <th>U.S. viewers(millions)</th>\n",
       "      <th>No.</th>\n",
       "      <th>Original air date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Winter Is Coming\"</td>\n",
       "      <td>Tim Van Patten</td>\n",
       "      <td>David Benioff &amp; D. B. Weiss</td>\n",
       "      <td>April 17, 2011 (2011-04-17)</td>\n",
       "      <td>2.22[21]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"The Kingsroad\"</td>\n",
       "      <td>Tim Van Patten</td>\n",
       "      <td>David Benioff &amp; D. B. Weiss</td>\n",
       "      <td>April 24, 2011 (2011-04-24)</td>\n",
       "      <td>2.20[22]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>\"Lord Snow\"</td>\n",
       "      <td>Brian Kirk</td>\n",
       "      <td>David Benioff &amp; D. B. Weiss</td>\n",
       "      <td>May 1, 2011 (2011-05-01)</td>\n",
       "      <td>2.44[23]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>\"Cripples, Bastards, and Broken Things\"</td>\n",
       "      <td>Brian Kirk</td>\n",
       "      <td>Bryan Cogman</td>\n",
       "      <td>May 8, 2011 (2011-05-08)</td>\n",
       "      <td>2.45[24]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>\"The Wolf and the Lion\"</td>\n",
       "      <td>Brian Kirk</td>\n",
       "      <td>David Benioff &amp; D. B. Weiss</td>\n",
       "      <td>May 15, 2011 (2011-05-15)</td>\n",
       "      <td>2.58[25]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>\"A Golden Crown\"</td>\n",
       "      <td>Daniel Minahan</td>\n",
       "      <td>Story by : David Benioff &amp; D. B. WeissTeleplay...</td>\n",
       "      <td>May 22, 2011 (2011-05-22)</td>\n",
       "      <td>2.44[26]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No.overall No. inseason                                    Title  \\\n",
       "0          1            1                       \"Winter Is Coming\"   \n",
       "1          2            2                          \"The Kingsroad\"   \n",
       "2          3            3                              \"Lord Snow\"   \n",
       "3          4            4  \"Cripples, Bastards, and Broken Things\"   \n",
       "4          5            5                  \"The Wolf and the Lion\"   \n",
       "5          6            6                         \"A Golden Crown\"   \n",
       "\n",
       "      Directed by                                         Written by  \\\n",
       "0  Tim Van Patten                        David Benioff & D. B. Weiss   \n",
       "1  Tim Van Patten                        David Benioff & D. B. Weiss   \n",
       "2      Brian Kirk                        David Benioff & D. B. Weiss   \n",
       "3      Brian Kirk                                       Bryan Cogman   \n",
       "4      Brian Kirk                        David Benioff & D. B. Weiss   \n",
       "5  Daniel Minahan  Story by : David Benioff & D. B. WeissTeleplay...   \n",
       "\n",
       "        Original air date [20] U.S. viewers(millions)  No. Original air date  \n",
       "0  April 17, 2011 (2011-04-17)               2.22[21]  NaN               NaN  \n",
       "1  April 24, 2011 (2011-04-24)               2.20[22]  NaN               NaN  \n",
       "2     May 1, 2011 (2011-05-01)               2.44[23]  NaN               NaN  \n",
       "3     May 8, 2011 (2011-05-08)               2.45[24]  NaN               NaN  \n",
       "4    May 15, 2011 (2011-05-15)               2.58[25]  NaN               NaN  \n",
       "5    May 22, 2011 (2011-05-22)               2.44[26]  NaN               NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(episodes).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code should be relatively straightforward at this point, though some\n",
    "things are worth pointing out:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We don’t come up with the “find_all('table', class_= 'wikiepisodetable')” line from thin air, although it might seem that way just by looking at the code. Recall what we said earlier about your browser’s developer tools becoming your best friend. Inspect the episode tables on the page. Note how they’re all defined by means of a `<table>` tag. However, the page also contains tables we do not want to include. Some further investigation leads us to a solution: all the episode tables have “wikiepisodetable” as a class, whereas the other tables do not. You’ll often have to puzzle your way through a page first before coming up with a solid approach. In many cases, you’ll have to perform multiple find and find_all iterations before ending up where you want to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For every table, we first want to retrieve the headers to use as keys in a Python dictionary. To do so, we first select the first `<tr>` tag, and select all `<th>` tags within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we loop through all the rows (the `<tr>` tags), except for the first one (the header row). For each row, we loop through the `<th>` and `<td>` tags to extract the column values (the first column is wrapped inside of a `<th>` tag, the others in `<td>` tags, which is why we need to handle both). At the end of each row, we’re ready to add a new entry to the “episodes” variable. To store each entry, we use a normal Python dictionary (episode_dict). The way how this object is constructed might look a bit strange in case you’re not very familiar with Python. That is, Python allows us to construct a complete list or dictionary “in one go” by putting a “for” construct inside the “[...]” or “{...}” brackets. Here, we use this to immediately loop through the headers and values lists to build the dictionary object. Note that this assumes that both of these lists have the same length, and that the order for both of these matches so that the header at “headers[2]”, or instance, is the header corresponding with the value over at “values[2]”. Since we’re dealing with rather simple tables here, this is a safe assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Are Tables Worth It?** You might not be very impressed with this example so\n",
    "far. Most modern browsers allow you to simply select or right-click tables on web\n",
    "pages and will be able to copy them straight into a spreadsheet program such as\n",
    "Excel anyway. That’s true, and if you only have one table to extract, this is definitely\n",
    "the easier route to follow. Once you start dealing with many tables, however,\n",
    "especially if they’re spread over multiple pages, or need to periodically refresh\n",
    "tabular data from a particular web page, the benefit of writing a scraper starts to\n",
    "become more apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from web - pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Odprti podatki Slovenije](https://podatki.gov.si/)\n",
    "\n",
    "\n",
    "Na portalu OPSI boste našli vse od podatkov, orodij, do koristnih virov, s katerimi boste lahko razvijali spletne in mobilne aplikacije, oblikovali lastne infografike in drugo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer: https://support.spatialkey.com/spatialkey-sample-csv-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_ebola.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31 entries, 0 to 30\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Country  31 non-null     object \n",
      " 1   Month    31 non-null     int64  \n",
      " 2   Year     31 non-null     int64  \n",
      " 3   Lat      31 non-null     float64\n",
      " 4   Lon      31 non-null     float64\n",
      " 5   Value    22 non-null     float64\n",
      "dtypes: float64(3), int64(2), object(1)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has a built-in function, read_html , which uses libraries like lxml and Beauti‐\n",
    "ful Soup to automatically parse tables out of HTML files as DataFrame objects. To\n",
    "show how this works, I downloaded an HTML file (used in the pandas documenta‐\n",
    "tion) from the United States FDIC government agency showing bank failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Spletna stran: https://www.fdic.gov/bank/individual/failed/banklist.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.read_html: ` Read HTML tables into a list of DataFrame objects. -> [Dokumentacija](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas.read_html function has a number of options, but by default it searches\n",
    "for and attempts to parse all tabular data contained within `<table>` tags. The result is a list of DataFrame objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.6.1-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.8/site-packages (from html5lib) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from html5lib) (0.5.1)\n",
      "Installing collected packages: html5lib\n",
      "Successfully installed html5lib-1.1\n"
     ]
    }
   ],
   "source": [
    "! pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html('https://www.fdic.gov/bank/individual/failed/banklist.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 563 entries, 0 to 562\n",
      "Data columns (total 6 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Bank Name              563 non-null    object\n",
      " 1   City                   563 non-null    object\n",
      " 2   ST                     563 non-null    object\n",
      " 3   CERT                   563 non-null    int64 \n",
      " 4   Acquiring Institution  563 non-null    object\n",
      " 5   Closing Date           563 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 26.5+ KB\n"
     ]
    }
   ],
   "source": [
    "failures.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>City</th>\n",
       "      <th>ST</th>\n",
       "      <th>CERT</th>\n",
       "      <th>Acquiring Institution</th>\n",
       "      <th>Closing Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Almena State Bank</td>\n",
       "      <td>Almena</td>\n",
       "      <td>KS</td>\n",
       "      <td>15426</td>\n",
       "      <td>Equity Bank</td>\n",
       "      <td>October 23, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First City Bank of Florida</td>\n",
       "      <td>Fort Walton Beach</td>\n",
       "      <td>FL</td>\n",
       "      <td>16748</td>\n",
       "      <td>United Fidelity Bank, fsb</td>\n",
       "      <td>October 16, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The First State Bank</td>\n",
       "      <td>Barboursville</td>\n",
       "      <td>WV</td>\n",
       "      <td>14361</td>\n",
       "      <td>MVB Bank, Inc.</td>\n",
       "      <td>April 3, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ericson State Bank</td>\n",
       "      <td>Ericson</td>\n",
       "      <td>NE</td>\n",
       "      <td>18265</td>\n",
       "      <td>Farmers and Merchants Bank</td>\n",
       "      <td>February 14, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>City National Bank of New Jersey</td>\n",
       "      <td>Newark</td>\n",
       "      <td>NJ</td>\n",
       "      <td>21111</td>\n",
       "      <td>Industrial Bank</td>\n",
       "      <td>November 1, 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Bank Name               City  ST   CERT  \\\n",
       "0                 Almena State Bank             Almena  KS  15426   \n",
       "1        First City Bank of Florida  Fort Walton Beach  FL  16748   \n",
       "2              The First State Bank      Barboursville  WV  14361   \n",
       "3                Ericson State Bank            Ericson  NE  18265   \n",
       "4  City National Bank of New Jersey             Newark  NJ  21111   \n",
       "\n",
       "        Acquiring Institution       Closing Date  \n",
       "0                 Equity Bank   October 23, 2020  \n",
       "1   United Fidelity Bank, fsb   October 16, 2020  \n",
       "2              MVB Bank, Inc.      April 3, 2020  \n",
       "3  Farmers and Merchants Bank  February 14, 2020  \n",
       "4             Industrial Bank   November 1, 2019  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will learn in later chapters, from here we could proceed to do some data\n",
    "cleaning and analysis, like computing the number of bank failures by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_timestamps = pd.to_datetime(failures['Closing Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010    157\n",
       "2009    140\n",
       "2011     92\n",
       "2012     51\n",
       "2008     25\n",
       "2013     24\n",
       "2014     18\n",
       "2002     11\n",
       "2017      8\n",
       "2015      8\n",
       "2016      5\n",
       "2001      4\n",
       "2019      4\n",
       "2004      4\n",
       "2007      3\n",
       "2003      3\n",
       "2020      2\n",
       "2000      2\n",
       "Name: Closing Date, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_timestamps.dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and Visualizing IMDB Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next series of examples moves on toward including some more data science-­\n",
    "oriented use cases. We’re going to start simple by scraping a list of reviews for episodes\n",
    "of a TV series, using IMDB (the Internet Movie Database). We’ll use Game of Thrones as\n",
    "an example, the episode list for which can be found at http://www.imdb.com/title/tt0944947/episodes. Note that IMDB’s overview is spread out across multiple pages\n",
    "(per season or per year), so we iterate over the seasons we want to retrieve using an\n",
    "extra loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://www.imdb.com/title/tt0944947/episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episodes = []\n",
    "ratings = []\n",
    "\n",
    "# Go over seasons 1 to 8\n",
    "for season in range(1,9):\n",
    "    r = requests.get(url, params={'season': season})\n",
    "    #print(r.status_code) # 1 korak preverimo če dela\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    listing = soup.find('div', class_='eplist')\n",
    "    #print(listing) # 2 korak preverimo če dela\n",
    "    for epnr, div in enumerate(listing.find_all('div', recursive= False)):\n",
    "        episode = f'{season}.{epnr + 1}'\n",
    "        rating_el = div.find(class_='ipl-rating-star__rating')\n",
    "        #print(episode, rating_el)\n",
    "        #print('-----------------')\n",
    "        rating = float(rating_el.get_text(strip=True))\n",
    "        #print('Episode: ', episode, ' --rating:', rating)\n",
    "        episodes.append(episode)\n",
    "        ratings.append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.1, 8.8, 8.7, 8.8, 9.1, 9.2, 9.2, 9.0, 9.6, 9.5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', '1.9', '1.10']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the scraped ratings using “matplotlib”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "positions = [a for a in range(len(ratings))]\n",
    "plt.bar(positions, ratings, align='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Fast Track data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Vir](https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stran: https://www.fasttrack.co.uk/league-tables/tech-track-100/league-table/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the League Table webpage, a table containing 100 results is displayed. When inspecting the page it is easy to see a pattern in the html. The results are contained in rows within the table:\n",
    "\n",
    "`<table class=\"tableSorter\">`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the url\n",
    "urlpage =  'http://www.fasttrack.co.uk/league-tables/tech-track-100/league-table/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the website and return the html to the variable 'page'\n",
    "page = requests.get(urlpage)\n",
    "# parse the html using beautiful soup and store in variable 'soup'\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results 101\n"
     ]
    }
   ],
   "source": [
    "# find results within table\n",
    "table = soup.find('table', attrs={'class': 'tableSorter2'})\n",
    "results = table.find_all('tr')\n",
    "print('Number of results', len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr>\n",
       "<th>Rank</th>\n",
       "<th>Company</th>\n",
       "<th class=\"\">Location</th>\n",
       "<th class=\"no-word-wrap\">Year end</th>\n",
       "<th class=\"\" style=\"text-align:right;\">Annual sales rise over 3 years</th>\n",
       "<th class=\"\" style=\"text-align:right;\">Latest sales £000s</th>\n",
       "<th class=\"\" style=\"text-align:right;\">Staff</th>\n",
       "<th class=\"\">Comment</th>\n",
       "<!--\t\t\t\t<th>FYE</th>-->\n",
       "</tr>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr>\n",
       "<td>1</td>\n",
       "<td><a href=\"https://www.fasttrack.co.uk/company_profile/revolut-2/\"><span class=\"company-name\">Revolut</span></a>Digital banking services provider</td>\n",
       "<td>East London</td>\n",
       "<td>Dec 18</td>\n",
       "<td style=\"text-align:right;\">507.56%</td>\n",
       "<td style=\"text-align:right;\">*58,300</td>\n",
       "<td style=\"text-align:right;\">700</td>\n",
       "<td>Valued at $1.7bn in 2018 and reported to be raising an additional $500m this year that could value it at $5bn</td>\n",
       "<!--\t\t\t\t\t\t<td>Dec 18</td>-->\n",
       "</tr>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and write headers to a list \n",
    "rows = []\n",
    "\n",
    "for row in results[0].find_all('th'):\n",
    "    rows.append(row.contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rank', 'Company', 'Location', 'Year end', 'Annual sales rise over 3 years', 'Latest sales £000s', 'Staff', 'Comment']\n"
     ]
    }
   ],
   "source": [
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Rank', 'Company Name', 'Webpage', 'Description', 'Location', 'Year end', 'Annual sales rise over 3 years', 'Sales £000s', 'Staff', 'Comments']]\n"
     ]
    }
   ],
   "source": [
    "# create and write headers to a list \n",
    "rows = []\n",
    "rows.append(['Rank', 'Company Name', 'Webpage', 'Description', 'Location', 'Year end', 'Annual sales rise over 3 years', 'Sales £000s', 'Staff', 'Comments'])\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to loop over the results, process the data and append to rows which can be written to a csv.\n",
    "\n",
    "To find the results in the loop:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over results\n",
    "for result in results:\n",
    "    # find all columns per result\n",
    "    data = result.find_all('td')\n",
    "    # check that columns have data \n",
    "    if len(data) == 0: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td>100</td>,\n",
       " <td><a href=\"https://www.fasttrack.co.uk/company_profile/dianomi-5/\"><span class=\"company-name\">Dianomi</span></a>Financial marketing platform</td>,\n",
       " <td>Central London</td>,\n",
       " <td>Dec 18</td>,\n",
       " <td style=\"text-align:right;\">49.21%</td>,\n",
       " <td style=\"text-align:right;\">*14,615</td>,\n",
       " <td style=\"text-align:right;\">27</td>,\n",
       " <td>Works with eight of the top 10 global asset managers</td>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write columns to variables\n",
    "rank = data[0].getText()\n",
    "company = data[1].getText()\n",
    "location = data[2].getText()\n",
    "yearend = data[3].getText()\n",
    "salesrise = data[4].getText()\n",
    "sales = data[5].getText()\n",
    "staff = data[6].getText()\n",
    "comments = data[7].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DianomiFinancial marketing platform'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*14,615'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract description from the name\n",
    "companyname = data[1].find('span', attrs={'class':'company-name'}).getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dianomi'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companyname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = company.replace(companyname, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Financial marketing platform'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted characters\n",
    "sales = sales.strip('*').strip('†').replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14615'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last variable we would like to save is the company website. As discussed above, the second column contains a link to another page that has an overview of each company. Each company page has it’s own table, which most of the time contains the company website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<td><a href=\"https://www.fasttrack.co.uk/company_profile/dianomi-5/\"><span class=\"company-name\">Dianomi</span></a>Financial marketing platform</td>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a few of the company pages, as in the screenshot above, the urls are in last row in the table so we can search within the last row for the `<a>` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to link and extract company website\n",
    "url = data[1].find('a').get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.fasttrack.co.uk/company_profile/dianomi-5/'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url)\n",
    "# parse the html \n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the last result in the table and get the link\n",
    "try:\n",
    "    tableRow = soup.find('table').find_all('tr')[-1]\n",
    "    webpage = tableRow.find('a').get('href')\n",
    "except:\n",
    "    webpage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.dianomi.com'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to save this data for analysis and this can be done very simply within python from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv and write rows to output file\n",
    "with open('techtrack100.csv','w', newline='') as f_output:\n",
    "    csv_output = csv.writer(f_output)\n",
    "    csv_output.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank,Company Name,Webpage,Description,Location,Year end,Annual sales rise over 3 years,Sales £000s,Staff,Comments\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat techtrack100.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Celotni program skupaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# specify the url\n",
    "urlpage =  'http://www.fasttrack.co.uk/league-tables/tech-track-100/league-table/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the website and return the html to the variable 'page'\n",
    "page = requests.get(urlpage)\n",
    "# parse the html using beautiful soup and store in variable 'soup'\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results 101\n"
     ]
    }
   ],
   "source": [
    "# find results within table\n",
    "table = soup.find('table', attrs={'class': 'tableSorter2'})\n",
    "results = table.find_all('tr')\n",
    "print('Number of results', len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and write headers to a list \n",
    "rows = []\n",
    "rows.append(['Rank', 'Company Name', 'Webpage', \n",
    "             'Description', 'Location', 'Year end', \n",
    "             'Annual sales rise over 3 years', 'Sales £000s', \n",
    "             'Staff', 'Comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Company is Revolut\n",
      "2 - Company is Bizuma\n",
      "3 - Company is Global-e\n",
      "4 - Company is Jungle Creations\n",
      "5 - Company is Oxford Nanopore Technologies\n",
      "6 - Company is Lendable\n",
      "7 - Company is Verve\n",
      "8 - Company is Onfido\n",
      "9 - Company is Reverse Media Group\n",
      "10 - Company is Rebound Returns\n",
      "11 - Company is ClearScore\n",
      "12 - Company is SPOKE\n",
      "13 - Company is FSB\n",
      "14 - Company is Decibel\n",
      "15 - Company is Miss Group\n",
      "16 - Company is ComplyAdvantage\n",
      "17 - Company is OTA Insight\n",
      "18 - Company is Light Source\n",
      "19 - Company is Digi2al\n",
      "20 - Company is Perkbox\n",
      "21 - Company is Lending Works\n",
      "22 - Company is Faculty\n",
      "23 - Company is Internet Mobile Communications\n",
      "24 - Company is Fruugo\n",
      "25 - Company is Featurespace\n",
      "26 - Company is Monterosa\n",
      "27 - Company is Lockwood Publishing\n",
      "28 - Company is Plan.com\n",
      "29 - Company is Capital on Tap\n",
      "30 - Company is In Touch Networks\n",
      "31 - Company is DV Signage\n",
      "32 - Company is Ogury\n",
      "33 - Company is Chameleon Technology\n",
      "34 - Company is iwoca\n",
      "35 - Company is Byte\n",
      "36 - Company is Matillion\n",
      "37 - Company is Bought By Many\n",
      "38 - Company is Checkout.com\n",
      "39 - Company is Fleximize\n",
      "40 - Company is Hyperdrive Innovation\n",
      "41 - Company is GoCardless\n",
      "42 - Company is Azuri Technologies\n",
      "43 - Company is Receipt Bank\n",
      "44 - Company is gohenry\n",
      "45 - Company is Gousto\n",
      "46 - Company is Depop\n",
      "47 - Company is Giacom\n",
      "48 - Company is Account Technologies\n",
      "49 - Company is Gigaclear\n",
      "50 - Company is Oakbrook\n",
      "51 - Company is Hyperoptic\n",
      "52 - Company is TransferWise\n",
      "53 - Company is Assetz Capital\n",
      "54 - Company is Darktrace\n",
      "55 - Company is Payen\n",
      "56 - Company is Thread\n",
      "57 - Company is SuperAwesome\n",
      "58 - Company is Vizolution\n",
      "59 - Company is BigChange\n",
      "60 - Company is Symetrica\n",
      "61 - Company is StarLeaf\n",
      "62 - Company is Azimo\n",
      "63 - Company is Endomag\n",
      "64 - Company is QHi Group\n",
      "65 - Company is Threads Styling\n",
      "66 - Company is Unify Communications\n",
      "67 - Company is MiQ\n",
      "68 - Company is Neyber\n",
      "69 - Company is TVSquared\n",
      "70 - Company is The Access Group\n",
      "71 - Company is ContactEngine\n",
      "72 - Company is Hutch\n",
      "73 - Company is Sideshow\n",
      "74 - Company is Optal\n",
      "75 - Company is Leisure Pass Group\n",
      "76 - Company is Brompton Technology\n",
      "77 - Company is iPlato\n",
      "78 - Company is Currencycloud\n",
      "79 - Company is Tharsus\n",
      "80 - Company is ENSEK\n",
      "81 - Company is Captify\n",
      "82 - Company is Adaptive\n",
      "83 - Company is Student Beans\n",
      "84 - Company is Godel Technologies\n",
      "85 - Company is MPB\n",
      "86 - Company is Smartsearch\n",
      "87 - Company is Liberis\n",
      "88 - Company is cloudThing\n",
      "89 - Company is Zappi\n",
      "90 - Company is Ticketer\n",
      "91 - Company is Pharmacy 2U\n",
      "92 - Company is Adaptavist\n",
      "93 - Company is Alternative Airlines\n",
      "94 - Company is Biosite Systems\n",
      "95 - Company is JustPark\n",
      "96 - Company is WorldRemit\n",
      "97 - Company is E3D Online\n",
      "98 - Company is Parentpay\n",
      "99 - Company is Victor\n",
      "100 - Company is Dianomi\n"
     ]
    }
   ],
   "source": [
    "#loop over results\n",
    "for num, result in enumerate(results):\n",
    "    # find all columns per result\n",
    "    data = result.find_all('td')\n",
    "    # check that columns have data \n",
    "    if len(data) == 0: \n",
    "        continue\n",
    "        \n",
    "    # write columns to variables\n",
    "    rank = data[0].getText()\n",
    "    company = data[1].getText()\n",
    "    location = data[2].getText()\n",
    "    yearend = data[3].getText()\n",
    "    salesrise = data[4].getText()\n",
    "    sales = data[5].getText()\n",
    "    staff = data[6].getText()\n",
    "    comments = data[7].getText() \n",
    "    \n",
    "    #print('Company is', company)\n",
    "    # Company is WonderblyPersonalised children's books\n",
    "    # print('Sales', sales)\n",
    "    # Sales *25,860\n",
    "\n",
    "    # extract description from the name\n",
    "    companyname = data[1].find('span', attrs={'class':'company-name'}).getText()    \n",
    "    description = company.replace(companyname, '')\n",
    "    print(num, '- Company is', companyname)\n",
    "    \n",
    "    # remove unwanted characters\n",
    "    sales = sales.strip('*').strip('†').replace(',','')\n",
    "    \n",
    "    # go to link and extract company website\n",
    "    url = data[1].find('a').get('href')\n",
    "    page = requests.get(url)\n",
    "    # parse the html using beautiful soup and store in variable 'soup'\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    # find the last result in the table and get the link\n",
    "    try:\n",
    "        tableRow = soup.find('table').find_all('tr')[-1]\n",
    "        webpage = tableRow.find('a').get('href')\n",
    "    except:\n",
    "        webpage = None\n",
    "    \n",
    "    # write each result to rows\n",
    "    rows.append([rank, companyname, webpage, description, location, yearend, salesrise, sales, staff, comments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'Revolut',\n",
       " 'http://www.revolut.com',\n",
       " 'Digital banking services provider',\n",
       " 'East London',\n",
       " 'Dec 18',\n",
       " '507.56%',\n",
       " '58300',\n",
       " '700',\n",
       " 'Valued at $1.7bn in 2018 and reported to be raising an additional $500m this year that could value it at $5bn']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create csv and write rows to output file\n",
    "with open('OUT_techtrack100.csv','w', newline='') as f_output:\n",
    "    csv_output = csv.writer(f_output)\n",
    "    csv_output.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OUT_techtrack100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Webpage</th>\n",
       "      <th>Description</th>\n",
       "      <th>Location</th>\n",
       "      <th>Year end</th>\n",
       "      <th>Annual sales rise over 3 years</th>\n",
       "      <th>Sales £000s</th>\n",
       "      <th>Staff</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Revolut</td>\n",
       "      <td>http://www.revolut.com</td>\n",
       "      <td>Digital banking services provider</td>\n",
       "      <td>East London</td>\n",
       "      <td>Dec 18</td>\n",
       "      <td>507.56%</td>\n",
       "      <td>58300</td>\n",
       "      <td>700</td>\n",
       "      <td>Valued at $1.7bn in 2018 and reported to be ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bizuma</td>\n",
       "      <td>http://www.bizuma.com</td>\n",
       "      <td>B2B e-commerce platform</td>\n",
       "      <td>Central London</td>\n",
       "      <td>Mar 19</td>\n",
       "      <td>315.18%</td>\n",
       "      <td>26414</td>\n",
       "      <td>114</td>\n",
       "      <td>Connects wholesale buyers and sellers from ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Global-e</td>\n",
       "      <td>http://www.global-e.com</td>\n",
       "      <td>Cross-border ecommerce solutions</td>\n",
       "      <td>Central London</td>\n",
       "      <td>Dec 18</td>\n",
       "      <td>303.09%</td>\n",
       "      <td>29297</td>\n",
       "      <td>28</td>\n",
       "      <td>Its technology helps ecommerce retailers local...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jungle Creations</td>\n",
       "      <td>http://www.junglecreations.com</td>\n",
       "      <td>Social media &amp; ecommerce services</td>\n",
       "      <td>East London</td>\n",
       "      <td>Dec 18</td>\n",
       "      <td>302.53%</td>\n",
       "      <td>15972</td>\n",
       "      <td>159</td>\n",
       "      <td>Launched the first-ever delivery-only restaura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Oxford Nanopore Technologies</td>\n",
       "      <td>http://www.nanoporetech.com</td>\n",
       "      <td>DNA analysis technology</td>\n",
       "      <td>Oxford</td>\n",
       "      <td>Dec 18</td>\n",
       "      <td>251.87%</td>\n",
       "      <td>32500</td>\n",
       "      <td>439</td>\n",
       "      <td>Has raised £451m in funding and was valued at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                  Company Name                         Webpage  \\\n",
       "0     1                       Revolut          http://www.revolut.com   \n",
       "1     2                        Bizuma           http://www.bizuma.com   \n",
       "2     3                      Global-e         http://www.global-e.com   \n",
       "3     4              Jungle Creations  http://www.junglecreations.com   \n",
       "4     5  Oxford Nanopore Technologies     http://www.nanoporetech.com   \n",
       "\n",
       "                         Description        Location Year end  \\\n",
       "0  Digital banking services provider     East London   Dec 18   \n",
       "1            B2B e-commerce platform  Central London   Mar 19   \n",
       "2   Cross-border ecommerce solutions  Central London   Dec 18   \n",
       "3  Social media & ecommerce services     East London   Dec 18   \n",
       "4            DNA analysis technology          Oxford   Dec 18   \n",
       "\n",
       "  Annual sales rise over 3 years  Sales £000s Staff  \\\n",
       "0                        507.56%        58300   700   \n",
       "1                        315.18%        26414   114   \n",
       "2                        303.09%        29297    28   \n",
       "3                        302.53%        15972   159   \n",
       "4                        251.87%        32500   439   \n",
       "\n",
       "                                            Comments  \n",
       "0  Valued at $1.7bn in 2018 and reported to be ra...  \n",
       "1  Connects wholesale buyers and sellers from ove...  \n",
       "2  Its technology helps ecommerce retailers local...  \n",
       "3  Launched the first-ever delivery-only restaura...  \n",
       "4  Has raised £451m in funding and was valued at ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
