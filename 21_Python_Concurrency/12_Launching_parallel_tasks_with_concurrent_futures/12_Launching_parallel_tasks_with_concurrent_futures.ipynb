{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0025a0c",
   "metadata": {},
   "source": [
    "# Launching parallel tasks with concurrent futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83beb409",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/concurrent.futures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e560a5b",
   "metadata": {},
   "source": [
    "Python standard library also houses a module called the concurrent.futures. This module was added in Python 3.2 for **providing the developers a high-level interface to launch asynchronous tasks**. It’s a generalized abstraction layer on top of threading and multiprocessing modules for providing an interface to run tasks concurrently using pools of threads or processes. It’s the perfect tool when you just want to run a piece of eligible code concurrently and **don’t need the added modularity that the threading and multiprocessing APIs expose**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9bb0e",
   "metadata": {},
   "source": [
    "The  `concurrent.futures` module provides a high-level interface for asynchronously executing callables.\n",
    "\n",
    "The asynchronous execution can be performed with threads, using `ThreadPoolExecutor`, or separate processes, using `ProcessPoolExecutor`. Both implement the same interface, which is defined by the abstract Executor class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acefa268",
   "metadata": {},
   "source": [
    "Internally, **these two classes interact with the pools and manage the workers**. Futures are used for managing results computed by the workers. To use a pool of workers:\n",
    "1. an application creates an instance of the appropriate executor class and then submits them for it to run. \n",
    "2. When each task is started, a Future instance is returned. \n",
    "    - When the result of the task is needed, an application can use the Future object to block until the result is available. \n",
    "   \n",
    "Various APIs are provided to make it convenient to wait for tasks to complete, so that the Future objects do not need to be managed directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5ce2f",
   "metadata": {},
   "source": [
    "## Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf42855",
   "metadata": {},
   "source": [
    "This module features the `Executor` class which is an abstract class and it can not be used directly. However it has two very useful concrete subclasses – `ThreadPoolExecutor` and `ProcessPoolExecutor`. As their names suggest, one uses multi threading and the other one uses multi-processing. In both case, we get a pool of threads or processes and we can submit tasks to this pool. The pool would assign tasks to the available resources (threads or processes) and schedule them to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2e36e",
   "metadata": {},
   "source": [
    "Since both ThreadPoolExecutor and ProcessPoolExecutor have the same API interface, in both cases I’ll primarily talk about two methods that they provide. Their descriptions have been collected from the official docs verbatim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a844a",
   "metadata": {},
   "source": [
    "### submit(fn, args, *kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f2ea47",
   "metadata": {},
   "source": [
    "> `submit(fn, /, *args, **kwargs)`: Schedules the callable, fn, to be executed as fn(*args, **kwargs) and returns a Future object representing the execution of the callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80e7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "Resutl 1: hello1\n",
      "Resutl 2: hello2\n"
     ]
    }
   ],
   "source": [
    "# 01_example.py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    "\n",
    "def return_after_5_secs(message):\n",
    "    sleep(5)\n",
    "    return message\n",
    "\n",
    "pool = ThreadPoolExecutor(3)\n",
    "\n",
    "future1 = pool.submit(return_after_5_secs, (\"hello1\"))\n",
    "future2 = pool.submit(return_after_5_secs, (\"hello2\"))\n",
    "\n",
    "print(future1.done())\n",
    "sleep(6)\n",
    "print(future1.done())\n",
    "\n",
    "print(f\"Resutl 1: {future1.result()}\")\n",
    "print(f\"Resutl 2: {future2.result()}\")\n",
    "pool.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679def1c",
   "metadata": {},
   "source": [
    "I hope the code is pretty self explanatory. We first construct a ThreadPoolExecutor with the number of threads we want in the pool. By default the number is 5 but we chose to use 3 just because we can ;-). Then we submitted a task to the thread pool executor which waits 5 seconds before returning the message it gets as it’s first argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a5095",
   "metadata": {},
   "source": [
    "**When we `submit()` a task, we get back a `Future`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d50091",
   "metadata": {},
   "source": [
    "As we can see in the docs, the `Future` object has a method – `done()` which tells us if the future has resolved, that is a value has been set for that particular future object. When a task finishes (returns a value or is interrupted by an exception), the thread pool executor sets the value to the future object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047e58f",
   "metadata": {},
   "source": [
    "In our example, the task doesn’t complete until 5 seconds, so the first call to `done()` will return `False`. We take a really short nap for 5 secs and then it’s done. We can get the result of the future by calling the `result()` method on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8126be",
   "metadata": {},
   "source": [
    "> `done()`: Return True if the call was successfully cancelled or finished running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737c73d",
   "metadata": {},
   "source": [
    "> `result(timeout=None)`: Return the value returned by the call. If the call hasn’t yet completed then this method will wait up to timeout seconds. If the call hasn’t completed in timeout seconds, then a `concurrent.futures.TimeoutError` will be raised. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time. If the future is cancelled before completing then CancelledError will be raised. If the call raised an exception, this method will raise the same exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a17a4",
   "metadata": {},
   "source": [
    "> `shutdown(wait=True, *, cancel_futures=False)`: Signal the executor that it should free any resources that it is using when the currently pending futures are done executing. Calls to Executor.submit() and Executor.map() made after shutdown will raise RuntimeError."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0dd2aa",
   "metadata": {},
   "source": [
    "### map(func, *iterables, timeout=None, chunksize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c75778",
   "metadata": {},
   "source": [
    "Similar to map(func, *iterables) except:\n",
    "- the iterables are collected immediately rather than lazily;\n",
    "- func is executed asynchronously and several calls to func may be made concurrently.\n",
    "\n",
    "The returned iterator raises a `concurrent.futures.TimeoutError` if `__next__()` is called and the result isn’t available after timeout seconds from the original call to `Executor.map()`. Timeout can be an int or a float. If timeout is not specified or None, there is no limit to the wait time.\n",
    "\n",
    "If a func call raises an exception, then that exception will be raised when its value is retrieved from the iterator.\n",
    "\n",
    "> When using ProcessPoolExecutor, this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For **very long iterables, using a large value for chunksize can significantly improve performance** compared to the default size of 1. **With `ThreadPoolExecutor`, chunksize has no effect**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04eda3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello1\n",
      "hello2\n"
     ]
    }
   ],
   "source": [
    "# 02_example.py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    "\n",
    "def return_after_5_secs(message):\n",
    "    sleep(5)\n",
    "    return message\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = executor.map(return_after_5_secs, (\"hello1\", \"hello2\"))\n",
    "    for result in results:\n",
    "        print(result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082f9ed",
   "metadata": {},
   "source": [
    "Both executors have a common method – `map()`. Like the built in function, the **map method allows multiple calls to a provided function, passing each of the items in an iterable to that function**. Except, in this case, the functions are called concurrently. For multiprocessing, this iterable is broken into chunks and each of these chunks is passed to the function in separate processes. We can control the chunk size by passing a third parameter, `chunk_size`. By default the chunk size is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdac16",
   "metadata": {},
   "source": [
    "## Generic Workflows for Running Tasks Concurrently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b91a25",
   "metadata": {},
   "source": [
    "A lot of scripts contains some variants of the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca86f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in get_tasks():\n",
    "    perform(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cf10c",
   "metadata": {},
   "source": [
    "Here, `get_tasks` returns an iterable that contains the target tasks or arguments on which a particular task function needs to applied. **Tasks are usually blocking callables and they run one after another**, with only one task running at a time. The logic is simple to reason with because of its sequential execution flow. This is fine when the number of tasks is small or the execution time requirement and complexity of the individual tasks is low. However, this can quickly get out of hands when the number of tasks is huge or the individual tasks are time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874f5d6",
   "metadata": {},
   "source": [
    "A general rule of thumb is using:\n",
    "- `ThreadPoolExecutor` when the tasks are primarily I/O bound like - sending multiple http requests to many urls, saving a large number of files to disk etc. \n",
    "- `ProcessPoolExecutor` should be used in tasks that are primarily CPU bound like - running callables that are computation heavy, applying pre-process methods over a large number of images, manipulating many text files at once etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe631e",
   "metadata": {},
   "source": [
    "### Running Tasks with Executor.submit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84773dc4",
   "metadata": {},
   "source": [
    "When you have a number of tasks, you can schedule them in one go and wait for them all to complete and then you can collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f747fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power is 0\n",
      "The power is 1\n",
      "The power is 4\n",
      "The power is 9\n",
      "The power is 25\n",
      "The power is 36\n",
      "The power is 49\n",
      "The power is 16\n",
      "The power is 64\n",
      "The power is 81\n",
      "The power is 100\n",
      "The power is 121\n",
      "The power is 144\n",
      "The power is 169\n",
      "The power is 196\n",
      "The power is 289\n",
      "The power is 256\n",
      "The power is 225\n",
      "The power is 324\n",
      "The power is 361\n"
     ]
    }
   ],
   "source": [
    "# 03_example.py\n",
    "import time\n",
    "from concurrent import futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_tasks():\n",
    "    return list(range(20))\n",
    "    \n",
    "\n",
    "def perform(number):\n",
    "    time.sleep(1)\n",
    "    return number * number\n",
    "    \n",
    "    \n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures_set = {executor.submit(perform, task) for task in get_tasks()}\n",
    "\n",
    "    for fut in futures.as_completed(futures_set):\n",
    "        print(f\"The power is {fut.result()}\")\n",
    "        # vidimo ni vrstnega reda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207505a3",
   "metadata": {},
   "source": [
    "Here you start by creating an Executor, which manages all the tasks that are running – either in separate processes or threads. **Using the with statement creates a context manager, which ensures any stray threads or processes get cleaned up via calling** the `executor.shutdown()` method implicitly when you’re done.\n",
    "\n",
    "Then a **set comprehension has been used here to start all the tasks**. The `executor.submit()` method schedules each task. **This creates a Future object, which represents the task to be done.** Once all the tasks have been scheduled, the method `concurrent.futures_as_completed()` is called, which **yields the futures as they’re done** – that is, as each task completes. The `fut.result()` method **gives you the return value of perform(task)**, or throws an exception in case of failure.\n",
    "\n",
    "The `executor.submit()` method **schedules the tasks asynchronously and doesn’t hold any contexts regarding the original tasks**. So if you want to map the results with the original tasks, you need to track those yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd0b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power of 0 is 0\n",
      "The power of 1 is 1\n",
      "The power of 2 is 4\n",
      "The power of 3 is 9\n",
      "The power of 4 is 16\n",
      "The power of 5 is 25\n",
      "The power of 6 is 36\n",
      "The power of 7 is 49\n",
      "The power of 8 is 64\n",
      "The power of 9 is 81\n",
      "The power of 10 is 100\n",
      "The power of 11 is 121\n",
      "The power of 13 is 169\n",
      "The power of 12 is 144\n",
      "The power of 14 is 196\n",
      "The power of 15 is 225\n",
      "The power of 16 is 256\n",
      "The power of 17 is 289\n",
      "The power of 18 is 324\n",
      "The power of 19 is 361\n"
     ]
    }
   ],
   "source": [
    "# 04_example.py\n",
    "import time\n",
    "from concurrent import futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def get_tasks():\n",
    "    return list(range(20))\n",
    "    \n",
    "\n",
    "def perform(number):\n",
    "    time.sleep(1)\n",
    "    return number * number\n",
    "    \n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures_set = {executor.submit(perform, task): task for task in get_tasks()}\n",
    "\n",
    "    for fut in futures.as_completed(futures_set):\n",
    "        original_task = futures_set[fut]\n",
    "        print(f\"The power of {original_task} is {fut.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92191c91",
   "metadata": {},
   "source": [
    "Notice the variable futures where the original tasks are mapped with their corresponding futures using a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecc122",
   "metadata": {},
   "source": [
    "### Running Tasks with Executor.map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95345da2",
   "metadata": {},
   "source": [
    "Another way the results can be collected in the same order they’re scheduled is via using executor.map() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ba65b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The power of 0 is 0\n",
      "The power of 1 is 1\n",
      "The power of 2 is 4\n",
      "The power of 3 is 9\n",
      "The power of 4 is 16\n",
      "The power of 5 is 25\n",
      "The power of 6 is 36\n",
      "The power of 7 is 49\n",
      "The power of 8 is 64\n",
      "The power of 9 is 81\n",
      "The power of 10 is 100\n",
      "The power of 11 is 121\n",
      "The power of 12 is 144\n",
      "The power of 13 is 169\n",
      "The power of 14 is 196\n",
      "The power of 15 is 225\n",
      "The power of 16 is 256\n",
      "The power of 17 is 289\n",
      "The power of 18 is 324\n",
      "The power of 19 is 361\n"
     ]
    }
   ],
   "source": [
    "# 05_example.py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def get_tasks():\n",
    "    return list(range(20))\n",
    "    \n",
    "\n",
    "def perform(number):\n",
    "    time.sleep(1)\n",
    "    return number * number\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for arg, res in zip(get_tasks(), executor.map(perform, get_tasks())):\n",
    "        print(f\"The power of {arg} is {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582aefc9",
   "metadata": {},
   "source": [
    "Notice how the map function takes the entire iterable at once. It **spits out the results immediately rather than lazily and in the same order they’re scheduled**. If any unhandled exception occurs during the operation, it’ll also be raised immediately and the execution won’t go any further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce825802",
   "metadata": {},
   "source": [
    "In Python 3.5+, `executor.map()` receives an optional argument: `chunksize`. While using `ProcessPoolExecutor`, for very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With `ThreadPoolExecutor`, chunksize has no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd416d",
   "metadata": {},
   "source": [
    "### wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d71aa",
   "metadata": {},
   "source": [
    "The `wait()` function would return a named tuple which contains two set – one set contains the futures which completed (either got result or exception) and the other set containing the ones which didn’t complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e1559",
   "metadata": {},
   "source": [
    "> `concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED)`: Wait for the Future instances (possibly created by different Executor instances) given by fs to complete. Duplicate futures given to fs are removed and will be returned only once. Returns a named 2-tuple of sets. The first set, named done, contains the futures that completed (finished or cancelled futures) before the wait completed. The second set, named not_done, contains the futures that did not complete (pending or running futures).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "238cf929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "def return_after_5_secs(num):\n",
    "    sleep(randint(1, 5))\n",
    "    if num == 3:\n",
    "        raise ValueError\n",
    "    return f\"Return of {num}\"\n",
    "\n",
    "pool = ThreadPoolExecutor(5)\n",
    "futures = []\n",
    "\n",
    "for x in range(5):\n",
    "    futures.append(pool.submit(return_after_5_secs, x))\n",
    "\n",
    "done_futures = wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b80dbe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<Future at 0x7fc1f63a6a00 state=finished returned str>, <Future at 0x7fc1f63a6c70 state=finished returned str>, <Future at 0x7fc1f6fc1e80 state=finished returned str>, <Future at 0x7fc1f6550550 state=finished returned str>, <Future at 0x7fc1f63a23a0 state=finished raised ValueError>}\n"
     ]
    }
   ],
   "source": [
    "print(done_futures.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec732b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return of 4\n",
      "Return of 1\n",
      "Return of 2\n",
      "Return of 0\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "for df in done_futures.done:\n",
    "    try:\n",
    "        print(df.result())\n",
    "    except ValueError as e:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ed7a3",
   "metadata": {},
   "source": [
    "We can control the behavior of the `wait` function by defining when it should return. We can pass one of these values to the `return_when` param of the function: `FIRST_COMPLETED`, `FIRST_EXCEPTION` and `ALL_COMPLETED`. By default, it’s set to `ALL_COMPLETED`, so the wait function returns only when all futures complete. But using that parameter, we can choose to return when the first future completes or first exception encounters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddb981",
   "metadata": {},
   "source": [
    "## Download & Save Files from URLs with Multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f0950",
   "metadata": {},
   "source": [
    "Before proceeding with the examples, let’s write a small decorator that’ll be helpful to measure and compare the execution time between concurrent and sequential code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e7c45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06_example.py\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def timeit(method):\n",
    "    @wraps(method)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{method.__name__} => {(end_time-start_time)*1000} ms\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d064c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func => 0.03600120544433594 ms\n"
     ]
    }
   ],
   "source": [
    "@timeit\n",
    "def func(n):\n",
    "    return list(range(n))\n",
    "\n",
    "a = func(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bda7f5",
   "metadata": {},
   "source": [
    "This will print out the name of the method and how long it took to execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f9c44",
   "metadata": {},
   "source": [
    "First, let’s download some pdf files from a bunch of URLs and save them to the disk. This is presumably an I/O bound task and we’ll be using the ThreadPoolExecutor class to carry out the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06_example.py\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def download_one(url):\n",
    "    \"\"\"\n",
    "    Downloads the specified URL and saves it to disk\n",
    "    \"\"\"\n",
    "\n",
    "    req = urllib.request.urlopen(url)\n",
    "    fullpath = Path(url)\n",
    "    fname = fullpath.name\n",
    "    fname_path = Path(__file__).parent.joinpath(fname)\n",
    "    ext = fullpath.suffix\n",
    "\n",
    "    if not ext:\n",
    "        raise RuntimeError(\"URL does not contain an extension\")\n",
    "\n",
    "    with open(fname_path, \"wb\") as handle:\n",
    "        while True:\n",
    "            chunk = req.read(1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            handle.write(chunk)\n",
    "\n",
    "    msg = f\"Finished downloading {fname}\"\n",
    "    return msg\n",
    "\n",
    "\n",
    "@timeit\n",
    "def download_all(urls):\n",
    "    \"\"\"\n",
    "    Create a thread pool and download specified urls\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=13) as executor:\n",
    "        return executor.map(download_one, urls, timeout=60)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = (\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040.pdf\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040a.pdf\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040es.pdf\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\",\n",
    "    )\n",
    "\n",
    "    results = download_all(urls)\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5f158",
   "metadata": {},
   "source": [
    "In the above code snippet, I have primary defined two functions. The download_one function downloads a pdf file from a given URL and saves it to the disk. It checks whether the file in URL has an extension and in the absence of an extension, it raises RunTimeError. If an extension is found in the file name, it downloads the file chunk by chunk and saves to the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15173f",
   "metadata": {},
   "source": [
    "Notice in this concurrent version, the `download_one` function is the same as before but in the `download_all` function, a ThreadPoolExecutor context manager wraps the `executor.map()` method. The download_one function is passed into the map along with the iterable containing the URLs. The timeout parameter determines how long a thread will spend before giving up on a single task in the pipeline. The max_workers means how many worker you want to deploy to spawn and manage the threads. A general rule of thumb is using `2 * multiprocessing.cpu_count() + 1`. My machine has 6 physical cores with 12 threads. So 13 is the value I chose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12323503",
   "metadata": {},
   "source": [
    "> Note: You can also try running the above functions with ProcessPoolExecutor via the same interface and notice that the threaded version performs slightly better than due to the nature of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab049e0",
   "metadata": {},
   "source": [
    "There is one small problem with the example above. The `executor.map()` method **returns a generator which allows to iterate through the results once ready**. That means **if any error occurs inside map, it’s not possible to handle that and resume the generator** after the exception occurs. From PEP255:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a0649",
   "metadata": {},
   "source": [
    "> If an unhandled exception– including, but not limited to, StopIteration –is raised by, or passes through, a generator function, then the exception is passed on to the caller in the usual way, and subsequent attempts to resume the generator function raise StopIteration. In other words, **an unhandled exception terminates a generator’s useful life**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900f362",
   "metadata": {},
   "source": [
    "> Pokažemo najprej pokvarjen URL brez error handlinga."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10977f",
   "metadata": {},
   "source": [
    "To get around that, you can use the `executor.submit()` method to create futures, accumulated the futures in a list, iterate through the futures and handle the exceptions manually. See the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07_example.py\n",
    "import time\n",
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def timeit(method):\n",
    "    @wraps(method)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{method.__name__} => {(end_time-start_time)*1000} ms\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def download_one(url):\n",
    "    \"\"\"\n",
    "    Downloads the specified URL and saves it to disk\n",
    "    \"\"\"\n",
    "\n",
    "    req = urllib.request.urlopen(url)\n",
    "    fullpath = Path(url)\n",
    "    fname = fullpath.name\n",
    "    fname_path = Path(__file__).parent.joinpath(fname)\n",
    "    ext = fullpath.suffix\n",
    "\n",
    "    if not ext:\n",
    "        raise RuntimeError(\"URL does not contain an extension\")\n",
    "\n",
    "    with open(fname_path, \"wb\") as handle:\n",
    "        while True:\n",
    "            chunk = req.read(1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            handle.write(chunk)\n",
    "\n",
    "    msg = f\"Finished downloading {fname}\"\n",
    "    return msg\n",
    "\n",
    "\n",
    "@timeit\n",
    "def download_all(urls):\n",
    "    \"\"\"\n",
    "    Create a thread pool and download specified urls\n",
    "    \"\"\"\n",
    "\n",
    "    futures_list = []\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=13) as executor:\n",
    "        for url in urls:\n",
    "            futures = executor.submit(download_one, url)\n",
    "            futures_list.append(futures)\n",
    "\n",
    "        for future in futures_list:\n",
    "            try:\n",
    "                result = future.result(timeout=60)\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                results.append(None)\n",
    "                print(exc)\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = (\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040.pdf\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040es.pdf\",\n",
    "        \"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\",\n",
    "    )\n",
    "\n",
    "    results = download_all(urls)\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bf7f6",
   "metadata": {},
   "source": [
    "## Running Multiple CPU Bound Subroutines with Multi-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c8740",
   "metadata": {},
   "source": [
    "The following example shows a CPU bound hashing function. The primary function will sequentially run a compute intensive hash algorithm multiple times. Then another function will again run the primary function multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1a2a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done 0\n",
      "1 done 1\n",
      "2 done 2\n",
      "3 done 3\n",
      "4 done 4\n",
      "5 done 5\n",
      "6 done 6\n",
      "7 done 7\n",
      "8 done 8\n",
      "9 done 9\n",
      "hash_all => 1151.2749195098877 ms\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def timeit(method):\n",
    "    @wraps(method)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{method.__name__} => {(end_time-start_time)*1000} ms\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def hash_one(n):\n",
    "    \"\"\"A somewhat CPU-intensive task.\"\"\"\n",
    "\n",
    "    for i in range(1, n):\n",
    "        hashlib.pbkdf2_hmac(\"sha256\", b\"password\", b\"salt\", i * 10000)\n",
    "\n",
    "    return f\"done {n}\"\n",
    "\n",
    "\n",
    "@timeit\n",
    "def hash_all(n):\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        for arg, res in zip(range(n), executor.map(hash_one, range(n), chunksize=2)):\n",
    "            print(arg, res)\n",
    "\n",
    "    return \"done\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hash_all(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d1e43",
   "metadata": {},
   "source": [
    "If you analyze the hash_one and hash_all functions, you can see that together, they are actually running two compute intensive nested for loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdd4f7",
   "metadata": {},
   "source": [
    "If you look closely, even in the concurrent version, the for loop in hash_one function is running sequentially. However, the other for loop in the hash_all function is being executed through multiple processes. Here, I have used 10 workers and a chunksize of 2. The number of workers and chunksize were adjusted to achieve maximum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f789d5",
   "metadata": {},
   "source": [
    "However, **we can not use any objects that is not picklable**. So we need to carefully choose what we use/return inside the callable passed to process pool executor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081ef61",
   "metadata": {},
   "source": [
    "> Things that are usually not pickable are, for example, sockets, file(handler)s, database connections, and so on. Everything that's build up (recursively) from basic python types (dicts, lists, primitives, objects, object references, even circular) can be pickled by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70fa0e",
   "metadata": {},
   "source": [
    "## Deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fffe2",
   "metadata": {},
   "source": [
    " If your task at hand requires queuing, spawning multiple threads from multiple processes then you will still need to resort to the lower level threading and multiprocessing modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97a724",
   "metadata": {},
   "source": [
    "Another pitfall of using concurrency is **deadlock situations** that might occur while using ThreadPoolExecutor. When a callable associated with a Future waits on the results of another Future, they might never release their control of the threads and cause deadlock. Let’s see a slightly modified example from the official docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def wait_on_b():\n",
    "    time.sleep(5)\n",
    "    print(b.result())  # b will never complete because it is waiting on a.\n",
    "    return 5\n",
    "\n",
    "\n",
    "def wait_on_a():\n",
    "    time.sleep(5)\n",
    "    print(a.result())  # a will never complete because it is waiting on b.\n",
    "    return 6\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    # here, the future from a depends on the future from b\n",
    "    # and vice versa\n",
    "    # so this is never going to be completed\n",
    "    a = executor.submit(wait_on_b)\n",
    "    b = executor.submit(wait_on_a)\n",
    "\n",
    "    print(\"Result from wait_on_b\", a.result())\n",
    "    print(\"Result from wait_on_a\", b.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d491e7",
   "metadata": {},
   "source": [
    "In the above example, function wait_on_b depends on the result (result of the Future object) of function wait_on_a and at the same time the later function’s result depends on that of the former function. So the code block in the context manager will never execute due to having inter dependencies. This creates the deadlock situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da640239",
   "metadata": {},
   "source": [
    "## Combining Asyncio with Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8340f",
   "metadata": {},
   "source": [
    "What if I need to combine many I/O operations with heavy calculations?\n",
    "\n",
    "We can do that too. Say you need to scrape 100 web pages for a specific piece of information, and then you need to save that piece of info in a file for later. We can separate the compute power across each of our computer's cores by making each process scrape a fraction of the pages.\n",
    "\n",
    "For this script, let's install Beautiful Soup to help us easily scrape our pages: pip install beautifulsoup4. This time we actually have quite a few imports. Here they are, and here's why we're using them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio_mulitproc.py\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import time\n",
    "from math import floor\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74552b",
   "metadata": {},
   "source": [
    "First, we're going to create an async function that makes a request to Wikipedia to get back random pages. We'll scrape each page we get back for its title using BeautifulSoup, and then we'll append it to a given file; we'll separate each title with a tab. The function will take two arguments:\n",
    "- num_pages - Number of pages to request and scrape for titles\n",
    "- output_file - The file to append our titles to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_and_scrape_pages(num_pages: int, output_file: str):\n",
    "    \"\"\"\n",
    "    Makes {{ num_pages }} requests to Wikipedia to receive {{ num_pages }} random\n",
    "    articles, then scrapes each page for its title and appends it to {{ output_file }},\n",
    "    separating each title with a tab: \"\\\\t\"\n",
    "    #### Arguments\n",
    "    ---\n",
    "    num_pages: int -\n",
    "        Number of random Wikipedia pages to request and scrape\n",
    "    output_file: str -\n",
    "        File to append titles to\n",
    "    \"\"\"\n",
    "    async with \\\n",
    "    aiohttp.ClientSession() as client, \\\n",
    "    aiofiles.open(output_file, \"a+\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        for _ in range(num_pages):\n",
    "            async with client.get(\"https://en.wikipedia.org/wiki/Special:Random\") as response:\n",
    "                if response.status > 399:\n",
    "                    # I was getting a 429 Too Many Requests at a higher volume of requests\n",
    "                    response.raise_for_status()\n",
    "\n",
    "                page = await response.text()\n",
    "                soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "                title = soup.find(\"h1\").text\n",
    "\n",
    "                await f.write(title + \"\\t\")\n",
    "\n",
    "        await f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e9dea",
   "metadata": {},
   "source": [
    "We're both asynchronously opening an aiohttp `ClientSession` and our output file. The mode, a+, means append to the file and create it if it doesn't already exist. Encoding our strings as utf-8 ensures we don't get an error if our titles contain international characters. If we get an error response, we'll raise it instead of continuing (at high request volumes I was getting a 429 Too Many Requests). We asynchronously get the text from our response, then we parse the title and asynchronously and append it to our file. After we append all of our titles, we append a new line: \"\\n\".\n",
    "\n",
    "Our next function is the function we'll start with each new process to allow running it asynchronously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_scraping(num_pages: int, output_file: str, i: int):\n",
    "    \"\"\" Starts an async process for requesting and scraping Wikipedia pages \"\"\"\n",
    "    print(f\"Process {i} starting...\")\n",
    "    asyncio.run(get_and_scrape_pages(num_pages, output_file))\n",
    "    print(f\"Process {i} finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b485f1",
   "metadata": {},
   "source": [
    "Now for our main function. Let's start with some constants (and our function declaration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    NUM_PAGES = 50  # Number of pages to scrape altogether\n",
    "    NUM_CORES = cpu_count()  # Our number of CPU cores (including logical cores)\n",
    "    OUTPUT_FILE = str(\n",
    "        Path(__file__).parent.joinpath(\"wiki_titles.tsv\")\n",
    "    )  # File to append our scraped titles to\n",
    "\n",
    "    PAGES_PER_CORE = floor(NUM_PAGES / NUM_CORES)\n",
    "    PAGES_FOR_FINAL_CORE = (\n",
    "        PAGES_PER_CORE + NUM_PAGES % PAGES_PER_CORE\n",
    "    )  # For our final core\n",
    "\n",
    "    futures = []\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(NUM_CORES) as executor:\n",
    "        for i in range(NUM_CORES - 1):\n",
    "            new_future = executor.submit(\n",
    "                start_scraping,  # Function to perform\n",
    "                # v Arguments v\n",
    "                num_pages=PAGES_PER_CORE,\n",
    "                output_file=OUTPUT_FILE,\n",
    "                i=i,\n",
    "            )\n",
    "            futures.append(new_future)\n",
    "\n",
    "        futures.append(\n",
    "            executor.submit(\n",
    "                start_scraping, PAGES_FOR_FINAL_CORE, OUTPUT_FILE, NUM_CORES - 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a674119",
   "metadata": {},
   "source": [
    "We create an array to store our futures, then we create a ProcessPoolExecutor, setting its max_workers equal to our number of cores. We iterate over a range equal to our number of cores minus 1, running a new process with our start_scraping function. We then append it our futures list. Our final core will potentially have extra work to do as it will scrape a number of pages equal to each of our other cores, but will additionally scrape a number of pages equal to the remainder that we got when dividing our total number of pages to scrape by our total number of cpu cores.\n",
    "\n",
    "Make sure to actually run your main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting: Please wait (This may take a while)....\")\n",
    "    start = time.time()\n",
    "    main()\n",
    "    print(f\"Time to complete: {round(time.time() - start, 2)} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
