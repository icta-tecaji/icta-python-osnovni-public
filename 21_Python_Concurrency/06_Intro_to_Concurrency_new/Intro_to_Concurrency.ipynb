{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viri:\n",
    "- [Speed Up Your Python Program With Concurrency](https://realpython.com/python-concurrency/)\n",
    "- [What is the Python Global Interpreter Lock (GIL)?](https://realpython.com/python-gil/)\n",
    "- [Raymond Hettinger, Keynote on Concurrency, PyBay 2017](https://pybay.com/site_media/slides/raymond2017-keynote/intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Concurrency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary definition of concurrency is simultaneous occurrence. In Python, the things that are occurring simultaneously are called by different names (thread, task, process) but at a high level, they all refer to a sequence of instructions that run in order.\n",
    "\n",
    "I like to think of them as different trains of thought. Each one can be stopped at certain points, and the CPU or brain that is processing them can switch to a different one. The state of each one is saved so it can be restarted right where it was interrupted.\n",
    "\n",
    "You might wonder why Python uses different words for the same concept. It turns out that threads, tasks, and processes are only the same if you view them from a high level. Once you start digging into the details, they all represent slightly different things. You’ll see more of how they are different as you progress through the examples.\n",
    "\n",
    "Now let’s talk about the simultaneous part of that definition. You have to be a little careful because, when you get down to the details, only multiprocessing actually runs these trains of thought at literally the same time. Threading and asyncio both run on a single processor and therefore only run one at a time. They just cleverly find ways to take turns to speed up the overall process. Even though they don’t run different trains of thought simultaneously, we still call this concurrency.\n",
    "\n",
    "The way the threads or tasks take turns is the big difference between threading and asyncio. In threading, the operating system actually knows about each thread and can interrupt it at any time to start running a different thread. This is called **pre-emptive multitasking** since the operating system can pre-empt your thread to make the switch.\n",
    "\n",
    "**Pre-emptive multitasking is handy in that the code in the thread doesn’t need to do anything to make the switch.** It can also be **difficult because of that “at any time” phrase**. This switch can happen in the middle of a single Python statement, even a trivial one like x = x + 1.\n",
    "\n",
    "Asyncio, on the other hand, uses **cooperative multitasking**. The tasks must cooperate by announcing when they are ready to be switched out. That means that the code in the task has to change slightly to make this happen.\n",
    "\n",
    "The benefit of doing this extra work up front is that you always know where your task will be swapped out. It will not be swapped out in the middle of a Python statement unless that statement is marked. You’ll see later how this can simplify parts of your design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Parallelism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you’ve looked at concurrency that happens on a single processor. What about all of those CPU cores your cool, new laptop has? How can you make use of them? multiprocessing is the answer.\n",
    "\n",
    "With multiprocessing, Python creates new processes. A **process here can be thought of as almost a completely different program**, though technically they’re usually defined as a collection of resources where the resources include memory, file handles and things like that. One way to think about it is that each process runs in its own Python interpreter.\n",
    "\n",
    "Because they are different processes, each of your trains of thought in a multiprocessing program can run on a different core. Running on a different core means that they actually can run at the same time, which is fabulous. There are some complications that arise from doing this, but Python does a pretty good job of smoothing them over most of the time.\n",
    "\n",
    "Now that you have an idea of what concurrency and parallelism are, let’s review their differences, and then we can look at why they can be useful:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"table-responsive\">\n",
    "<table class=\"table table-hover\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Concurrency Type</th>\n",
    "<th>Switching Decision</th>\n",
    "<th>Number of Processors</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Pre-emptive multitasking (<code>threading</code>)</td>\n",
    "<td>The operating system decides when to switch tasks external to Python.</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Cooperative multitasking (<code>asyncio</code>)</td>\n",
    "<td>The tasks decide when to give up control.</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Multiprocessing (<code>multiprocessing</code>)</td>\n",
    "<td>The processes all run at the same time on different processors.</td>\n",
    "<td>Many</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these types of concurrency can be useful. Let’s take a look at what types of programs they can help you speed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-100\" src=\"images/async_topic_map.png\" sizes=\"75vw\" alt=\"Timing Diagram of an I/O Bound Program\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Is Concurrency Useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concurrency can make a big difference for two types of problems. These are generally called **CPU-bound and I/O-bound**.\n",
    "\n",
    "I/O-bound problems cause your program to slow down because it frequently must wait for input/output (I/O) from some external resource. They arise frequently when your program is working with things that are much slower than your CPU.\n",
    "\n",
    "Examples of things that are slower than your CPU are legion, but your program thankfully does not interact with most of them. The slow things your program will interact with most frequently are the **file system and network connections**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see what that looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-100\" src=\"images/IOBound.webp\" sizes=\"75vw\" alt=\"Timing Diagram of an I/O Bound Program\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above, the blue boxes show time when your program is doing work, and the red boxes are time spent waiting for an I/O operation to complete. This diagram is not to scale because requests on the internet can take several orders of magnitude longer than CPU instructions, so your program can end up spending most of its time waiting. This is what your browser is doing most of the time.\n",
    "\n",
    "On the flip side, there are classes of programs that do significant computation without talking to the network or accessing a file. These are the CPU-bound programs, because the resource limiting the speed of your program is the CPU, not the network or the file system.\n",
    "\n",
    "Here’s a corresponding diagram for a CPU-bound program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-100\" src=\"images/CPUBound.webp\" sizes=\"75vw\" alt=\"Timing Diagram of an CPU Bound Program\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work through the examples in the following section, you’ll see that different forms of concurrency work better or worse with CPU-bound and I/O-bound programs. **Adding concurrency to your program adds extra code and complications, so you’ll need to decide if the potential speed up is worth the extra effort**. By the end of this article, you should have enough info to start making that decision.\n",
    "\n",
    "Here’s a quick summary to clarify this concept:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"table-responsive\">\n",
    "<table class=\"table table-hover\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>I/O-Bound Process</th>\n",
    "<th>CPU-Bound Process</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Your program spends most of its time talking to a slow device, like a network connection, a hard drive, or a printer.</td>\n",
    "<td>You program spends most of its time doing CPU operations.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Speeding it up involves overlapping the times spent waiting for these devices.</td>\n",
    "<td>Speeding it up involves finding ways to do more computations in the same amount of time.</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threads vs Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads switch preemptively. This is convenient because you don’t need to add explicit code to cause a task switch.\n",
    "\n",
    "The cost of this convenience is that you have to assume a switch can happen at any time. Accordingly, critical sections have to be guarded with locks.\n",
    "\n",
    "The limit on threads is total CPU power minus the cost of task switches and synchronization overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async switches cooperatively, so you do need to add explicit code “yield” or “await” to cause a task switch.\n",
    "\n",
    "Now you control when task switches occur, so locks and other synchronization are no longer needed.\n",
    "\n",
    "Also, the cost task switches is very low. Calling a pure Python function has more overhead than restarting a generator or awaitable.\n",
    "\n",
    "This means that async is very cheap.\n",
    "\n",
    "In return, you’ll need a non-blocking version of just about everything you do. Accordingly, the async world has a huge ecosystem of support tools. This increases the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Async maximizes CPU utilization because it has less overhead than threads.\n",
    "- Threading typically works with existing code and tools as long as locks are added around critical sections.\n",
    "- For complex systems, async is much easier to get right than threads with locks.\n",
    "- Threads require very little tooling (locks and queues).\n",
    "- Async needs a great deal of tooling (futures, event loops, and non-blocking versions of just about everything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Python Global Interpreter Lock (GIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python Global Interpreter Lock or GIL, in simple words, is a mutex (or a lock) that allows only one thread to hold the control of the Python interpreter.\n",
    "\n",
    "This means that only one thread can be in a state of execution at any point in time. The impact of the GIL isn’t visible to developers who execute single-threaded programs, but it can be a performance bottleneck in CPU-bound and multi-threaded code.\n",
    "\n",
    "Since the GIL allows only one thread to execute at a time even in a multi-threaded architecture with more than one CPU core, the GIL has gained a reputation as an “infamous” feature of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What problem did the GIL solve for Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python uses reference counting for memory management. It means that objects created in Python have a reference count variable that keeps track of the number of references that point to the object. When this count reaches zero, the memory occupied by the object is released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at a brief code example to demonstrate how reference counting works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "a = []\n",
    "b = a\n",
    "sys.getrefcount(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the reference count for the empty list object [] was 3. The list object was referenced by a, b and the argument passed to sys.getrefcount().\n",
    "\n",
    "Back to the GIL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem was that this **reference count variable needed protection from race condition** where two threads increase or decrease its value simultaneously. If this happens, it can cause either leaked memory that is never released or, even worse, incorrectly release the memory while a reference to that object still exists. This can can cause crashes or other “weird” bugs in your Python programs.\n",
    "\n",
    "This reference count variable can be kept safe by adding locks to all data structures that are shared across threads so that they are not modified inconsistently.\n",
    "\n",
    "But adding a lock to each object or groups of objects means multiple locks will exist which can cause another problem—Deadlocks (deadlocks can only happen if there is more than one lock). Another side effect would be decreased performance caused by the repeated acquisition and release of locks.\n",
    "\n",
    "The GIL is a single lock on the interpreter itself which adds a rule that execution of any Python bytecode requires acquiring the interpreter lock. This prevents deadlocks (as there is only one lock) and doesn’t introduce much performance overhead. But it **effectively makes any CPU-bound Python program single-threaded**.\n",
    "\n",
    "The GIL, although used by interpreters for other languages like Ruby, is not the only solution to this problem. Some languages avoid the requirement of a GIL for thread-safe memory management by using approaches other than reference counting, such as garbage collection.\n",
    "\n",
    "On the other hand, this means that those languages often have to compensate for the loss of single threaded performance benefits of a GIL by adding other performance boosting features like JIT compilers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why was the GIL chosen as the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why was an approach that is seemingly so obstructing used in Python? Was it a bad decision by the developers of Python?\n",
    "\n",
    "Well, in the words of Larry Hastings, the design decision of the GIL is one of the things that made Python as popular as it is today.\n",
    "\n",
    "Python has been around since the days when operating systems did not have a concept of threads. Python was designed to be easy-to-use in order to make development quicker and more and more developers started using it.\n",
    "\n",
    "A lot of extensions were being written for the existing C libraries whose features were needed in Python. To prevent inconsistent changes, these C extensions required a thread-safe memory management which the GIL provided.\n",
    "\n",
    "The GIL is simple to implement and was easily added to Python. It provides a performance increase to single-threaded programs as only one lock needs to be managed.\n",
    "\n",
    "C libraries that were not thread-safe became easier to integrate. And these C extensions became one of the reasons why Python was readily adopted by different communities.\n",
    "\n",
    "As you can see, the GIL was a pragmatic solution to a difficult problem that the CPython developers faced early on in Python’s life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The impact on multi-threaded Python programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look at a typical Python program—or any computer program for that matter—there’s a difference between those that are CPU-bound in their performance and those that are I/O-bound.\n",
    "\n",
    "CPU-bound programs are those that are pushing the CPU to its limit. This includes programs that do mathematical computations like matrix multiplications, searching, image processing, etc.\n",
    "\n",
    "I/O-bound programs are the ones that spend time waiting for Input/Output which can come from a user, file, database, network, etc. I/O-bound programs sometimes have to wait for a significant amount of time till they get what they need from the source due to the fact that the source may need to do its own processing before the input/output is ready, for example, a user thinking about what to enter into an input prompt or a database query running in its own process.\n",
    "\n",
    "Let’s have a look at a simple CPU-bound program that performs a countdown:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds - 4.315563201904297\n"
     ]
    }
   ],
   "source": [
    "# single_threaded.py\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "COUNT = 50000000\n",
    "\n",
    "def countdown(n):\n",
    "    while n>0:\n",
    "        n -= 1\n",
    "\n",
    "start = time.time()\n",
    "countdown(COUNT)\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken in seconds -', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code on my system with 4 cores gave the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Time taken in seconds - 6.20024037361145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I modified the code a bit to do to the same countdown using two threads in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds - 5.419163227081299\n"
     ]
    }
   ],
   "source": [
    "# multi_threaded.py\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "COUNT = 50000000\n",
    "\n",
    "def countdown(n):\n",
    "    while n>0:\n",
    "        n -= 1\n",
    "\n",
    "t1 = Thread(target=countdown, args=(COUNT//2,))\n",
    "t2 = Thread(target=countdown, args=(COUNT//2,))\n",
    "\n",
    "start = time.time()\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken in seconds -', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both versions take almost same amount of time to finish. In the multi-threaded version the GIL prevented the CPU-bound threads from executing in parellel.\n",
    "\n",
    "The GIL does not have much impact on the performance of I/O-bound multi-threaded programs as the lock is shared between threads while they are waiting for I/O.\n",
    "\n",
    "But a program whose threads are entirely CPU-bound, e.g., a program that processes an image in parts using threads, would not only become single threaded due to the lock but will also see an increase in execution time, as seen in the above example, in comparison to a scenario where it was written to be entirely single-threaded.\n",
    "\n",
    "This increase is the result of acquire and release overheads added by the lock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why hasn’t the GIL been removed yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developers of Python receive a lot of complaints regarding this but a language as popular as Python cannot bring a change as significant as the removal of GIL without causing backward incompatibility issues.\n",
    "\n",
    "The GIL can obviously be removed and this has been done multiple times in the past by the developers and researchers but **all those attempts broke the existing C extensions which depend heavily on the solution that the GIL provides**.\n",
    "\n",
    "Of course, there are other solutions to the problem that the GIL solves but some of them decrease the performance of single-threaded and multi-threaded I/O-bound programs and some of them are just too difficult. After all, you wouldn’t want your existing Python programs to run slower after a new version comes out, right?\n",
    "\n",
    "The creator and BDFL of Python, Guido van Rossum, gave an answer to the community in September 2007 in his article “It isn’t Easy to remove the GIL”:\n",
    "\n",
    ">“I’d welcome a set of patches into Py3k only if the performance for a single-threaded program (and for a multi-threaded but I/O-bound program) does not decrease”\n",
    "\n",
    "And this condition hasn’t been fulfilled by any of the attempts made since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why wasn’t it removed in Python 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3 did have a chance to start a lot of features from scratch and in the process, broke some of the existing C extensions which then required changes to be updated and ported to work with Python 3. This was the reason why the early versions of Python 3 saw slower adoption by the community.\n",
    "\n",
    "But why wasn’t GIL removed alongside?\n",
    "\n",
    "Removing the GIL would have made Python 3 slower in comparison to Python 2 in single-threaded performance and you can imagine what that would have resulted in. You can’t argue with the single-threaded performance benefits of the GIL. So the result is that Python 3 still has the GIL.\n",
    "\n",
    "But Python 3 did bring a major improvement to the existing GIL—\n",
    "\n",
    "We discussed the impact of GIL on “only CPU-bound” and “only I/O-bound” multi-threaded programs but what about the programs where some threads are I/O-bound and some are CPU-bound?\n",
    "\n",
    "In such programs, Python’s GIL was known to starve the I/O-bound threads by not giving them a chance to acquire the GIL from CPU-bound threads.\n",
    "\n",
    "This was because of a mechanism built into Python that forced threads to release the GIL after a fixed interval of continuous use and if nobody else acquired the GIL, the same thread could continue its use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: sys.getcheckinterval() and sys.setcheckinterval() are deprecated.  Use sys.getswitchinterval() instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# The interval is set to 100 instructions:\n",
    "sys.getcheckinterval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem in this mechanism was that most of the time the CPU-bound thread would reacquire the GIL itself before other threads could acquire it. This was researched by David Beazley and visualizations can be found here.\n",
    "\n",
    "This problem was fixed in Python 3.2 in 2009 by Antoine Pitrou who added a mechanism of looking at the number of GIL acquisition requests by other threads that got dropped and not allowing the current thread to reacquire GIL before other threads got a chance to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with Python’s GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the GIL is causing you problems, here a few approaches you can try:\n",
    "\n",
    "Multi-processing vs multi-threading: The most popular way is to use a multi-processing approach where you use multiple processes instead of threads. Each Python process gets its own Python interpreter and memory space so the GIL won’t be a problem. Python has a multiprocessing module which lets us create processes easily like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds - 1.9075770378112793\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "COUNT = 50000000\n",
    "def countdown(n):\n",
    "    while n>0:\n",
    "        n -= 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(processes=2)\n",
    "    start = time.time()\n",
    "    r1 = pool.apply_async(countdown, [COUNT//2])\n",
    "    r2 = pool.apply_async(countdown, [COUNT//2])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end = time.time()\n",
    "    print('Time taken in seconds -', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this on my system gave this output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Time taken in seconds - 4.060242414474487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decent performance increase compared to the multi-threaded version, right?\n",
    "\n",
    "The time didn’t drop to half of what we saw above because process management has its own overheads. Multiple processes are heavier than multiple threads, so, keep in mind that this could become a scaling bottleneck.\n",
    "\n",
    "Alternative Python interpreters: Python has multiple interpreter implementations. CPython, Jython, IronPython and PyPy, written in C, Java, C# and Python respectively, are the most popular ones. GIL exists only in the original Python implementation that is CPython. If your program, with its libraries, is available for one of the other implementations then you can try them out as well.\n",
    "\n",
    "Just wait it out: While many Python users take advantage of the single-threaded performance benefits of GIL. The multi-threading programmers don’t have to fret as some of the brightest minds in the Python community are working to remove the GIL from CPython. One such attempt is known as the Gilectomy.\n",
    "\n",
    "The Python GIL is often regarded as a mysterious and difficult topic. But keep in mind that as a Pythonista you’re usually only affected by it if you are writing C extensions or if you’re using CPU-bound multi-threading in your programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Speed Up an I/O-Bound Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by focusing on I/O-bound programs and a common problem: downloading content over the network. For our example, you will be downloading web pages from a few sites, but it really could be any network traffic. It’s just easier to visualize and set up with web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll start with a non-concurrent version of this task. Note that this program requires the requests module. You should run pip install requests before running it, probably using a virtualenv. This version does not use concurrency at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io_bound_sync.py\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "def download_site(url, session):\n",
    "    with session.get(url) as response:\n",
    "        print(f\"Read {len(response.content)} from {url}\")\n",
    "\n",
    "\n",
    "def download_all_sites(sites):\n",
    "    with requests.Session() as session:\n",
    "        for url in sites:\n",
    "            download_site(url, session)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites = [\n",
    "        \"https://www.jython.org\",\n",
    "        \"http://olympus.realpython.org/dice\",\n",
    "    ] * 40\n",
    "    start_time = time.time()\n",
    "    download_all_sites(sites)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(sites)} in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a fairly short program. download_site() just downloads the contents from a URL and prints the size. One small thing to point out is that we’re using a Session object from requests.\n",
    "\n",
    "It is possible to simply use get() from requests directly, but creating a Session object allows requests to do some fancy networking tricks and really speed things up.\n",
    "\n",
    "> The Session object allows you to persist certain parameters across requests. It also persists cookies across all requests made from the Session instance, and will use urllib3’s connection pooling. So if you’re making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase (see HTTP persistent connection).\n",
    "\n",
    "download_all_sites() creates the Session and then walks through the list of sites, downloading each one in turn. Finally, it prints out how long this process took so you can have the satisfaction of seeing how much concurrency has helped us in the following examples.\n",
    "\n",
    "The processing diagram for this program will look much like the I/O-bound diagram in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Network traffic is dependent on many factors that can vary from second to second. I’ve seen the times of these tests double from one run to another due to network issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the Synchronous Version Rocks**\n",
    "\n",
    "The great thing about this version of code is that, well, it’s easy. It was comparatively easy to write and debug. It’s also more straight-forward to think about. There’s only one train of thought running through it, so you can predict what the next step is and how it will behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Problems With the Synchronous Version**\n",
    "\n",
    "The big problem here is that it’s relatively slow compared to the other solutions we’ll provide. Here’s an example of what the final output gave on my machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Downloaded 80 in 6.340069770812988 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Your results may vary significantly. When running this script, I saw the times vary from 14.2 to 21.9 seconds. For this article, I took the fastest of three runs as the time. The differences between the methods will still be clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being slower isn’t always a big issue, however. If the program you’re running takes only 2 seconds with a synchronous version and is only run rarely, it’s probably not worth adding concurrency. You can stop here.\n",
    "\n",
    "What if your program is run frequently? What if it takes hours to run? Let’s move on to concurrency by rewriting this program using threading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threading Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably guessed, writing a threaded program takes more effort. You might be surprised at how little extra effort it takes for simple cases, however. Here’s what the same program looks like with threading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io_bound_thread.py\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "thread_local = threading.local()\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(thread_local, \"session\"):\n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session\n",
    "\n",
    "\n",
    "def download_site(url):\n",
    "    session = get_session()\n",
    "    with session.get(url) as response:\n",
    "        print(f\"Read {len(response.content)} from {url}\")\n",
    "\n",
    "\n",
    "def download_all_sites(sites):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(download_site, sites)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites = [\n",
    "        \"https://www.jython.org\",\n",
    "        \"http://olympus.realpython.org/dice\",\n",
    "    ] * 40\n",
    "    start_time = time.time()\n",
    "    download_all_sites(sites)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(sites)} in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Thread-local data is data whose values are thread specific. To manage thread-local data, just create an instance of local (or a subclass) and store attributes on it. The instance’s values will be different for separate threads.\n",
    "- While some resources need to be locked so multiple threads can use them, others need to be protected so that they are hidden from the views of threads that do not \"own\" them. **The local() function creates an object capable of hiding values from view in separate threads.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you add threading, the overall structure is the same and you only needed to make a few changes. download_all_sites() changed from calling the function once per site to a more complex structure.\n",
    "\n",
    "In this version, you’re creating a ThreadPoolExecutor, which seems like a complicated thing. Let’s break that down: ThreadPoolExecutor = Thread + Pool + Executor.\n",
    "\n",
    "You already know about the Thread part. That’s just a train of thought we mentioned earlier. The Pool portion is where it starts to get interesting. This object is going to create a pool of threads, each of which can run concurrently. Finally, the Executor is the part that’s going to control how and when each of the threads in the pool will run. It will execute the request in the pool.\n",
    "\n",
    "Helpfully, the standard library implements ThreadPoolExecutor as a context manager so you can use the with syntax to manage creating and freeing the pool of Threads.\n",
    "\n",
    "Once you have a ThreadPoolExecutor, you can use its handy .map() method. This method runs the passed-in function on each of the sites in the list. The great part is that it automatically runs them concurrently using the pool of threads it is managing.\n",
    "\n",
    "Those of you coming from other languages, or even Python 2, are probably wondering where the usual objects and functions are that manage the details you’re used to when dealing with threading, things like Thread.start(), Thread.join(), and Queue.\n",
    "\n",
    "These are all still there, and you can use them to achieve fine-grained control of how your threads are run. But, starting with Python 3.2, the standard library added a higher-level abstraction called Executors that manage many of the details for you if you don’t need that fine-grained control.\n",
    "\n",
    "The other interesting change in our example is that each thread needs to create its own requests.Session() object. When you’re looking at the documentation for requests, it’s not necessarily easy to tell, but reading this issue, it seems fairly clear that you need a separate Session for each thread.\n",
    "\n",
    "This is one of the interesting and difficult issues with threading. **Because the operating system is in control of when your task gets interrupted and another task starts, any data that is shared between the threads needs to be protected, or thread-safe. Unfortunately requests.Session() is not thread-safe**.\n",
    "\n",
    "There are several strategies for making data accesses thread-safe depending on what the data is and how you’re using it. One of them is to use thread-safe data structures like Queue from Python’s queue module.\n",
    "\n",
    "These objects use low-level primitives like threading.Lock to ensure that only one thread can access a block of code or a bit of memory at the same time. You are using this strategy indirectly by way of the ThreadPoolExecutor object.\n",
    "\n",
    "Another strategy to use here is something called thread local storage. Threading.local() creates an object that look like a global but is specific to each individual thread. In your example, this is done with threadLocal and get_session():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threadLocal = threading.local()\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(threadLocal, \"session\"):\n",
    "        threadLocal.session = requests.Session()\n",
    "    return threadLocal.session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThreadLocal is in the threading module to specifically solve this problem. It looks a little odd, but you only want to create one of these objects, not one for each thread. The object itself takes care of separating accesses from different threads to different data.\n",
    "\n",
    "When get_session() is called, the session it looks up is specific to the particular thread on which it’s running. So each thread will create a single session the first time it calls get_session() and then will simply use that session on each subsequent call throughout its lifetime.\n",
    "\n",
    "Finally, a quick note about picking the number of threads. You can see that the example code uses 5 threads. Feel free to play around with this number and see how the overall time changes. You might expect that having one thread per download would be the fastest but, at least on my system it was not. I found the fastest results somewhere between 5 and 10 threads. If you go any higher than that, then the **extra overhead of creating and destroying the threads erases any time savings**.\n",
    "\n",
    "The difficult answer here is that the correct number of threads is not a constant from one task to another. Some experimentation is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the threading Version Rocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s fast! Here’s the fastest run of my tests. Remember that the non-concurrent version took more than 14 seconds:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Downloaded 80 in 1.6867749691009521 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s what its execution timing diagram looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-66\" src=\"https://files.realpython.com/media/Threading.3eef48da829e.png\" width=\"1197\" height=\"537\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/Threading.3eef48da829e.png&amp;w=299&amp;sig=3fa41f9fd4fadcde180155707620dcd593c6a56f 299w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/Threading.3eef48da829e.png&amp;w=598&amp;sig=fbfdfa285a8e3f0872dc5d61de021c385f6e0ec1 598w, https://files.realpython.com/media/Threading.3eef48da829e.png 1197w\" sizes=\"75vw\" alt=\"Timing Diagram of a Threading Solution\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses multiple threads to have multiple open requests out to web sites at the same time, allowing your program to overlap the waiting times and get the final result faster! Yippee! That was the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Problems with the threading Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as you can see from the example, it **takes a little more code** to make this happen, and you really have to give some **thought to what data is shared between threads**.\n",
    "\n",
    "Threads can interact in ways that are subtle and hard to detect. These interactions can cause **race conditions** that frequently result in random, intermittent bugs that can be quite difficult to find. Those of you who are unfamiliar with the concept of race conditions might want to expand and read the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race conditions are an entire class of subtle bugs that can and frequently do happen in multi-threaded code. Race conditions happen because the programmer has not sufficiently protected data accesses to prevent threads from interfering with each other. You need to take extra steps when writing threaded code to ensure things are thread-safe.\n",
    "\n",
    "What’s going on here is that the operating system is controlling when your thread runs and when it gets swapped out to let another thread run. This thread swapping can occur at any point, even while doing sub-steps of a Python statement. As a quick example, look at this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "def increment_counter(fake_value):\n",
    "    global counter\n",
    "    for _ in range(100):\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fake_data = [x for x in range(5000)]\n",
    "    counter = 0\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5000) as executor:\n",
    "        executor.map(increment_counter, fake_data)\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is quite similar to the structure you used in the threading example above. The difference is that each of the threads is accessing the same global variable counter and incrementing it. Counter is not protected in any way, so it is not thread-safe.\n",
    "\n",
    "In order to increment counter, each of the threads needs to read the current value, add one to it, and the save that value back to the variable. That happens in this line: counter += 1.\n",
    "\n",
    "Because the operating system knows nothing about your code and can swap threads at any point in the execution, it’s possible for this swap to happen after a thread has read the value but before it has had the chance to write it back. If the new code that is running modifies counter as well, then the first thread has a stale copy of the data and trouble will ensue.\n",
    "\n",
    "As you can imagine, hitting this exact situation is fairly rare. You can run this program thousands of times and never see the problem. That’s what makes this type of problem quite difficult to debug as it can be quite hard to reproduce and can cause random-looking errors to show up.\n",
    "\n",
    "As a further example, I want to remind you that requests.Session() is not thread-safe. This means that there are places where the type of interaction described above could happen if multiple threads use the same Session. I bring this up not to cast aspersions on requests but rather to point out that these are difficult problems to resolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asyncio Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you jump into examining the asyncio example code, let’s talk more about how asyncio works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**asyncio Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a simplified version of asycio. There are many details that are glossed over here, but it still conveys the idea of how it works.\n",
    "\n",
    "The general concept of asyncio is that a single Python object, called the event loop, controls how and when each task gets run. The event loop is aware of each task and knows what state it’s in. In reality, there are many states that tasks could be in, but for now let’s imagine a simplified event loop that just has two states.\n",
    "\n",
    "The **ready state** will indicate that a task has work to do and is ready to be run, and the **waiting state** means that the task is waiting for some external thing to finish, such as a network operation.\n",
    "\n",
    "Your simplified event loop maintains two lists of tasks, one for each of these states. It selects one of the ready tasks and starts it back to running. That task is in complete control until it cooperatively hands the control back to the event loop.\n",
    "\n",
    "When the running task gives control back to the event loop, the event loop places that task into either the ready or waiting list and then goes through each of the tasks in the waiting list to see if it has become ready by an I/O operation completing. It knows that the tasks in the ready list are still ready because it knows they haven’t run yet.\n",
    "\n",
    "Once all of the tasks have been sorted into the right list again, the event loop picks the next task to run, and the process repeats. Your simplified event loop picks the task that has been waiting the longest and runs that. This process repeats until the event loop is finished.\n",
    "\n",
    "An important point of asyncio is that **the tasks never give up control without intentionally doing so**. They never get interrupted in the middle of an operation. This allows us to share resources a bit more easily in asyncio than in threading. **You don’t have to worry about making your code thread-safe**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**async and await**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s talk about two new keywords that were added to Python: async and await. In light of the discussion above, you can view await as the magic that allows the task to hand control back to the event loop. When your code awaits a function call, it’s a signal that the call is likely to be something that takes a while and that the task should give up control.\n",
    "\n",
    "It’s easiest to think of async as a flag to Python telling it that the function about to be defined uses await. There are some cases where this is not strictly true, like asynchronous generators, but it holds for many cases and gives you a simple model while you’re getting started.\n",
    "\n",
    "One exception to this that you’ll see in the next code is the async with statement, which creates a context manager from an object you would normally await. While the semantics are a little different, the idea is the same: to flag this context manager as something that can get swapped out.\n",
    "\n",
    "As I’m sure you can imagine, there’s some complexity in managing the interaction between the event loop and the tasks. For developers starting out with asyncio, these details aren’t important, but you do need to remember that any function that calls await needs to be marked with async. You’ll get a syntax error otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve got a basic understanding of what asyncio is, let’s walk through the asyncio version of the example code and figure out how it works. Note that this version adds aiohttp. You should run pip install aiohttp before running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io_bound_async.py\n",
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "async def download_site(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        print(\"Read {0} from {1}\".format(response.content_length, url))\n",
    "\n",
    "\n",
    "async def download_all_sites(sites):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in sites:\n",
    "            task = asyncio.create_task(download_site(session, url))\n",
    "            tasks.append(task)\n",
    "        await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites = [\n",
    "        \"https://www.jython.org\",\n",
    "        \"http://olympus.realpython.org/dice\",\n",
    "    ] * 80\n",
    "    start_time = time.time()\n",
    "    asyncio.run(download_all_sites(sites))\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is a bit more complex than the previous two. It has a similar structure, but there’s a bit more work setting up the tasks than there was creating the ThreadPoolExecutor. Let’s start at the top of the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download_site()**\n",
    "\n",
    "download_site() at the top is almost identical to the threading version with the exception of the async keyword on the function definition line and the async with keywords when you actually call session.get(). You’ll see later why Session can be passed in here rather than using thread-local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download_all_sites()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download_all_sites() is where you will see the biggest change from the threading example.\n",
    "\n",
    "You can share the session across all tasks, so the session is created here as a context manager. The tasks can share the session because they are all running on the same thread. There is no way one task could interrupt another while the session is in a bad state.\n",
    "\n",
    "Inside that context manager, it creates a list of tasks using asyncio.ensure_future(), which also takes care of starting them. Once all the tasks are created, this function uses asyncio.gather() to keep the session context alive until all of the tasks have completed.\n",
    "\n",
    "The threading code does something similar to this, but the details are conveniently handled in the ThreadPoolExecutor. There currently is not an AsyncioPoolExecutor class.\n",
    "\n",
    "There is one small but important change buried in the details here, however. Remember how we talked about the number of threads to create? It wasn’t obvious in the threading example what the optimal number of threads was.\n",
    "\n",
    "One of the cool advantages of asyncio is that it scales far better than threading. Each task takes far fewer resources and less time to create than a thread, so creating and running more of them works well. This example just creates a separate task for each site to download, which works out quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`__main__`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the nature of asyncio means that you have to start up the event loop and tell it which tasks to run. The __main__ section at the bottom of the file contains the code to get_event_loop() and then run_until_complete(). If nothing else, they’ve done an excellent job in naming those functions.\n",
    "\n",
    "If you’ve updated to Python 3.7, the Python core developers simplified this syntax for you. Instead of the asyncio.get_event_loop().run_until_complete() tongue-twister, you can just use asyncio.run()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the asyncio Version Rocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s really fast! In the tests on my machine, this was the fastest version of the code by a good margin:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Downloaded 160 sites in 0.9474620819091797 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution timing diagram looks quite similar to what’s happening in the threading example. It’s just that the I/O requests are all done by the same thread:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-66\" src=\"https://files.realpython.com/media/Asyncio.31182d3731cf.png\" width=\"400\" height=\"250\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/Asyncio.31182d3731cf.png&amp;w=233&amp;sig=eb7529b1aaefa8e02a64fbde5d58d7cc2973d1dd 233w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/Asyncio.31182d3731cf.png&amp;w=466&amp;sig=33afb6333aba2b90a760f0fadd7cddc7de08862a 466w, https://files.realpython.com/media/Asyncio.31182d3731cf.png 933w\" sizes=\"75vw\" alt=\"Timing Diagram of a Asyncio Solution\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lack of a nice wrapper like the ThreadPoolExecutor makes this code a bit more complex than the threading example. This is a case where you have to do a little extra work to get much better performance.\n",
    "\n",
    "Also there’s a common argument that having to add async and await in the proper locations is an extra complication. To a small extent, that is true. The flip side of this argument is that it forces you to think about when a given task will get swapped out, which can help you create a better, faster, design.\n",
    "\n",
    "The scaling issue also looms large here. Running the threading example above with a thread for each site is noticeably slower than running it with a handful of threads. Running the asyncio example with hundreds of tasks didn’t slow it down at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Problems With the asyncio Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of issues with asyncio at this point. You **need special async versions of libraries to gain the full advantage of asycio**. Had you just used requests for downloading the sites, it would have been much slower because requests is not designed to notify the event loop that it’s blocked. This issue is getting smaller and smaller as time goes on and more libraries embrace asyncio.\n",
    "\n",
    "Another, more subtle, issue is that all of the advantages of cooperative multitasking get thrown away if one of the tasks doesn’t cooperate. A **minor mistake in code can cause a task to run off and hold the processor for a long time**, starving other tasks that need running. There is no way for the event loop to break in if a task does not hand control back to it.\n",
    "\n",
    "With that in mind, let’s step up to a radically different approach to concurrency, multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiprocessing Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous approaches, the multiprocessing version of the code takes full advantage of the multiple CPUs that your cool, new computer has. Or, in my case, that my clunky, old laptop has. Let’s start with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io_bound_multiproc.py\n",
    "import requests\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "session = None\n",
    "\n",
    "\n",
    "def set_global_session():\n",
    "    global session\n",
    "    if not session:\n",
    "        session = requests.Session()\n",
    "\n",
    "\n",
    "def download_site(url):\n",
    "    with session.get(url) as response:\n",
    "        name = multiprocessing.current_process().name\n",
    "        print(f\"{name}:Read {len(response.content)} from {url}\")\n",
    "\n",
    "\n",
    "def download_all_sites(sites):\n",
    "    with multiprocessing.Pool(initializer=set_global_session) as pool:\n",
    "        pool.map(download_site, sites)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sites = [\n",
    "        \"https://www.jython.org\",\n",
    "        \"http://olympus.realpython.org/dice\",\n",
    "    ] * 80\n",
    "    start_time = time.time()\n",
    "    download_all_sites(sites)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(sites)} in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much shorter than the asyncio example and actually looks quite similar to the threading example, but before we dive into the code, let’s take a quick tour of what multiprocessing does for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\r\n"
     ]
    }
   ],
   "source": [
    "# število procesorjev\n",
    "!nproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multiprocessing in a Nutshell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, all of the examples of concurrency in this article run only on a single CPU or core in your computer. The reasons for this have to do with the current design of CPython and something called the Global Interpreter Lock, or GIL.\n",
    "\n",
    "This article won’t dive into the hows and whys of the GIL. It’s enough for now to know that the synchronous, threading, and asyncio versions of this example all run on a single CPU.\n",
    "\n",
    "multiprocessing in the standard library was designed to break down that barrier and run your code across multiple CPUs. At a high level, it does this by creating a new instance of the Python interpreter to run on each CPU and then farming out part of your program to run on it.\n",
    "\n",
    "As you can imagine, bringing up a separate Python interpreter is not as fast as starting a new thread in the current Python interpreter. It’s a heavyweight operation and comes with some restrictions and difficulties, but for the correct problem, it can make a huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multiprocessing Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has a few small changes from our synchronous version. The first one is in download_all_sites(). Instead of simply calling download_site() repeatedly, it creates a multiprocessing.Pool object and has it map download_site to the iterable sites. This should look familiar from the threading example.\n",
    "\n",
    "What happens here is that the Pool creates a number of separate Python interpreter processes and has each one run the specified function on some of the items in the iterable, which in our case is the list of sites. The communication between the main process and the other processes is handled by the multiprocessing module for you.\n",
    "\n",
    "The line that creates Pool is worth your attention. First off, it does not specify how many processes to create in the Pool, although that is an optional parameter. By default, multiprocessing.Pool() will determine the number of CPUs in your computer and match that. This is frequently the best answer, and it is in our case.\n",
    "\n",
    "For this problem, increasing the number of processes did not make things faster. It actually slowed things down because the cost for setting up and tearing down all those processes was larger than the benefit of doing the I/O requests in parallel.\n",
    "\n",
    "Next we have the initializer=set_global_session part of that call. Remember that each process in our Pool has its own memory space. That means that they cannot share things like a Session object. You don’t want to create a new Session each time the function is called, you want to create one for each process.\n",
    "\n",
    "The initializer function parameter is built for just this case. There is not a way to pass a return value back from the initializer to the function called by the process download_site(), but you can initialize a global session variable to hold the single session for each process. Because each process has its own memory space, the global for each one will be different.\n",
    "\n",
    "That’s really all there is to it. The rest of the code is quite similar to what you’ve seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the multiprocessing Version Rocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiprocessing version of this example is great because it’s relatively easy to set up and requires little extra code. It also takes full advantage of the CPU power in your computer. The execution timing diagram for this code looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-100\" src=\"https://files.realpython.com/media/MProc.7cf3be371bbc.png\" width=\"1000\" height=\"1000\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/MProc.7cf3be371bbc.png&amp;w=473&amp;sig=344e84d959d450c574483ae579b0c04ba18ffd65 473w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/MProc.7cf3be371bbc.png&amp;w=946&amp;sig=e7be5f8a271be230c889c1e9973ff11bbaf920be 946w, https://files.realpython.com/media/MProc.7cf3be371bbc.png 1893w\" sizes=\"75vw\" alt=\"Timing Diagram of a Multiprocessing Solution\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Problems With the multiprocessing Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the example does require some extra setup, and the global session object is strange. You have to spend some time thinking about which variables will be accessed in each process.\n",
    "\n",
    "Finally, it is clearly slower than the asyncio and threading versions in this example.\n",
    "\n",
    "That’s not surprising, as I/O-bound problems are not really why multiprocessing exists. You’ll see more as you step into the next section and look at CPU-bound examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Speed Up a CPU-Bound Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s shift gears here a little bit. The examples so far have all dealt with an I/O-bound problem. Now, you’ll look into a CPU-bound problem. As you saw, an I/O-bound problem spends most of its time waiting for external operations, like a network call, to complete. A CPU-bound problem, on the other hand, does few I/O operations, and its overall execution time is a factor of how fast it can process the required data.\n",
    "\n",
    "For the purposes of our example, we’ll use a somewhat silly function to create something that takes a long time to run on the CPU. This function computes the sum of the squares of each number from 0 to the passed-in value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_bound(number):\n",
    "    return sum(i * i for i in range(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll be passing in large numbers, so this will take a while. Remember, this is just a placeholder for your code that actually does something useful and requires significant processing time, like computing the roots of equations or sorting a large data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU-Bound Synchronous Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at the non-concurrent version of the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 6.946254730224609 seconds\n"
     ]
    }
   ],
   "source": [
    "# cpu_bound_sync.py\n",
    "import time\n",
    "\n",
    "\n",
    "def cpu_bound(number):\n",
    "    return sum(i * i for i in range(number))\n",
    "\n",
    "\n",
    "def find_sums(numbers):\n",
    "    for number in numbers:\n",
    "        cpu_bound(number)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numbers = [5_000_000 + x for x in range(10)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    find_sums(numbers)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Duration {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calls cpu_bound() 20 times with a different large number each time. It does all of this on a single thread in a single process on a single CPU. The execution timing diagram looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-100\" src=\"https://files.realpython.com/media/CPUBound.d2d32cb2626c.png\" width=\"1737\" height=\"486\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/CPUBound.d2d32cb2626c.png&amp;w=434&amp;sig=6bbdd46096cca225291e357d86691e08939744a4 434w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/CPUBound.d2d32cb2626c.png&amp;w=868&amp;sig=88b3c863f684b8109ed17e4a013aab1e666dba1f 868w, https://files.realpython.com/media/CPUBound.d2d32cb2626c.png 1737w\" sizes=\"75vw\" alt=\"Timing Diagram of an CPU Bound Program\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the I/O-bound examples, the CPU-bound examples are usually fairly consistent in their run times. This one takes about 7.8 seconds on my machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Duration 5.663684606552124 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we can do better than this. This is all running on a single CPU with no concurrency. Let’s see what we can do to make it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threading and asyncio Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much do you think rewriting this code using threading or asyncio will speed this up?\n",
    "\n",
    "If you answered “Not at all,” give yourself a cookie. If you answered, “It will slow it down,” give yourself two cookies.\n",
    "\n",
    "Here’s why: In your I/O-bound example above, much of the overall time was spent waiting for slow operations to finish. threading and asyncio sped this up by allowing you to overlap the times you were waiting instead of doing them sequentially.\n",
    "\n",
    "On a CPU-bound problem, however, there is no waiting. The CPU is cranking away as fast as it can to finish the problem. In Python, both threads and tasks run on the same CPU in the same process. That means that the one CPU is doing all of the work of the non-concurrent code plus the extra work of setting up threads or tasks. It takes more than 10 seconds:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I’ve written up a <code>threading</code> version of this code and placed it with the other example code in the <a href=\"https://github.com/realpython/materials/tree/master/concurrency-overview\">GitHub repo</a> so you can go test this yourself. Let’s not look at that just yet, however.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 10.384260177612305 seconds\n"
     ]
    }
   ],
   "source": [
    "# cpu_bound_thread.py\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "\n",
    "def cpu_bound(number):\n",
    "    return sum(i * i for i in range(number))\n",
    "\n",
    "\n",
    "def find_sums(numbers):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(cpu_bound, numbers)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numbers = [5_000_000 + x for x in range(10)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    find_sums(numbers)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Duration {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU-Bound multiprocessing Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’ve finally reached where multiprocessing really shines. Unlike the other concurrency libraries, multiprocessing is explicitly designed to share heavy CPU workloads across multiple CPUs. Here’s what its execution timing diagram looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-100\" src=\"https://files.realpython.com/media/CPUMP.69c1a7fad9c4.png\" width=\"1893\" height=\"1407\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/CPUMP.69c1a7fad9c4.png&amp;w=473&amp;sig=d831dfde47c654e01efbab1ad1368ba7dde37f2c 473w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/CPUMP.69c1a7fad9c4.png&amp;w=946&amp;sig=f3fec21b71d848313c1d249e8fd32f01c99e1bcb 946w, https://files.realpython.com/media/CPUMP.69c1a7fad9c4.png 1893w\" sizes=\"75vw\" alt=\"Timing Diagram of a CPU-Bound Multiprocessing Solution\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s what the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 2.66279673576355 seconds\n"
     ]
    }
   ],
   "source": [
    "# cpu_bound_multiproc.py\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "def cpu_bound(number):\n",
    "    return sum(i * i for i in range(number))\n",
    "\n",
    "\n",
    "def find_sums(numbers):\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.map(cpu_bound, numbers)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numbers = [5_000_000 + x for x in range(10)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    find_sums(numbers)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Duration {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little of this code had to change from the non-concurrent version. You had to import multiprocessing and then just change from looping through the numbers to creating a multiprocessing.Pool object and using its .map() method to send individual numbers to worker-processes as they become free.\n",
    "\n",
    "This was just what you did for the I/O-bound multiprocessing code, but here you don’t need to worry about the Session object.\n",
    "\n",
    "As mentioned above, the processes optional parameter to the multiprocessing.Pool() constructor deserves some attention. You can specify how many Process objects you want created and managed in the Pool. By default, it will determine how many CPUs are in your machine and create a process for each one. While this works great for our simple example, you might want to have a little more control in a production environment.\n",
    "\n",
    "Also, as we mentioned in the first section about threading, the multiprocessing.Pool code is built upon building blocks like Queue and Semaphore that will be familiar to those of you who have done multithreaded and multiprocessing code in other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the multiprocessing Version Rocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiprocessing version of this example is great because it’s relatively easy to set up and requires little extra code. It also takes full advantage of the CPU power in your computer.\n",
    "\n",
    "Hey, that’s exactly what I said the last time we looked at multiprocessing. The big difference is that this time it is clearly the best option. It takes 2.5 seconds on my machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Problems With the multiprocessing Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some drawbacks to using multiprocessing. They don’t really show up in this simple example, but splitting your problem up so each processor can work independently can sometimes be difficult.\n",
    "\n",
    "Also, many solutions require more communication between the processes. This can add some complexity to your solution that a non-concurrent program would not need to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def cpu_bound(number):\n",
    "    total_sum = 0\n",
    "    for i in range(number):\n",
    "        total_sum += (i*i)\n",
    "    return total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration 0.14608001708984375 seconds\n"
     ]
    }
   ],
   "source": [
    "def find_sums(numbers):\n",
    "    for number in numbers:\n",
    "        cpu_bound(number)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    numbers = [5_000_000 + x for x in range(10)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    find_sums(numbers)\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Duration {duration} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve covered a lot of ground here, so let’s review some of the key ideas and then discuss some decision points that will help you determine which, if any, concurrency module you want to use in your project.\n",
    "\n",
    "The first step of this process is deciding if you should use a concurrency module. While the examples here make each of the libraries look pretty simple, concurrency always comes with extra complexity and can often result in bugs that are difficult to find.\n",
    "\n",
    "Hold out on adding concurrency until you have a known performance issue and then determine which type of concurrency you need. As Donald Knuth has said, “Premature optimization is the root of all evil (or at least most of it) in programming.”\n",
    "\n",
    "Once you’ve decided that you should optimize your program, figuring out if your program is CPU-bound or I/O-bound is a great next step. Remember that I/O-bound programs are those that spend most of their time waiting for something to happen while CPU-bound programs spend their time processing data or crunching numbers as fast as they can.\n",
    "\n",
    "As you saw, CPU-bound problems only really gain from using multiprocessing. threading and asyncio did not help this type of problem at all.\n",
    "\n",
    "For I/O-bound problems, there’s a general rule of thumb in the Python community: “Use asyncio when you can, threading when you must.” asyncio can provide the best speed up for this type of program, but sometimes you will require critical libraries that have not been ported to take advantage of asyncio. Remember that any task that doesn’t give up control to the event loop will block all of the other tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
