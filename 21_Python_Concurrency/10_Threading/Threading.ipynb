{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viri:\n",
    "- [An Intro to Threading in Python](https://realpython.com/intro-to-python-threading/)\n",
    "- [PyBay 2017 Keynote](https://pybay.com/site_media/slides/raymond2017-keynote/threading.html)\n",
    "- [Educative - The threading Module](https://www.educative.io/courses/python-201-interactively-learn-advanced-concepts-in-python-3/gxpQM7PDjKr)\n",
    "- [threading — Manage Concurrent Operations Within a Process](https://pymotw.com/3/threading/index.html)\n",
    "- [The Other Async (Threads + Async = ❤️) - VIDEO](https://www.youtube.com/watch?v=x1ndXuw7S0s)\n",
    "- [Raymond Hettinger, Keynote on Concurrency, PyBay 2017](https://www.youtube.com/watch?v=9zinZmE3Ogk)\n",
    "- [Thinking about Concurrency, Raymond Hettinger, Python core developer](https://www.youtube.com/watch?v=Bv25Dwe84g0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python threading allows you to have different parts of your program run concurrently and can simplify your design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the threads in Python work best with I/O operations, such as downloading resources from the Internet or reading files and directories on your computer. If you need to do something that will be CPU intensive, then you will want to look at Python’s multiprocessing module instead. The reason for this is that Python has the Global Interpreter Lock (GIL) that basically makes all threads run inside of one master thread. Because of this, when you go to run multiple CPU intensive operations with threads, you may find that it actually runs slower. So we will be focusing on what threads do best: I/O operations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is a Thread?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thread let’s you run a piece of long running code as if it were a separate program. It’s kind of like calling subprocess except that you are calling a function or class instead of a separate program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thread is a separate flow of execution. This means that your program will have two things happening at once. But for most Python 3 implementations the different threads do not actually execute at the same time: they merely appear to.\n",
    "\n",
    "It’s tempting to think of threading as having two (or more) different processors running on your program, each one doing an independent task at the same time. That’s almost right. The threads may be running on different processors, but they will only be running one at a time.\n",
    "\n",
    "Getting multiple tasks running simultaneously requires a non-standard implementation of Python, writing some of your code in a different language, or using multiprocessing which comes with some extra overhead.\n",
    "\n",
    "Because of the way CPython implementation of Python works, threading may not speed up all tasks. This is due to interactions with the GIL that essentially limit one Python thread to run at a time.\n",
    "\n",
    "Tasks that spend much of their time waiting for external events are generally good candidates for threading. Problems that require heavy CPU computation and spend little time waiting for external events might not run faster at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is true for code written in Python and running on the standard CPython implementation. If your threads are written in C they have the ability to release the GIL and run concurrently. If you are running on a different Python implementation, check with the documentation too see how it handles threads.\n",
    "\n",
    "If you are running a standard Python implementation, writing in only Python, and have a CPU-bound problem, you should check out the multiprocessing module instead.\n",
    "\n",
    "Architecting your program to use threading can also provide gains in design clarity. Most of the examples you’ll learn about in this tutorial are not necessarily going to run faster because they use threads. Using threading in them helps to make the design cleaner and easier to reason about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve got an idea of what a thread is, let’s learn how to make one. The Python standard library provides threading, which contains most of the primitives you’ll see in this article. Thread, in this module, nicely encapsulates threads, providing a clean interface to work with them.\n",
    "\n",
    "To start a separate thread, you create a Thread instance and then tell it to .start():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:36: Main : before creating thread\n",
      "11:48:36: Main : before running thread\n",
      "11:48:36: Thread 1: starting\n",
      "11:48:36: Main : wait for the thread to finish\n",
      "11:48:36: Main : all done\n",
      "11:48:38: Thread 1: finishing\n"
     ]
    }
   ],
   "source": [
    "# single_thread.py\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def thread_function(name):\n",
    "    logging.info(\"Thread %s: starting\", name)\n",
    "    time.sleep(2)\n",
    "    logging.info(\"Thread %s: finishing\", name)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "    \n",
    "    logging.info(\"Main : before creating thread\")\n",
    "    x = threading.Thread(target=thread_function, args=(1,))\n",
    "    logging.info(\"Main : before running thread\")\n",
    "    x.start()\n",
    "    logging.info(\"Main : wait for the thread to finish\")\n",
    "    # x.join()\n",
    "    logging.info(\"Main : all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look around the logging statements, you can see that the main section is creating and starting the thread:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    x = threading.Thread(target=thread_function, args=(1,))\n",
    "    x.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a Thread, you pass it a function and a list containing the arguments to that function. In this case, you’re telling the Thread to run thread_function() and to pass it 1 as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this article, you’ll use sequential integers as names for your threads. There is threading.get_ident(), which returns a unique name for each thread, but these are usually neither short nor easily readable.\n",
    "\n",
    "thread_function() itself doesn’t do much. It simply logs some messages with a time.sleep() in between them.\n",
    "\n",
    "When you run this program as it is (with line twenty commented out), the output will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Main : before creating thread\n",
    "    Main : before running thread\n",
    "    Thread 1: starting\n",
    "    Main : wait for the thread to finish\n",
    "    Main : all done\n",
    "    Thread 1: finishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll notice that the Thread finished after the Main section of your code did. You’ll come back to why that is and talk about the mysterious line twenty in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daemon Threads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Up to this point, the example programs have implicitly waited to exit until all threads have completed their work. Sometimes programs spawn a thread as a daemon that runs without blocking the main program from exiting. Using daemon threads is useful for services where there may not be an easy way to interrupt the thread, or where letting the thread die in the middle of its work does not lose or corrupt data (for example, a thread that generates “heart beats” for a service monitoring tool). To mark a thread as a daemon, pass daemon=True when constructing it or call its set_daemon() method with True. The default is for threads to not be daemons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, a daemon is a process that runs in the background.\n",
    "\n",
    "Python threading has a more specific meaning for daemon. A daemon thread will shut down immediately when the program exits. One way to think about these definitions is to consider the daemon thread a thread that runs in the background without worrying about shutting it down.\n",
    "\n",
    "If a program is running Threads that are not daemons, then the program will wait for those threads to complete before it terminates. Threads that are daemons, however, are just killed wherever they are when the program is exiting.\n",
    "\n",
    "Let’s look a little more closely at the output of your program above. The last two lines are the interesting bit. When you run the program, you’ll notice that there is a pause (of about 2 seconds) after `__main__` has printed its all done message and before the thread is finished.\n",
    "\n",
    "This pause is Python waiting for the non-daemonic thread to complete. When your Python program ends, part of the shutdown process is to clean up the threading routine.\n",
    "\n",
    "If you look at the source for Python threading, you’ll see that threading._shutdown() walks through all of the running threads and calls .join() on every one that does not have the daemon flag set.\n",
    "\n",
    "So your program waits to exit because the thread itself is waiting in a sleep. As soon as it has completed and printed the message, .join() will return and the program can exit.\n",
    "\n",
    "Frequently, this behavior is what you want, but there are other options available to us. Let’s first repeat the program with a daemon thread. You do that by changing how you construct the Thread, adding the daemon=True flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:27:13: Main : before creating thread\n",
      "12:27:13: Main : before running thread\n",
      "12:27:13: Thread 1: starting\n",
      "12:27:13: Main : wait for the thread to finish\n",
      "12:27:13: Main : all done\n",
      "12:27:15: Thread 1: finishing\n"
     ]
    }
   ],
   "source": [
    "# single_thread.py\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def thread_function(name):\n",
    "    logging.info(\"Thread %s: starting\", name)\n",
    "    time.sleep(2)\n",
    "    logging.info(\"Thread %s: finishing\", name)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "    \n",
    "    logging.info(\"Main : before creating thread\")\n",
    "    x = threading.Thread(target=thread_function, args=(1,), daemon=True)\n",
    "    logging.info(\"Main : before running thread\")\n",
    "    x.start()\n",
    "    logging.info(\"Main : wait for the thread to finish\")\n",
    "    #x.join()\n",
    "    logging.info(\"Main : all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the program now, you should see this output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Main : before creating thread\n",
    "    Main : before running thread\n",
    "    Thread 1: starting\n",
    "    Main : wait for the thread to finish\n",
    "    Main : all done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here is that the final line of the output is missing. thread_function() did not get a chance to complete. It was a daemon thread, so when __main__ reached the end of its code and the program wanted to finish, the daemon was killed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join() a Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daemon threads are handy, but what about when you want to wait for a thread to stop? What about when you want to do that and not exit your program? Now let’s go back to your original program and look at that commented out line twenty:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # x.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tell one thread to wait for another thread to finish, you call .join(). If you uncomment that line, the main thread will pause and wait for the thread x to complete running.\n",
    "\n",
    "Did you test this on the code with the daemon thread or the regular thread? It turns out that it doesn’t matter. If you .join() a thread, that statement will wait until either kind of thread is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Many Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example code so far has only been working with two threads: the main thread and one you started with the threading.Thread object.\n",
    "\n",
    "Frequently, you’ll want to start a number of threads and have them do interesting work. Let’s start by looking at the harder way of doing that, and then you’ll move on to an easier method.\n",
    "\n",
    "The harder way of starting multiple threads is the one you already know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:57:20: Main : create and start thread 0.\n",
      "17:57:20: Thread 0: starting\n",
      "17:57:20: Main : create and start thread 1.\n",
      "17:57:20: Thread 1: starting\n",
      "17:57:20: Main : create and start thread 2.\n",
      "17:57:20: Thread 2: starting\n",
      "17:57:20: Main : before joining thread 0.\n",
      "17:57:22: Thread 0: finishing\n",
      "17:57:22: Main : thread 0 done\n",
      "17:57:22: Main : before joining thread 1.\n",
      "17:57:22: Thread 1: finishing\n",
      "17:57:22: Main : thread 1 done\n",
      "17:57:22: Main : before joining thread 2.\n",
      "17:57:22: Thread 2: finishing\n",
      "17:57:22: Main : thread 2 done\n"
     ]
    }
   ],
   "source": [
    "#multiple_threads.py\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def thread_function(name):\n",
    "    logging.info(\"Thread %s: starting\", name)\n",
    "    time.sleep(2)\n",
    "    logging.info(\"Thread %s: finishing\", name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    threads = list()\n",
    "    for index in range(3):\n",
    "        logging.info(\"Main : create and start thread %d.\", index)\n",
    "        x = threading.Thread(target=thread_function, args=(index,))\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "\n",
    "    for index, thread in enumerate(threads):\n",
    "        logging.info(\"Main : before joining thread %d.\", index)\n",
    "        thread.join()\n",
    "        logging.info(\"Main : thread %d done\", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the same mechanism you saw above to start a thread, create a Thread object, and then call .start(). The program keeps a list of Thread objects so that it can then wait for them later using .join().\n",
    "\n",
    "Running this code multiple times will likely produce some interesting results. Here’s an example output from my machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Main : create and start thread 0.\n",
    "    Thread 0: starting\n",
    "    Main : create and start thread 1.\n",
    "    Thread 1: starting\n",
    "    Main : create and start thread 2.\n",
    "    Thread 2: starting\n",
    "    Main : before joining thread 0.\n",
    "    Thread 2: finishing\n",
    "    Thread 1: finishing\n",
    "    Thread 0: finishing\n",
    "    Main : thread 0 done\n",
    "    Main : before joining thread 1.\n",
    "    Main : thread 1 done\n",
    "    Main : before joining thread 2.\n",
    "    Main : thread 2 done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you walk through the output carefully, you’ll see all three threads getting started in the order you might expect, but in this case they finish in the opposite order! Multiple runs will produce different orderings. Look for the Thread x: finishing message to tell you when each thread is done.\n",
    "\n",
    "The order in which threads are **run is determined by the operating system and can be quite hard to predict**. It may (and likely will) vary from run to run, so you need to be aware of that when you design algorithms that use threading.\n",
    "\n",
    "Fortunately, Python gives you several primitives that you’ll look at later to help coordinate threads and get them running together. Before that, let’s look at how to make managing a group of threads a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have more than one thread, then you may find yourself needing to consider how to avoid conflicts. What I mean by this is that you may have a **use case where more than one thread will need to access the same resource at the same time**. If you don’t think about these issues and plan accordingly, then you will end up with some issues that always happen at the worst of times and usually in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you move on to some of the other features tucked away in Python threading, let’s talk a bit about one of the more difficult issues you’ll run into when writing threaded programs: race conditions.\n",
    "\n",
    "Once you’ve seen what a race condition is and looked at one happening, you’ll move on to some of the primitives provided by the standard library to prevent race conditions from happening.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race conditions can occur when two or more threads access a shared piece of data or resource. In this example, you’re going to create a large race condition that happens every time, but **be aware that most race conditions are not this obvious**. Frequently, they only occur rarely, and they can produce confusing results. As you can imagine, this makes them quite difficult to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Scripting style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with working code that is clear, simple, and runs top to bottom. This is easy to develop and test incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripting_ex.py\n",
    "counter = 0\n",
    "\n",
    "print('Starting up')\n",
    "for i in range(10):\n",
    "    counter += 1\n",
    "    print('The count is %d' % counter)\n",
    "    print('---------------')\n",
    "print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get your app tested and debugged in a singled threaded mode first before you start threading. Threading NEVER makes debugging easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Function style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A next step in development is to factor re-usable code into functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_ex.py\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "\n",
    "    counter += 1\n",
    "    print('The count is %d' % counter)\n",
    "    print('---------------')\n",
    "\n",
    "print('Starting up')\n",
    "for i in range(10):\n",
    "    worker()\n",
    "print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is just a matter of launching a few worker threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multithread_ex.py\n",
    "import threading\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "\n",
    "    counter += 1\n",
    "    print('The count is %d' % counter)\n",
    "    print('---------------')\n",
    "\n",
    "print('Starting up')\n",
    "for i in range(10):\n",
    "    threading.Thread(target=worker).start()\n",
    "print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you normally wouldn’t want to print your output to stdout. This can end up being a really jumbled mess when you do. Instead, **you should use Python’s logging module. It’s thread-safe and does an excellent job.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing proves the code is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot the race conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most people spot the “counter increment” race condition, but most don’t immediately see the “print function” race condition.\n",
    "\n",
    "Testing cannot prove the absence of errors. It is still useful, don’t rely on it. Many interest racing conditions don’t reveal themselves in test environments.\n",
    "\n",
    "Why didn’t testing reveal the flaws?\n",
    "\n",
    "What can we do to improve the effectiveness of testing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzing is a technique for amplifying race conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzing.py\n",
    "import threading, time, random\n",
    "\n",
    "##########################################################################################\n",
    "# Fuzzing is a technique for amplifying race condition errors to make them more visible\n",
    "\n",
    "FUZZ = True\n",
    "\n",
    "def fuzz():\n",
    "    if FUZZ:\n",
    "        time.sleep(random.random())\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "\n",
    "    fuzz()\n",
    "    oldcnt = counter\n",
    "    fuzz()\n",
    "    counter = oldcnt + 1\n",
    "    fuzz()\n",
    "    print('The count is %d' % counter, end='')\n",
    "    fuzz()\n",
    "    print()\n",
    "    fuzz()\n",
    "    print('---------------', end='')\n",
    "    fuzz()\n",
    "    print()\n",
    "    fuzz()\n",
    "\n",
    "print('Starting up')\n",
    "fuzz()\n",
    "for i in range(10):\n",
    "    threading.Thread(target=worker).start()\n",
    "    fuzz()\n",
    "print('Finishing up')\n",
    "fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is limited to relatively small blocks of code and is imperfect in that is can’t prove the absence of errors.\n",
    "\n",
    "Still, fuzzed tests do reveal the presence of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race Conditions explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Fortunately, this race condition will happen every time, and you’ll walk through it in detail to explain what is happening.\n",
    "\n",
    "For this example, you’re going to write a class that updates a database. Okay, you’re not really going to have a database: you’re just going to fake it, because that’s not the point of this article.\n",
    "\n",
    "Your FakeDatabase will have `.__init__()` and .update() methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# racecond.py\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "class FakeDatabase:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "\n",
    "    def update(self, name):\n",
    "        logging.info(\"Thread %s: starting update\", name)\n",
    "        local_copy = self.value\n",
    "        local_copy += 1\n",
    "        time.sleep(0.1)\n",
    "        self.value = local_copy\n",
    "        logging.info(\"Thread %s: finishing update\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FakeDatabase is keeping track of a single number: .value. This is going to be the shared data on which you’ll see the race condition.\n",
    "\n",
    "`.__init__()` simply initializes .value to zero. So far, so good.\n",
    "\n",
    ".update() looks a little strange. It’s simulating reading a value from a database, doing some computation on it, and then writing a new value back to the database.\n",
    "\n",
    "In this case, reading from the database just means copying .value to a local variable. The computation is just to add one to the value and then .sleep() for a little bit. Finally, it writes the value back by copying the local value back to .value.\n",
    "\n",
    "Here’s how you’ll use this FakeDatabase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:15:54: Testing update. Starting value is 0.\n",
      "18:15:54: Thread 0: starting update\n",
      "18:15:54: Thread 1: starting update\n",
      "18:15:54: Thread 0: finishing update\n",
      "18:15:54: Thread 1: finishing update\n",
      "18:15:54: Testing update. Ending value is 1.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    database = FakeDatabase()\n",
    "    threads = list()\n",
    "    logging.info(\"Testing update. Starting value is %d.\", database.value)\n",
    "    for index in range(3):\n",
    "        x = threading.Thread(target=database.update, args=(index,))\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "\n",
    "    for index, thread in enumerate(threads):\n",
    "        thread.join()\n",
    "\n",
    "    logging.info(\"Testing update. Ending value is %d.\", database.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the usage above, index is passed as the first and only positional argument to database.update(). You’ll see later in this article where you can pass multiple arguments in a similar manner.\n",
    "\n",
    "Since each thread runs .update(), and .update() adds one to .value, you might expect database.value to be 2 when it’s printed out at the end. But you wouldn’t be looking at this example if that was the case. If you run the above code, the output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Testing unlocked update. Starting value is 0.\n",
    "    Thread 0: starting update\n",
    "    Thread 1: starting update\n",
    "    Thread 0: finishing update\n",
    "    Thread 1: finishing update\n",
    "    Testing unlocked update. Ending value is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have expected that to happen, but let’s look at the details of what’s really going on here, as that will make the solution to this problem easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you dive into this issue with two threads, let’s step back and talk a bit about some details of how threads work.\n",
    "\n",
    "You won’t be diving into all of the details here, as that’s not important at this level. We’ll also be simplifying a few things in a way that won’t be technically accurate but will give you the right idea of what is happening.\n",
    "\n",
    "When you tell your ThreadPoolExecutor to run each thread, you tell it which function to run and what parameters to pass to it: `executor.submit(database.update, index)`.\n",
    "\n",
    "The result of this is that each of the threads in the pool will call database.update(index). Note that database is a reference to the one FakeDatabase object created in `__main__`. Calling .update() on that object calls an instance method on that object.\n",
    "\n",
    "Each thread is going to have a reference to the same FakeDatabase object, database. Each thread will also have a unique value, index, to make the logging statements a bit easier to read:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-66\" src=\"https://files.realpython.com/media/intro-threading-shared-database.267a5d8c6aa1.png\" width=\"1623\" height=\"663\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-shared-database.267a5d8c6aa1.png&amp;w=405&amp;sig=a2cf6c8e812b6c948114c50b5402d51fb25ed155 405w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-shared-database.267a5d8c6aa1.png&amp;w=811&amp;sig=3708e9d6821e459c00fe576ffaa01cd2cab14b70 811w, https://files.realpython.com/media/intro-threading-shared-database.267a5d8c6aa1.png 1623w\" sizes=\"75vw\" alt=\"Thread 1 and Thread 2 use the same shared database.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the thread starts running .update(), it has its own version of all of the data local to the function. In the case of .update(), this is local_copy. This is definitely a good thing. Otherwise, two threads running the same function would always confuse each other. It means that all variables that are scoped (or local) to a function are thread-safe.\n",
    "\n",
    "Now you can start walking through what happens if you run the program above with a single thread and a single call to .update().\n",
    "\n",
    "The image below steps through the execution of .update() if only a single thread is run. The statement is shown on the left followed by a diagram showing the values in the thread’s local_value and the shared database.value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-66\" src=\"https://files.realpython.com/media/intro-threading-single-thread.85204fa11210.png\" width=\"400\" height=\"1000\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-single-thread.85204fa11210.png&amp;w=345&amp;sig=3964a752874e9128c41941a52f9c7342e2f1e12a 345w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-single-thread.85204fa11210.png&amp;w=691&amp;sig=552252777e57ea21cc6aaa6ed39263aa1e60c1da 691w, https://files.realpython.com/media/intro-threading-single-thread.85204fa11210.png 1383w\" sizes=\"75vw\" alt=\"Single thread modifying a shared database\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram is laid out so that time increases as you move from top to bottom. It begins when Thread 1 is created and ends when it is terminated.\n",
    "\n",
    "When Thread 1 starts, FakeDatabase.value is zero. The first line of code in the method, local_copy = self.value, copies the value zero to the local variable. Next it increments the value of local_copy with the local_copy += 1 statement. You can see .value in Thread 1 getting set to one.\n",
    "\n",
    "Next time.sleep() is called, which makes the current thread pause and allows other threads to run. Since there is only one thread in this example, this has no effect.\n",
    "\n",
    "When Thread 1 wakes up and continues, it copies the new value from local_copy to FakeDatabase.value, and then the thread is complete. You can see that database.value is set to one.\n",
    "\n",
    "So far, so good. You ran .update() once and FakeDatabase.value was incremented to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to the race condition, the two threads will be running concurrently but not at the same time. They will each have their own version of local_copy and will each point to the same database. It is this shared database object that is going to cause the problems.\n",
    "\n",
    "The program starts with Thread 1 running .update():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-66\" src=\"https://files.realpython.com/media/intro-threading-two-threads-part1.c1c0e65a8481.png\" width=\"500\" height=\"400\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-two-threads-part1.c1c0e65a8481.png&amp;w=488&amp;sig=5a03e51ef3b111ab7525d287bfc56571927da0db 488w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-two-threads-part1.c1c0e65a8481.png&amp;w=976&amp;sig=fe92c3576eefae044f17a55010816e0d1c9ec240 976w, https://files.realpython.com/media/intro-threading-two-threads-part1.c1c0e65a8481.png 1953w\" sizes=\"75vw\" alt=\"Thread 1 gets a copy of shared data and increments it.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Thread 1 calls time.sleep(), it allows the other thread to start running. This is where things get interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thread 2 starts up and does the same operations. It’s also copying database.value into its private local_copy, and this shared database.value has not yet been updated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-66\" src=\"https://files.realpython.com/media/intro-threading-two-threads-part2.df42d4fbfe21.png\" width=\"500\" height=\"400\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-two-threads-part2.df42d4fbfe21.png&amp;w=503&amp;sig=469e3f134c8c24d34a4923a3afa7c4261b199935 503w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-two-threads-part2.df42d4fbfe21.png&amp;w=1006&amp;sig=59d3261bd00bf21276180b03684a5b36cc949c01 1006w, https://files.realpython.com/media/intro-threading-two-threads-part2.df42d4fbfe21.png 2013w\" sizes=\"75vw\" alt=\"Thread 2 gets a copy of shared data and increments it.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Thread 2 finally goes to sleep, the shared database.value is still unmodified at zero, and both private versions of local_copy have the value one.\n",
    "\n",
    "Thread 1 now wakes up and saves its version of local_copy and then terminates, giving Thread 2 a final chance to run. Thread 2 has no idea that Thread 1 ran and updated database.value while it was sleeping. It stores its version of local_copy into database.value, also setting it to one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img loading=\"lazy\" class=\"img-fluid mx-auto d-block w-66\" src=\"https://files.realpython.com/media/intro-threading-two-threads-part3.18576920f88f.png\" width=\"500\" height=\"400\" srcset=\"https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-two-threads-part3.18576920f88f.png&amp;w=503&amp;sig=3b11ef66873019a9edf48a7ff7445bc60d6daada 503w, https://robocrop.realpython.net/?url=https%3A//files.realpython.com/media/intro-threading-two-threads-part3.18576920f88f.png&amp;w=1006&amp;sig=bb0878750a37d23cfca07f30e95f289b1ee82770 1006w, https://files.realpython.com/media/intro-threading-two-threads-part3.18576920f88f.png 2013w\" sizes=\"75vw\" alt=\"Both threads write 1 to shared database.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two threads have interleaving access to a single shared object, overwriting each other’s results. Similar race conditions can arise when one thread frees memory or closes a file handle before the other thread is finished accessing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Synchronization Using Lock\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to avoid or solve race conditions. You won’t look at all of them here, but there are a couple that are used frequently. Let’s start with Lock.\n",
    "\n",
    "To solve your race condition above, you need to find a way to allow only one thread at a time into the read-modify-write section of your code. The most common way to do this is called Lock in Python. In some other languages this same idea is called a mutex. Mutex comes from MUTual EXclusion, which is exactly what a Lock does.\n",
    "\n",
    "A Lock is an object that acts like a hall pass. Only one thread at a time can have the Lock. Any other thread that wants the Lock must wait until the owner of the Lock gives it up.\n",
    "\n",
    "The basic functions to do this are .acquire() and .release(). A thread will call my_lock.acquire() to get the lock. If the lock is already held, the calling thread will wait until it is released. There’s an important point here. If one thread gets the lock but never gives it back, your program will be stuck. You’ll read more about this later.\n",
    "\n",
    "Fortunately, Python’s Lock will also operate as a context manager, so you can use it in a with statement, and it gets released automatically when the with block exits for any reason.\n",
    "\n",
    "Let’s look at the FakeDatabase with a Lock added to it. The calling function stays the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locked_db.py\n",
    "\n",
    "class FakeDatabase:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def locked_update(self, name):\n",
    "        logging.info(\"Thread %s: starting update\", name)\n",
    "        logging.debug(\"Thread %s about to lock\", name)\n",
    "        with self._lock:\n",
    "            logging.debug(\"Thread %s has lock\", name)\n",
    "            local_copy = self.value\n",
    "            local_copy += 1\n",
    "            time.sleep(0.1)\n",
    "            self.value = local_copy\n",
    "            logging.debug(\"Thread %s about to release lock\", name)\n",
    "        logging.debug(\"Thread %s after release\", name)\n",
    "        logging.info(\"Thread %s: finishing update\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    database = FakeDatabase()\n",
    "    threads = list()\n",
    "    logging.info(\"Testing update. Starting value is %d.\", database.value)\n",
    "    for index in range(3):\n",
    "        x = threading.Thread(target=database.locked_update, args=(index,))\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "\n",
    "    for index, thread in enumerate(threads):\n",
    "        thread.join()\n",
    "\n",
    "    logging.info(\"Testing update. Ending value is %d.\", database.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than adding a bunch of debug logging so you can see the locking more clearly, the big change here is to add a member called ._lock, which is a threading.Lock() object. This ._lock is initialized in the unlocked state and locked and released by the with statement.\n",
    "\n",
    "It’s worth noting here that the thread running this function will hold on to that Lock until it is completely finished updating the database. In this case, that means it will hold the Lock while it copies, updates, sleeps, and then writes the value back to the database.\n",
    "\n",
    "If you run this version with logging set to warning level, you’ll see this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    20:54:01: Testing update. Starting value is 0.\n",
    "    20:54:01: Thread 0: starting update\n",
    "    20:54:01: Thread 0 about to lock\n",
    "    20:54:01: Thread 0 has lock\n",
    "    20:54:01: Thread 1: starting update\n",
    "    20:54:01: Thread 1 about to lock\n",
    "    20:54:01: Thread 0 about to release lock\n",
    "    20:54:01: Thread 0 after release\n",
    "    20:54:01: Thread 0: finishing update\n",
    "    20:54:01: Thread 1 has lock\n",
    "    20:54:01: Thread 1 about to release lock\n",
    "    20:54:01: Thread 1 after release\n",
    "    20:54:01: Thread 1: finishing update\n",
    "    20:54:01: Testing update. Ending value is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this output you can see Thread 0 acquires the lock and is still holding it when it goes to sleep. Thread 1 then starts and attempts to acquire the same lock. Because Thread 0 is still holding it, Thread 1 has to wait. This is the mutual exclusion that a Lock provides.\n",
    "\n",
    "Many of the examples in the rest of this article will have WARNING and DEBUG level logging. We’ll generally only show the WARNING level output, as the DEBUG logs can be quite lengthy. Try out the programs with the logging turned up and see what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locks_threading_ex.py\n",
    "import threading, time, random\n",
    "\n",
    "counter_lock = threading.Lock()\n",
    "printer_lock = threading.Lock()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "    with counter_lock:\n",
    "        counter += 1\n",
    "        with printer_lock:\n",
    "            print('The count is %d' % counter)\n",
    "            print('---------------')\n",
    "\n",
    "with printer_lock:\n",
    "    print('Starting up')\n",
    "\n",
    "worker_threads = []\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target=worker)\n",
    "    worker_threads.append(t)\n",
    "    t.start()\n",
    "for t in worker_threads:\n",
    "    t.join()\n",
    "\n",
    "with printer_lock:\n",
    "    print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Locks don’t lock anything. They are just flags and can be ignored. It is a cooperative tool, not an enforced tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In general, locks should be considered a low level primitive that is difficult to reason about in non-trivial examples. For more complex applications, you’re almost always better off with using atomic message queues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The more locks you are acquire at one time, the more you lose the advantages of concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you move on, you should look at a common problem when using Locks. As you saw, if the Lock has already been acquired, a second call to .acquire() will wait until the thread that is holding the Lock calls .release(). What do you think happens when you run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "l = threading.Lock()\n",
    "print(\"before first acquire\")\n",
    "l.acquire()\n",
    "print(\"before second acquire\")\n",
    "l.acquire()\n",
    "print(\"acquired lock twice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the program calls l.acquire() the second time, it hangs waiting for the Lock to be released. In this example, you can fix the deadlock by removing the second call, but deadlocks usually happen from one of two subtle things:\n",
    "1. An implementation bug where a Lock is not released properly\n",
    "2. A design issue where a utility function needs to be called by functions that might or might not already have the Lock\n",
    "\n",
    "The first situation happens sometimes, but using a Lock as a context manager greatly reduces how often. It is recommended to write code whenever possible to make use of context managers, as they help to avoid situations where an exception skips you over the .release() call.\n",
    "\n",
    "The design issue can be a bit trickier in some languages. Thankfully, Python threading has a second object, called RLock, that is designed for just this situation. It allows a thread to .acquire() an RLock multiple times before it calls .release(). That thread is still required to call .release() the same number of times it called .acquire(), but it should be doing that anyway.\n",
    "\n",
    "Lock and RLock are two of the basic tools used in threaded programming to prevent race conditions. There are a few other that work in different ways. Before you look at them, let’s shift to a slightly different problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producer-Consumer Threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Producer-Consumer Problem is a standard computer science problem used to look at threading or process synchronization issues. You’re going to look at a variant of it to get some ideas of what primitives the Python threading module provides.\n",
    "\n",
    "For this example, you’re going to imagine a program that needs to read messages from a network and write them to disk. The program does not request a message when it wants. It must be listening and accept messages as they come in. The messages will not come in at a regular pace, but will be coming in bursts. This part of the program is called the producer.\n",
    "\n",
    "On the other side, once you have a message, you need to write it to a database. The database access is slow, but fast enough to keep up to the average pace of messages. It is not fast enough to keep up when a burst of messages comes in. This part is the consumer.\n",
    "\n",
    "In between the producer and the consumer, you will create a Pipeline that will be the part that changes as you learn about different synchronization objects.\n",
    "\n",
    "That’s the basic layout. Let’s look at a solution using Lock. It doesn’t work perfectly, but it uses tools you already know, so it’s a good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer-Consumer Using Lock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an article about Python threading, and since you just read about the Lock primitive, let’s try to solve this problem with two threads using a Lock or two.\n",
    "\n",
    "The general design is that there is a producer thread that reads from the fake network and puts the message into a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_locks.py\n",
    "import random \n",
    "import concurrent.futures\n",
    "import logging\n",
    "import time\n",
    "import threading\n",
    "\n",
    "SENTINEL = object()\n",
    "\n",
    "def producer(pipeline):\n",
    "    \"\"\"Pretend we're getting a message from the network.\"\"\"\n",
    "    for index in range(10):\n",
    "        message = random.randint(1, 101)\n",
    "        logging.info(\"Producer got message: %s\", message)\n",
    "        pipeline.set_message(message, \"Producer\")\n",
    "\n",
    "    # Send a sentinel message to tell consumer we're done\n",
    "    pipeline.set_message(SENTINEL, \"Producer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a fake message, the producer gets a random number between one and one hundred. It calls .set_message() on the pipeline to send it to the consumer.\n",
    "\n",
    "The producer also uses a SENTINEL value to signal the consumer to stop after it has sent ten values. This is a little awkward, but don’t worry, you’ll see ways to get rid of this SENTINEL value after you work through this example.\n",
    "\n",
    "On the other side of the pipeline is the consumer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(pipeline):\n",
    "    \"\"\"Pretend we're saving a number in the database.\"\"\"\n",
    "    message = 0\n",
    "    while message is not SENTINEL:\n",
    "        message = pipeline.get_message(\"Consumer\")\n",
    "        if message is not SENTINEL:\n",
    "            logging.info(\"Consumer storing message: %s\", message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumer reads a message from the pipeline and writes it to a fake database, which in this case is just printing it to the display. If it gets the SENTINEL value, it returns from the function, which will terminate the thread.\n",
    "\n",
    "Before you look at the really interesting part, the Pipeline, here’s the `__main__` section, which spawns these threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.DEBUG,\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "    #logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.submit(producer, pipeline)\n",
    "        executor.submit(consumer, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be worthwhile to walk through the DEBUG logging messages to see exactly where each thread acquires and releases the locks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s take a look at the Pipeline that passes messages from the producer to the consumer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Class to allow a single element pipeline between producer and consumer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.message = 0\n",
    "        self.producer_lock = threading.Lock()\n",
    "        self.consumer_lock = threading.Lock()\n",
    "        self.consumer_lock.acquire()\n",
    "\n",
    "    def get_message(self, name):\n",
    "        logging.debug(\"%s:about to acquire getlock\", name)\n",
    "        self.consumer_lock.acquire()\n",
    "        logging.debug(\"%s:have getlock\", name)\n",
    "        message = self.message\n",
    "        logging.debug(\"%s:about to release setlock\", name)\n",
    "        self.producer_lock.release()\n",
    "        logging.debug(\"%s:setlock released\", name)\n",
    "        return message\n",
    "\n",
    "    def set_message(self, message, name):\n",
    "        logging.debug(\"%s:about to acquire setlock\", name)\n",
    "        self.producer_lock.acquire()\n",
    "        logging.debug(\"%s:have setlock\", name)\n",
    "        self.message = message\n",
    "        logging.debug(\"%s:about to release getlock\", name)\n",
    "        self.consumer_lock.release()\n",
    "        logging.debug(\"%s:getlock released\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That’s a lot of code. A pretty high percentage of that is just logging statements to make it easier to see what’s happening when you run it. Here’s the same code with all of the logging statements removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Class to allow a single element pipeline between producer and consumer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.message = 0\n",
    "        self.producer_lock = threading.Lock()\n",
    "        self.consumer_lock = threading.Lock()\n",
    "        self.consumer_lock.acquire()\n",
    "\n",
    "    def get_message(self, name):\n",
    "        self.consumer_lock.acquire()\n",
    "        message = self.message\n",
    "        self.producer_lock.release()\n",
    "        return message\n",
    "\n",
    "    def set_message(self, message, name):\n",
    "        self.producer_lock.acquire()\n",
    "        self.message = message\n",
    "        self.consumer_lock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems a bit more manageable. The Pipeline in this version of your code has three members:\n",
    "1. .message stores the message to pass.\n",
    "2. .producer_lock is a threading.Lock object that restricts access to the message by the producer thread.\n",
    "3. .consumer_lock is also a threading.Lock that restricts access to the message by the consumer thread.\n",
    "\n",
    "`__init__()` initializes these three members and then calls .acquire() on the .consumer_lock. This is the state you want to start in. The producer is allowed to add a new message, but the consumer needs to wait until a message is present.\n",
    "\n",
    ".get_message() and .set_messages() are nearly opposites. .get_message() calls .acquire() on the consumer_lock. This is the call that will make the consumer wait until a message is ready.\n",
    "\n",
    "Once the consumer has acquired the .consumer_lock, it copies out the value in .message and then calls .release() on the .producer_lock. Releasing this lock is what allows the producer to insert the next message into the pipeline.\n",
    "\n",
    "Before you go on to .set_message(), there’s something subtle going on in .get_message() that’s pretty easy to miss. It might seem tempting to get rid of message and just have the function end with return self.message. See if you can figure out why you don’t want to do that before moving on.\n",
    "\n",
    "Here’s the answer. As soon as the consumer calls .producer_lock.release(), it can be swapped out, and the producer can start running. That could happen before .release() returns! This means that there is a slight possibility that when the function returns self.message, that could actually be the next message generated, so you would lose the first message. This is another example of a race condition.\n",
    "\n",
    "Moving on to .set_message(), you can see the opposite side of the transaction. The producer will call this with a message. It will acquire the .producer_lock, set the .message, and the call .release() on then consumer_lock, which will allow the consumer to read that value.\n",
    "\n",
    "Let’s run the code that has logging set to WARNING."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, you might find it odd that the producer gets two messages before the consumer even runs. If you look back at the producer and .set_message(), you will notice that the only place it will wait for a Lock is when it attempts to put the message into the pipeline. This is done after the producer gets the message and logs that it has it.\n",
    "\n",
    "When the producer attempts to send this second message, it will call .set_message() the second time and it will block.\n",
    "\n",
    "The operating system can swap threads at any time, but it generally lets each thread have a reasonable amount of time to run before swapping it out. That’s why the producer usually runs until it blocks in the second call to .set_message().\n",
    "\n",
    "Once a thread is blocked, however, the operating system will always swap it out and find a different thread to run. In this case, the only other thread with anything to do is the consumer.\n",
    "\n",
    "The consumer calls .get_message(), which reads the message and calls .release() on the .producer_lock, thus allowing the producer to run again the next time threads are swapped.\n",
    "\n",
    "Notice that the first message was 43, and that is exactly what the consumer read, even though the producer had already generated the 45 message.\n",
    "\n",
    "While it works for this limited test, it is not a great solution to the producer-consumer problem in general because it only allows a single value in the pipeline at a time. When the producer gets a burst of messages, it will have nowhere to put them.\n",
    "\n",
    "Let’s move on to a better way to solve this problem, using a Queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer-Consumer Using Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be able to handle more than one value in the pipeline at a time, you’ll need a data structure for the pipeline that allows the number to grow and shrink as data backs up from the producer.\n",
    "\n",
    "Python’s standard library has a queue module which, in turn, has a Queue class. Let’s change the Pipeline to use a Queue instead of just a variable protected by a Lock. You’ll also use a different way to stop the worker threads by using a different primitive from Python threading, an Event.\n",
    "\n",
    "Let’s start with the Event. The threading.Event object allows one thread to signal an event while many other threads can be waiting for that event to happen. The key usage in this code is that the threads that are waiting for the event do not necessarily need to stop what they are doing, they can just check the status of the Event every once in a while.\n",
    "\n",
    "The triggering of the event can be many things. In this example, the main thread will simply sleep for a while and then .set() it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_queue.py\n",
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO,\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "    # logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    event = threading.Event()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.submit(producer, pipeline, event)\n",
    "        executor.submit(consumer, pipeline, event)\n",
    "\n",
    "        time.sleep(0.1)\n",
    "        logging.info(\"Main: about to set event\")\n",
    "        event.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only changes here are the creation of the event object on line 6, passing the event as a parameter on lines 8 and 9, and the final section on lines 11 to 13, which sleep for a second, log a message, and then call .set() on the event.\n",
    "\n",
    "The producer also did not have to change too much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producer(pipeline, event):\n",
    "    \"\"\"Pretend we're getting a number from the network.\"\"\"\n",
    "    while not event.is_set():\n",
    "        message = random.randint(1, 101)\n",
    "        logging.info(\"Producer got message: %s\", message)\n",
    "        pipeline.set_message(message, \"Producer\")\n",
    "\n",
    "    logging.info(\"Producer received EXIT event. Exiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now will loop until it sees that the event was set on line 3. It also no longer puts the SENTINEL value into the pipeline.\n",
    "\n",
    "consumer had to change a little more:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(pipeline, event):\n",
    "    \"\"\"Pretend we're saving a number in the database.\"\"\"\n",
    "    while not event.is_set() or not pipeline.empty():\n",
    "        message = pipeline.get_message(\"Consumer\")\n",
    "        logging.info(\n",
    "            \"Consumer storing message: %s  (queue size=%s)\",\n",
    "            message,\n",
    "            pipeline.qsize(),\n",
    "        )\n",
    "\n",
    "    logging.info(\"Consumer received EXIT event. Exiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you got to take out the code related to the SENTINEL value, you did have to do a slightly more complicated while condition. Not only does it loop until the event is set, but it also needs to keep looping until the pipeline has been emptied.\n",
    "\n",
    "Making sure the queue is empty before the consumer finishes prevents another fun issue. If the consumer does exit while the pipeline has messages in it, there are two bad things that can happen. The first is that you lose those final messages, but the more serious one is that the producer can get caught attempting to add a message to a full queue and never return.\n",
    "\n",
    "This happens if the event gets triggered after the producer has checked the .is_set() condition but before it calls pipeline.set_message().\n",
    "\n",
    "If that happens, it’s possible for the producer to wake up and exit with the queue still completely full. The producer will then call .set_message() which will wait until there is space on the queue for the new message. The consumer has already exited, so this will not happen and the producer will not exit.\n",
    "\n",
    "The rest of the consumer should look familiar.\n",
    "\n",
    "The Pipeline has changed dramatically, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(queue.Queue):\n",
    "    def __init__(self):\n",
    "        super().__init__(maxsize=10)\n",
    "\n",
    "    def get_message(self, name):\n",
    "        logging.debug(\"%s:about to get from queue\", name)\n",
    "        value = self.get()\n",
    "        logging.debug(\"%s:got %d from queue\", name, value)\n",
    "        return value\n",
    "\n",
    "    def set_message(self, value, name):\n",
    "        logging.debug(\"%s:about to add %d to queue\", name, value)\n",
    "        self.put(value)\n",
    "        logging.debug(\"%s:added %d to queue\", name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Pipeline is a subclass of queue.Queue. Queue has an optional parameter when initializing to specify a maximum size of the queue.\n",
    "\n",
    "If you give a positive number for maxsize, it will limit the queue to that number of elements, causing .put() to block until there are fewer than maxsize elements. If you don’t specify maxsize, then the queue will grow to the limits of your computer’s memory.\n",
    "\n",
    ".get_message() and .set_message() got much smaller. They basically wrap .get() and .put() on the Queue. You might be wondering where all of the locking code that prevents the threads from causing race conditions went.\n",
    "\n",
    "The core devs who wrote the standard library knew that a Queue is frequently used in multi-threading environments and incorporated all of that locking code inside the Queue itself. Queue is thread-safe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Producer got message: 32\n",
    "    Producer got message: 51\n",
    "    Producer got message: 25\n",
    "    Producer got message: 94\n",
    "    Producer got message: 29\n",
    "    Consumer storing message: 32 (queue size=3)\n",
    "    Producer got message: 96\n",
    "    Consumer storing message: 51 (queue size=3)\n",
    "    Producer got message: 6\n",
    "    Consumer storing message: 25 (queue size=3)\n",
    "    Producer got message: 31\n",
    "\n",
    "    [many lines deleted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read through the output in my example, you can see some interesting things happening. Right at the top, you can see the producer got to create five messages and place four of them on the queue. It got swapped out by the operating system before it could place the fifth one.\n",
    "\n",
    "The consumer then ran and pulled off the first message. It printed out that message as well as how deep the queue was at that point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Consumer storing message: 32 (queue size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you know that the fifth message hasn’t made it into the pipeline yet. The queue is down to size three after a single message was removed. You also know that the queue can hold ten messages, so the producer thread didn’t get blocked by the queue. It was swapped out by the OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Your output will be different. Your output will change from run to run. That’s the fun part of working with threads!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the program starts to wrap up, can you see the main thread generating the event which causes the producer to exit immediately. The consumer still has a bunch of work do to, so it keeps running until it has cleaned out the pipeline.\n",
    "\n",
    "Try playing with different queue sizes and calls to time.sleep() in the producer or the consumer to simulate longer network or disk access times respectively. Even slight changes to these elements of the program will make large differences in your results.\n",
    "\n",
    "This is a much better solution to the producer-consumer problem, but you can simplify it even more. The Pipeline really isn’t needed for this problem. Once you take away the logging, it just becomes a queue.Queue.\n",
    "\n",
    "Here’s what the final code looks like using queue.Queue directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc_queue_direct.py\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import queue\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def producer(queue, event):\n",
    "    \"\"\"Pretend we're getting a number from the network.\"\"\"\n",
    "    while not event.is_set():\n",
    "        message = random.randint(1, 101)\n",
    "        logging.info(\"Producer got message: %s\", message)\n",
    "        queue.put(message)\n",
    "\n",
    "    logging.info(\"Producer received event. Exiting\")\n",
    "\n",
    "def consumer(queue, event):\n",
    "    \"\"\"Pretend we're saving a number in the database.\"\"\"\n",
    "    while not event.is_set() or not queue.empty():\n",
    "        message = queue.get()\n",
    "        logging.info(\n",
    "            \"Consumer storing message: %s (size=%d)\", message, queue.qsize()\n",
    "        )\n",
    "\n",
    "    logging.info(\"Consumer received event. Exiting\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    format = \"%(asctime)s: %(message)s\"\n",
    "    logging.basicConfig(format=format, level=logging.INFO,\n",
    "                        datefmt=\"%H:%M:%S\")\n",
    "\n",
    "    pipeline = queue.Queue(maxsize=10)\n",
    "    event = threading.Event()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        executor.submit(producer, pipeline, event)\n",
    "        executor.submit(consumer, pipeline, event)\n",
    "\n",
    "        time.sleep(0.1)\n",
    "        logging.info(\"Main: about to set event\")\n",
    "        event.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s easier to read and shows how using Python’s built-in primitives can simplify a complex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Careful Threading with Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the rules for threading are just for computing and programming. The physical world is full of concurrency as well. Many of these techniques has physical analogs that are useful for managing people and projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ALL shared resources SHALL be run in EXACTLY ONE thread. ALL communication with that thread SHALL be done using an atomic message queue: typically the Queue module, email, message queues like RabbitMQ or ZeroMQ, interesting you can communicate via a database as well.\n",
    "- Resources that need this technique: global variables, user input, output devices, files, sockets, etc.\n",
    "- Some resources that already have locks inside (thread-safe): logging module, decimal module (thread local variables), databases (reader locks and writer locks), email (this is an atomic message queue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One category of sequencing problems is to make sure that step A and step B happen sequentially. The solution is to put both in the same thread where all actions proceed sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can’t wait on daemon threads to complete (they are infinite loops). Instead, you join() on the queue itself. It waits until all the requested tasks are marked as being done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sometimes you need a global variable to communicate between functions. Global variables work great for this purpose in a single threaded program. In multi-threaded code, it mutable global state is a disaster. The better solution is to use a threading.local() that is global WITHIN a thread but not without."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Never try to kill a thread from something external to that thread. You never know if that thread is holding a lock. Python doesn’t provide a direct mechanism for kill threads externally; however, you can do it using ctypes, but that is a recipe for a deadlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue_threading.py\n",
    "import threading, queue\n",
    "\n",
    "counter = 0\n",
    "\n",
    "counter_queue = queue.Queue()\n",
    "\n",
    "def counter_manager():\n",
    "    'I have EXCLUSIVE rights to update the counter variable'\n",
    "    global counter\n",
    "\n",
    "    #counter smo izolirali v lastni thread\n",
    "    # imamo neskončno zanko -> \n",
    "    while True:\n",
    "        increment = counter_queue.get() # spi tukaj dokler ne dobi vrednosti v vrsto\n",
    "        counter += increment # povečamo counter \n",
    "        print_queue.put([f'The count is {counter}', '---------------']) # pošljemo sporočilo v print vrsto, \n",
    "        # ki postavi vse dogodke v vrsto\n",
    "        counter_queue.task_done() # ko konča pokliče task_done metodo\n",
    "\n",
    "t = threading.Thread(target=counter_manager)\n",
    "t.daemon = True # deamon thread, zato ne joinamo   \n",
    "t.start()\n",
    "del t\n",
    "\n",
    "###########################################################################################\n",
    "print_queue = queue.Queue()\n",
    "\n",
    "def print_manager(): #izoliramo print resource, ki ma eskluzivno pravico za uporabo print funkcije \n",
    "    'I have EXCLUSIVE rights to call the \"print\" keyword'\n",
    "    while True: \n",
    "        job = print_queue.get() # spi, dokler ne dobi elementa v vrsto\n",
    "        for line in job:\n",
    "            # torej če potrebujemo nekaj sequental, damo to v isti thread\n",
    "            print(line) # nevarnost race conditiona? ne, ker je edina funkcija ki to opravlja, tako da je edina ki zmaga race\n",
    "        print_queue.task_done()\n",
    "\n",
    "t = threading.Thread(target=print_manager)\n",
    "t.daemon = True # deamon thread\n",
    "t.start()\n",
    "del t\n",
    "\n",
    "###########################################################################################\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    counter_queue.put(1) # ne smemo konunicirati direktno, lahko samo preko vrste\n",
    "        \n",
    "print_queue.put(['Starting up'])\n",
    "worker_threads = []\n",
    "\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target=worker)\n",
    "    worker_threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for t in worker_threads:\n",
    "    t.join() # joinamo non-deamon threade -> garantiramo da so vsi workerji končani, s tem ne garantiramo da \n",
    "    # ni več podatkov v vrstah (torej da so že prešteti)\n",
    "    \n",
    "\n",
    "# Blocks until all items in the queue have been gotten and processed.\n",
    "counter_queue.join() # joinamo vrsto (čakamo na vrsto da je prazna) - torej ko bo za vsak dodan element poklican, task done\n",
    "\n",
    "print_queue.put(['Finishing up']) # pošljemo sporočilo za konec, lahko garantiramo da bo to zadnje sporočilo \n",
    "print_queue.join() # moremo počakati da printer konča, zato pokličemo join nad vrsto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Queue.task_done()` -> Indicate that a formerly enqueued task is complete. Used by queue consumer threads. For each get() used to fetch a task, a subsequent call to task_done() tells the queue that the processing on the task is complete. If a join() is currently blocking, it will resume when all items have been processed (meaning that a task_done() call was received for every item that had been put() into the queue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few more primitives offered by the Python threading module. While you didn’t need these for the examples above, they can come in handy in different use cases, so it’s good to be familiar with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semaphore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Python threading object to look at is threading.Semaphore. A Semaphore is a counter with a few special properties. The first one is that the counting is atomic. This means that there is a guarantee that the operating system will not swap out the thread in the middle of incrementing or decrementing the counter.\n",
    "\n",
    "The internal counter is incremented when you call .release() and decremented when you call .acquire().\n",
    "\n",
    "The next special property is that if a thread calls .acquire() when the counter is zero, that thread will block until a different thread calls .release() and increments the counter to one.\n",
    "\n",
    "Semaphores are frequently used to protect a resource that has a limited capacity. An example would be if you have a pool of connections and want to limit the size of that pool to a specific number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A threading.Timer is a way to schedule a function to be called after a certain amount of time has passed. You create a Timer by passing in a number of seconds to wait and a function to call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = threading.Timer(30.0, my_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You start the Timer by calling .start(). The function will be called on a new thread at some point after the specified time, but be aware that there is no promise that it will be called exactly at the time you want.\n",
    "\n",
    "If you want to stop a Timer that you’ve already started, you can cancel it by calling .cancel(). Calling .cancel() after the Timer has triggered does nothing and does not produce an exception.\n",
    "\n",
    "A Timer can be used to prompt a user for action after a specific amount of time. If the user does the action before the Timer expires, .cancel() can be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will look at using the ping command. In Linux, the ping command will run until you kill it. So the Timer class becomes especially handy in Linux-land. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from threading import Timer\n",
    "\n",
    "kill = lambda process: process.kill()\n",
    "cmd = ['ping', 'www.google.com']\n",
    "ping = subprocess.Popen(\n",
    "    cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "my_timer = Timer(5, kill, [ping])\n",
    "\n",
    "try:\n",
    "    my_timer.start()\n",
    "    stdout, stderr = ping.communicate()\n",
    "finally:\n",
    "    my_timer.cancel()\n",
    "\n",
    "print (str(stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just set up a lambda that we can use to kill the process. Then we start our ping job and create a Timer object. You will note that the first argument is the time in seconds to wait, then the function to call and the argument to pass to the function. In this case, our function is a lambda and we pass it a list of arguments where the list happens to only have one element. If you run this code, it should run for about 5 seconds and then print out the results of the ping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A threading.Barrier can be used to keep a fixed number of threads in sync. When creating a Barrier, the caller must specify how many threads will be synchronizing on it. Each thread calls .wait() on the Barrier. They all will remain blocked until the specified number of threads are waiting, and then the are all released at the same time.\n",
    "\n",
    "Remember that threads are scheduled by the operating system so, even though all of the threads are released simultaneously, they will be scheduled to run one at a time.\n",
    "\n",
    "One use for a Barrier is to allow a pool of threads to initialize themselves. Having the threads wait on a Barrier after they are initialized will ensure that none of the threads start running before all of the threads are finished with their initialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
