{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viri:\n",
    "- [multiprocessing — Manage Processes Like Threads](https://pymotw.com/3/multiprocessing/index.html)\n",
    "- [Multiprocessing in Python](https://www.educative.io/courses/python-201-interactively-learn-advanced-concepts-in-python-3/gx2qKN97D1j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiprocessing module was added to Python in version 2.6. It was originally defined in PEP 371 by Jesse Noller and Richard Oudkerk. The multiprocessing module allows you to spawn processes in much that same manner than you can spawn threads with the threading module. The idea here is that because you are now spawning processes, you can avoid the Global Interpreter Lock (GIL) and take full advantages of multiple processors on a machine.\n",
    "\n",
    "The multiprocessing package also includes some APIs that are not in the threading module at all. For example, there is a neat Pool class that you can use to parallelize executing a function across multiple inputs. We will be looking at Pool in a later section. We will start with the multiprocessing module’s Process class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started With Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to spawn a second process is to instantiate a Process object with a target function and call start() to let it begin working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing_simple.py\n",
    "import multiprocessing\n",
    "\n",
    "def worker():\n",
    "    \"\"\"worker function\"\"\"\n",
    "    print('Worker')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobs = []\n",
    "    for i in range(5):\n",
    "        p = multiprocessing.Process(target=worker)\n",
    "        jobs.append(p)\n",
    "        p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output includes the word “Worker” printed five times, although it may not come out entirely clean, depending on the order of execution, because each process is competing for access to the output stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It usually more useful to be able to spawn a process with arguments to tell it what work to do. Unlike with threading, in order to pass arguments to a multiprocessing Process the arguments must be able to be serialized using pickle. This example passes each worker a number to be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker: 0\n",
      "Worker: 2\n",
      "Worker: 1\n",
      "Worker: 4\n",
      "Worker: 3\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing_simpleargs.py\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def worker(num):\n",
    "    \"\"\"thread worker function\"\"\"\n",
    "    print('Worker:', num)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobs = []\n",
    "    for i in range(5):\n",
    "        p = multiprocessing.Process(target=worker, args=(i,))\n",
    "        jobs.append(p)\n",
    "        p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Process class is very similar to the threading module’s Thread class. Let’s try creating a series of processes that call the same function and see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 doubled to 20 by process id: 64\n",
      "5 doubled to 10 by process id: 63\n",
      "20 doubled to 40 by process id: 70\n",
      "15 doubled to 30 by process id: 67\n",
      "25 doubled to 50 by process id: 73\n"
     ]
    }
   ],
   "source": [
    "# example_get_pid.py\n",
    "import os\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "def doubler(number):\n",
    "    \"\"\"\n",
    "    A doubling function that can be used by a process\n",
    "    \"\"\"\n",
    "    result = number * 2\n",
    "    proc = os.getpid()\n",
    "    print(f'{number} doubled to {result} by process id: {proc}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    numbers = [5, 10, 15, 20, 25]\n",
    "    procs = []\n",
    "\n",
    "    for index, number in enumerate(numbers):\n",
    "        proc = Process(target=doubler, args=(number,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "\n",
    "    for proc in procs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One difference between the threading and multiprocessing examples is the extra protection for `__main__` used in the multiprocessing examples. Due to the way the new processes are started, **the child process needs to be able to import the script containing the target function**. Wrapping the main part of the application in a check for `__main__` **ensures that it is not run recursively in each child as the module is imported**. Another approach is to import the target function from a separate script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the platform, multiprocessing supports three ways to start a process. These start methods are:\n",
    "- `spawn`: The parent process starts a fresh python interpreter process. The child process will only inherit those resources necessary to run the process objects run() method. In particular, unnecessary file descriptors and handles from the parent process will not be inherited. Starting a process using this method is rather slow compared to using fork or forkserver. *Available on Unix and Windows. The default on Windows and macOS.*\n",
    "- `fork`: The parent process uses os.fork() to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic. *Available on Unix only. The default on Unix.*\n",
    "- `forkserver`:  When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited. *Available on Unix platforms which support passing file descriptors over Unix pipes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we import Process and create a doubler function. Inside the function, we double the number that was passed in. We also use Python’s os module to get the current process’s ID (or pid). This will tell us which process is calling the function. Then in the block of code at the bottom, we create a series of Processes and start them. The very last loop just calls the `join()` method on each process, which tells Python to wait for the process to terminate. If you need to stop a process, you can call its `terminate()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing arguments to identify or name the process is cumbersome, and unnecessary. Each Process instance has a name with a default value that can be changed as the process is created. Naming processes is useful for keeping track of them, especially in applications with multiple types of processes running simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it’s nicer to have a more human readable name for your process though. Fortunately, the Process class does allow you to access the name of your process. Let’s take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 doubled to 20 by: Process-15\n",
      "5 doubled to 10 by: Process-14\n",
      "15 doubled to 30 by: Process-16\n",
      "20 doubled to 40 by: Process-17\n",
      "25 doubled to 50 by: Process-18\n",
      "2 doubled to 4 by: Test\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing_names.py\n",
    "import os\n",
    "\n",
    "from multiprocessing import Process, current_process\n",
    "\n",
    "\n",
    "def doubler(number):\n",
    "    \"\"\"\n",
    "    A doubling function that can be used by a process\n",
    "    \"\"\"\n",
    "    result = number * 2\n",
    "    proc_name = current_process().name\n",
    "    print(f'{number} doubled to {result} by: {proc_name}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    numbers = [5, 10, 15, 20, 25]\n",
    "    procs = []\n",
    "    proc = Process(target=doubler, args=(5,))\n",
    "\n",
    "    for index, number in enumerate(numbers):\n",
    "        proc = Process(target=doubler, args=(number,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    "\n",
    "    proc = Process(target=doubler, name='Test', args=(2,))\n",
    "    proc.start()\n",
    "    procs.append(proc)\n",
    "\n",
    "    for proc in procs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, we import something extra: current_process. The current_process is basically the same thing as the threading module’s current_thread. We use it to grab the name of the thread that is calling our function. You will note that for the first five processes, we don’t set a name. Then for the sixth, we set the process name to \"Test\". Let’s see what we get for output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output demonstrates that the multiprocessing module assigns a number to each process as a part of its name by default. Of course, when we specify a name, a number isn’t going to get added to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daemon Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the main program will not exit until all of the children have exited. There are times when starting a background process that runs without blocking the main program from exiting is useful, such as in services where there may not be an easy way to interrupt the worker, or where letting it die in the middle of its work does not lose or corrupt data (for example, a task that generates “heart beats” for a service monitoring tool).\n",
    "\n",
    "To mark a process as a daemon, set its daemon attribute to True. The default is for processes to not be daemons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_daemon.py\n",
    "import multiprocessing\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "def daemon():\n",
    "    p = multiprocessing.current_process()\n",
    "    print('Starting:', p.name, p.pid)\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(2)\n",
    "    print('Exiting :', p.name, p.pid)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def non_daemon():\n",
    "    p = multiprocessing.current_process()\n",
    "    print('Starting:', p.name, p.pid)\n",
    "    sys.stdout.flush()\n",
    "    print('Exiting :', p.name, p.pid)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d = multiprocessing.Process(\n",
    "        name='daemon',\n",
    "        target=daemon,\n",
    "    )\n",
    "    d.daemon = True\n",
    "\n",
    "    n = multiprocessing.Process(\n",
    "        name='non-daemon',\n",
    "        target=non_daemon,\n",
    "    )\n",
    "    n.daemon = False\n",
    "\n",
    "    d.start()\n",
    "    time.sleep(1)\n",
    "    n.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output does not include the “Exiting” message from the daemon process, since all of the non-daemon processes (including the main program) exit before the daemon process wakes up from its two second sleep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daemon process is terminated automatically before the main program exits, which avoids leaving orphaned processes running. This can be verified by looking for the process id value printed when the program runs, and then checking for that process with a command like ps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting for Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wait until a process has completed its work and exited, use the join() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_daemon_join.py\n",
    "import multiprocessing\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "def daemon():\n",
    "    name = multiprocessing.current_process().name\n",
    "    print('Starting:', name)\n",
    "    time.sleep(2)\n",
    "    print('Exiting :', name)\n",
    "\n",
    "\n",
    "def non_daemon():\n",
    "    name = multiprocessing.current_process().name\n",
    "    print('Starting:', name)\n",
    "    print('Exiting :', name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d = multiprocessing.Process(\n",
    "        name='daemon',\n",
    "        target=daemon,\n",
    "    )\n",
    "    d.daemon = True\n",
    "\n",
    "    n = multiprocessing.Process(\n",
    "        name='non-daemon',\n",
    "        target=non_daemon,\n",
    "    )\n",
    "    n.daemon = False\n",
    "\n",
    "    d.start()\n",
    "    time.sleep(1)\n",
    "    n.start()\n",
    "\n",
    "    d.join()\n",
    "    n.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the main process waits for the daemon to exit using join(), the “Exiting” message is printed this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, join() blocks indefinitely. It is also possible to pass a timeout argument (a float representing the number of seconds to wait for the process to become inactive). If the process does not complete within the timeout period, join() returns anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_daemon_join_timeout.py\n",
    "import multiprocessing\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "def daemon():\n",
    "    name = multiprocessing.current_process().name\n",
    "    print('Starting:', name)\n",
    "    time.sleep(2)\n",
    "    print('Exiting :', name)\n",
    "\n",
    "\n",
    "def non_daemon():\n",
    "    name = multiprocessing.current_process().name\n",
    "    print('Starting:', name)\n",
    "    print('Exiting :', name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d = multiprocessing.Process(\n",
    "        name='daemon',\n",
    "        target=daemon,\n",
    "    )\n",
    "    d.daemon = True\n",
    "\n",
    "    n = multiprocessing.Process(\n",
    "        name='non-daemon',\n",
    "        target=non_daemon,\n",
    "    )\n",
    "    n.daemon = False\n",
    "\n",
    "    d.start()\n",
    "    n.start()\n",
    "\n",
    "    d.join(1)\n",
    "    print('d.is_alive()', d.is_alive())\n",
    "    n.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the timeout passed is less than the amount of time the daemon sleeps, the process is still “alive” after join() returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminating Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is better to use the poison pill method of signaling to a process that it should exit (see Passing Messages to Processes, later in this chapter), if a process appears hung or deadlocked it can be useful to be able to kill it forcibly. Calling terminate() on a process object kills the child process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_terminate.py\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "def slow_worker():\n",
    "    print('Starting worker')\n",
    "    time.sleep(0.1)\n",
    "    print('Finished worker')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = multiprocessing.Process(target=slow_worker)\n",
    "    print('BEFORE:', p, p.is_alive())\n",
    "\n",
    "    p.start()\n",
    "    print('DURING:', p, p.is_alive())\n",
    "\n",
    "    p.terminate()\n",
    "    print('TERMINATED:', p, p.is_alive())\n",
    "\n",
    "    p.join()\n",
    "    print('JOINED:', p, p.is_alive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to join() the process after terminating it in order to give the process management code time to update the status of the object to reflect the termination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Terminate the process. On Unix this is done using the SIGTERM signal; on Windows TerminateProcess() is used. Note that exit handlers and finally clauses, etc., will not be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Exit Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status code produced when the process exits can be accessed via the exitcode attribute. The ranges allowed are listed in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"docutils\" id=\"id10\">\n",
    "<caption><span class=\"caption-text\">Multiprocessing Exit Codes</span><a class=\"headerlink\" href=\"#id10\" title=\"Permalink to this table\"></a></caption>\n",
    "<colgroup>\n",
    "<col width=\"14%\">\n",
    "<col width=\"86%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Exit Code</th>\n",
    "<th class=\"head\">Meaning</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td><code class=\"docutils literal notranslate\"><span class=\"pre\">==</span> <span class=\"pre\">0</span></code></td>\n",
    "<td>no error was produced</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><code class=\"docutils literal notranslate\"><span class=\"pre\">&gt;</span> <span class=\"pre\">0</span></code></td>\n",
    "<td>the process had an error, and exited with that code</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;</span> <span class=\"pre\">0</span></code></td>\n",
    "<td>the process was killed with a signal of <code class=\"docutils literal notranslate\"><span class=\"pre\">-1</span> <span class=\"pre\">*</span> <span class=\"pre\">exitcode</span></code></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process for exit_error\n",
      "Starting process for exit_ok\n",
      "Starting process for return_value\n",
      "Starting process for raises\n",
      "Starting process for terminated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process raises:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-f36c71fbced5>\", line 20, in raises\n",
      "    raise RuntimeError('There was an error!')\n",
      "RuntimeError: There was an error!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit_error.exitcode = 1\n",
      "exit_ok.exitcode = 0\n",
      "return_value.exitcode = 0\n",
      "raises.exitcode = 1\n",
      "terminated.exitcode = -15\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing_exitcode.py\n",
    "import multiprocessing\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def exit_error():\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "def exit_ok():\n",
    "    return\n",
    "\n",
    "\n",
    "def return_value():\n",
    "    return 1\n",
    "\n",
    "\n",
    "def raises():\n",
    "    raise RuntimeError('There was an error!')\n",
    "\n",
    "\n",
    "def terminated():\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobs = []\n",
    "    funcs = [\n",
    "        exit_error,\n",
    "        exit_ok,\n",
    "        return_value,\n",
    "        raises,\n",
    "        terminated,\n",
    "    ]\n",
    "    for f in funcs:\n",
    "        print('Starting process for', f.__name__)\n",
    "        j = multiprocessing.Process(target=f, name=f.__name__)\n",
    "        jobs.append(j)\n",
    "        j.start()\n",
    "\n",
    "    jobs[-1].terminate()\n",
    "\n",
    "    for j in jobs:\n",
    "        j.join()\n",
    "        print(f'{j.name}.exitcode = {j.exitcode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes that raise an exception automatically get an exitcode of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging concurrency issues, it can be useful to have access to the internals of the objects provided by multiprocessing. There is a convenient module-level function to enable logging called `log_to_stderr()`. It sets up a logger object using logging and adds a handler so that log messages are sent to the standard error channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_log_to_stderr.py\n",
    "import multiprocessing\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "def worker():\n",
    "    print('Doing some work')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.log_to_stderr(logging.DEBUG)\n",
    "    p = multiprocessing.Process(target=worker)\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the logging level is set to NOTSET so no messages are produced. Pass a different level to initialize the logger to the level of detail desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manipulate the logger directly (change its level setting or add handlers), use `get_logger()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging processes is a little different than logging threads. The reason for this is that Python’s logging packages doesn’t use process shared locks, so it’s possible for you to end up with messages from different processes getting mixed up. Let’s try adding basic logging to the previous example. Here’s the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-22] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing some work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-22] process shutting down\n",
      "[INFO/Process-22] process exiting with exitcode 0\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing_get_logger.py\n",
    "import multiprocessing\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "def worker():\n",
    "    print('Doing some work')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.log_to_stderr()\n",
    "    logger = multiprocessing.get_logger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    p = multiprocessing.Process(target=worker)\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logger can also be configured through the logging configuration file API, using the name “multiprocessing”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to communicating between processes, the multiprocessing modules has two primary methods: Queues and Pipes. The Queue implementation is actually both thread and process safe. Let’s take a look at a fairly simple example that’s based on the Queue code from the previous chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data and putting it on the queue\n",
      "data found to be processed: 5\n",
      "10\n",
      "data found to be processed: 10\n",
      "20\n",
      "data found to be processed: 13\n",
      "26\n",
      "data found to be processed: -1\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing_process_communication.py\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "sentinel = -1\n",
    "\n",
    "def creator(data, q):\n",
    "    \"\"\"\n",
    "    Creates data to be consumed and waits for the consumer\n",
    "    to finish processing\n",
    "    \"\"\"\n",
    "    print('Creating data and putting it on the queue')\n",
    "    for item in data:\n",
    "        q.put(item)\n",
    "\n",
    "def my_consumer(q):\n",
    "    \"\"\"\n",
    "    Consumes some data and works on it\n",
    "\n",
    "    In this case, all it does is double the input\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        data = q.get()\n",
    "        print('data found to be processed: {}'.format(data))\n",
    "        processed = data * 2\n",
    "        print(processed)\n",
    "\n",
    "        if data is sentinel:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = Queue()\n",
    "    data = [5, 10, 13, -1]\n",
    "    process_one = Process(target=creator, args=(data, q))\n",
    "    process_two = Process(target=my_consumer, args=(q,))\n",
    "    process_one.start()\n",
    "    process_two.start()\n",
    "\n",
    "    q.close()\n",
    "    q.join_thread()\n",
    "\n",
    "    process_one.join()\n",
    "    process_two.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just need to import Queue and Process. Then we two functions, one to create data and add it to the queue and the second to consume the data and process it. Adding data to the Queue is done by using the Queue’s put() method whereas getting data from the Queue is done via the get method. The last chunk of code just creates the Queue object and a couple of Processes and then runs them. You will note that we call join() on our process objects rather than the Queue itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Messages to Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with threads, a common use pattern for multiple processes is to divide a job up among several workers to run in parallel. Effective use of multiple processes usually requires some communication between them, so that work can be divided and results can be aggregated. A simple way to communicate between processes with multiprocessing is to use a Queue to pass messages back and forth. Any object that can be serialized with pickle can pass through a Queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_queue.py\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class MyFancyClass:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def do_something(self):\n",
    "        proc_name = multiprocessing.current_process().name\n",
    "        print(f'Doing something fancy in {proc_name} for {self.name}!')\n",
    "\n",
    "def worker(q):\n",
    "    obj = q.get()\n",
    "    obj.do_something()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    queue = multiprocessing.Queue()\n",
    "\n",
    "    p = multiprocessing.Process(target=worker, args=(queue,))\n",
    "    p.start()\n",
    "\n",
    "    queue.put(MyFancyClass('Fancy Dan'))\n",
    "\n",
    "    # Wait for the worker to finish\n",
    "    queue.close()\n",
    "    queue.join_thread()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short example only passes a single message to a single worker, then the main process waits for the worker to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex example shows how to manage several workers consuming data from a [JoinableQueue](https://docs.python.org/3.8/library/multiprocessing.html#multiprocessing.JoinableQueue) and passing results back to the parent process. The poison pill technique is used to stop the workers. After setting up the real tasks, the main program adds one “stop” value per worker to the job queue. When a worker encounters the special value, it breaks out of its processing loop. The main process uses the task queue’s join() method to wait for all of the tasks to finish before processing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_producer_consumer.py\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "class Consumer(multiprocessing.Process):\n",
    "    def __init__(self, task_queue, result_queue):\n",
    "        multiprocessing.Process.__init__(self)\n",
    "        self.task_queue = task_queue\n",
    "        self.result_queue = result_queue\n",
    "\n",
    "    def run(self):\n",
    "        proc_name = self.name\n",
    "        while True:\n",
    "            next_task = self.task_queue.get()\n",
    "            if next_task is None:\n",
    "                # Poison pill means shutdown\n",
    "                print(f'{proc_name}: Exiting')\n",
    "                self.task_queue.task_done()\n",
    "                break\n",
    "            print(f'{proc_name}: {next_task}')\n",
    "            answer = next_task()\n",
    "            self.task_queue.task_done()\n",
    "            self.result_queue.put(answer)\n",
    "\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self):\n",
    "        time.sleep(0.1)  # pretend to take time to do the work\n",
    "        return '{self.a} * {self.b} = {product}'.format(\n",
    "            self=self, product=self.a * self.b)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '{self.a} * {self.b}'.format(self=self)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Establish communication queues\n",
    "    tasks = multiprocessing.JoinableQueue()\n",
    "    results = multiprocessing.Queue()\n",
    "\n",
    "    # Start consumers\n",
    "    num_consumers = multiprocessing.cpu_count() * 2\n",
    "    print(f'Creating {num_consumers} consumers')\n",
    "    consumers = [Consumer(tasks, results) for i in range(num_consumers)]\n",
    "    for w in consumers:\n",
    "        w.start()\n",
    "\n",
    "    # Enqueue jobs\n",
    "    num_jobs = 10\n",
    "    for i in range(num_jobs):\n",
    "        tasks.put(Task(i, i))\n",
    "\n",
    "    # Add a poison pill for each consumer\n",
    "    for i in range(num_consumers):\n",
    "        tasks.put(None)\n",
    "\n",
    "    # Wait for all of the tasks to finish\n",
    "    tasks.join()\n",
    "\n",
    "    # Start printing results\n",
    "    while num_jobs:\n",
    "        result = results.get()\n",
    "        print('Result:', result)\n",
    "        num_jobs -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the jobs enter the queue in order, their execution is parallelized so there is no guarantee about the order they will be completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locks - Controlling Access to Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations when a single resource needs to be shared between multiple processes, a Lock can be used to avoid conflicting accesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_nolock.py\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "def worker_with(stream):\n",
    "    stream.write('Lock acquired via with\\n')\n",
    "\n",
    "\n",
    "def worker_no_with(stream):\n",
    "    stream.write('Lock acquired directly\\n')\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    w = multiprocessing.Process(\n",
    "        target=worker_with,\n",
    "        args=(sys.stdout,),\n",
    "    )\n",
    "    nw = multiprocessing.Process(\n",
    "        target=worker_no_with,\n",
    "        args=(sys.stdout,),\n",
    "    )\n",
    "\n",
    "    w.start()\n",
    "    nw.start()\n",
    "\n",
    "    w.join()\n",
    "    nw.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_lock.py\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "def worker_with(lock, stream):\n",
    "    with lock:\n",
    "        stream.write('Lock acquired via with\\n')\n",
    "\n",
    "\n",
    "def worker_no_with(lock, stream):\n",
    "    lock.acquire()\n",
    "    try:\n",
    "        stream.write('Lock acquired directly\\n')\n",
    "    finally:\n",
    "        lock.release()\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    lock = multiprocessing.Lock()\n",
    "    w = multiprocessing.Process(\n",
    "        target=worker_with,\n",
    "        args=(lock, sys.stdout),\n",
    "    )\n",
    "    nw = multiprocessing.Process(\n",
    "        target=worker_no_with,\n",
    "        args=(lock, sys.stdout),\n",
    "    )\n",
    "\n",
    "    w.start()\n",
    "    nw.start()\n",
    "\n",
    "    w.join()\n",
    "    nw.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the messages printed to the console may be jumbled together if the two processes do not synchronize their access of the output stream with the lock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Pools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3.8/library/multiprocessing.html#module-multiprocessing.pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pool class can be used to manage a fixed number of workers for simple cases where the work to be done can be broken up and distributed between workers independently. The return values from the jobs are collected and returned as a list. The pool arguments include the number of processes and a function to run when starting the task process (invoked once per child)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing_pool.py\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def do_calculation(data):\n",
    "    return data * 2\n",
    "\n",
    "\n",
    "def start_process():\n",
    "    print('Starting', multiprocessing.current_process().name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inputs = list(range(10))\n",
    "    print('Input   :', inputs)\n",
    "\n",
    "    builtin_outputs = map(do_calculation, inputs)\n",
    "    print('Built-in:', builtin_outputs)\n",
    "\n",
    "    pool_size = multiprocessing.cpu_count()\n",
    "    \n",
    "    with multiprocessing.Pool(processes=pool_size, initializer=start_process,) as pool:\n",
    "        pool_outputs = pool.map(do_calculation, inputs)\n",
    "\n",
    "    print('Pool    :', pool_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the map() method is functionally equivalent to the built-in map(), except that individual tasks run in parallel. Since the pool is processing its inputs in parallel, close() and join() can be used to synchronize the main process with the task processes to ensure proper cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also **get the result of your process** in a pool by using the apply_async method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/ForkPoolWorker-23] child process calling self.run()\n",
      "[INFO/ForkPoolWorker-24] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing_pool_get_result.py\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def doubler(number):\n",
    "    return number * 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=2) as pool:\n",
    "        result = pool.apply_async(doubler, (25,))\n",
    "        print(result.get(timeout=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this allows us to do is actually ask for the result of the process. That is what the get function is all about. It tries to get our result. You will note that we also have a timeout set just in case something happened to the function we were calling. We don’t want it to block indefinitely after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concurrent.futures - ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ProcessPoolExecutor class is an Executor subclass that uses a pool of processes to execute calls asynchronously. ProcessPoolExecutor uses the multiprocessing module, which allows it to side-step the Global Interpreter Lock but also means that only picklable objects can be executed and returned.\n",
    "\n",
    "The `__main__` module must be importable by worker subprocesses. This means that ProcessPoolExecutor will not work in the interactive interpreter.\n",
    "\n",
    "Calling Executor or Future methods from a callable submitted to a ProcessPoolExecutor will result in deadlock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class concurrent.futures.ProcessPoolExecutor(max_workers=None, mp_context=None, initializer=None, initargs=())`\n",
    "\n",
    "An Executor subclass that executes calls asynchronously using a pool of at most max_workers processes. If max_workers is None or not given, it will default to the number of processors on the machine. If max_workers is lower or equal to 0, then a ValueError will be raised. On Windows, max_workers must be equal or lower than 61. If it is not then ValueError will be raised. If max_workers is None, then the default chosen will be at most 61, even if more processors are available. mp_context can be a multiprocessing context or None. It will be used to launch the workers. If mp_context is None or not given, the default multiprocessing context is used.\n",
    "\n",
    "initializer is an optional callable that is called at the start of each worker process; initargs is a tuple of arguments passed to the initializer. Should initializer raise an exception, all currently pending jobs will raise a BrokenProcessPool, as well any attempt to submit more jobs to the pool.\n",
    "\n",
    "Changed in version 3.3: When one of the worker processes terminates abruptly, a BrokenProcessPool error is now raised. Previously, behaviour was undefined but operations on the executor or its futures would often freeze or deadlock.\n",
    "\n",
    "Changed in version 3.7: The mp_context argument was added to allow users to control the start_method for worker processes created by the pool.\n",
    "\n",
    "Added the initializer and initargs arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, say you want to do something computationally intensive with Python and\n",
    "utilize multiple CPU cores. I’ll use an implementation of finding the greatest common\n",
    "divisor of two numbers as a proxy for a more computationally intense algorithm, like\n",
    "simulating fluid dynamics with the Navier-Stokes equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcd(pair):\n",
    "    a, b = pair\n",
    "    low = min(a, b)\n",
    "    for i in range(low, 0, -1):\n",
    "        if a % i == 0 and b % i == 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this function in serial takes a linearly increasing amount of time because there is\n",
    "no parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.510 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "numbers = [(1963309, 2265973), (2030677, 3814172), (1551645, 2229620), (2039045, 2020802)]\n",
    "\n",
    "start = time()\n",
    "results = list(map(gcd, numbers))\n",
    "end = time()\n",
    "\n",
    "print(f'Took {(end - start):.3f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code on multiple Python threads will yield no speed improvement because\n",
    "the GIL prevents Python from using multiple CPU cores in parallel. Here, I do the same\n",
    "computation as above using the concurrent.futures module with its\n",
    "ThreadPoolExecutor class and two worker threads (to match the number of CPU\n",
    "cores on my computer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.525 seconds.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "start = time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as pool:\n",
    "    results = list(pool.map(gcd, numbers))\n",
    "\n",
    "end = time()\n",
    "print(f'Took {(end - start):.3f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s even slower this time because of the overhead of starting and communicating with the\n",
    "pool of threads.\n",
    "\n",
    "Now for the surprising part: By changing a single line of code, something magical\n",
    "happens. If I replace the ThreadPoolExecutor with the ProcessPoolExecutor\n",
    "from the concurrent.futures module, everything speeds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.978 seconds.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "start = time()\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=2) as pool:\n",
    "    results = list(pool.map(gcd, numbers))\n",
    "\n",
    "end = time()\n",
    "print(f'Took {(end - start):.3f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on my dual-core machine, it’s significantly faster! How is this possible? Here’s\n",
    "what the ProcessPoolExecutor class actually does (via the low-level constructs\n",
    "provided by the multiprocessing module):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It takes each item from the numbers input data to map.\n",
    "2. It serializes it into binary data using the pickle module (see Item 44: “Make\n",
    "pickle Reliable with copyreg”).\n",
    "3. It copies the serialized data from the main interpreter process to a child interpreter\n",
    "process over a local socket.\n",
    "4. Next, it deserializes the data back into Python objects using pickle in the child\n",
    "process.\n",
    "5. It then imports the Python module containing the gcd function.\n",
    "6. It runs the function on the input data in parallel with other child processes.\n",
    "7. It serializes the result back into bytes.\n",
    "8. It copies those bytes back through the socket.\n",
    "9. It deserializes the bytes back into Python objects in the parent process.\n",
    "10. Finally, it merges the results from multiple children into a single list to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it looks simple to the programmer, the multiprocessing module and\n",
    "ProcessPoolExecutor class do a huge amount of work to make parallelism possible.\n",
    "In most other languages, the only touch point you need to coordinate two threads is a\n",
    "single lock or atomic operation. The overhead of using multiprocessing is high\n",
    "because of all of the serialization and deserialization that must happen between the parent\n",
    "and child processes.\n",
    "\n",
    "This scheme is well suited to certain types of isolated, high-leverage tasks. By isolated, I\n",
    "mean functions that don’t need to share state with other parts of the program. By highleverage,\n",
    "I mean **situations in which only a small amount of data must be transferred\n",
    "between the parent and child processes to enable a large amount of computation**. The\n",
    "greatest common denominator algorithm is one example of this, but many other\n",
    "mathematical algorithms work similarly.\n",
    "\n",
    "If your computation doesn’t have these characteristics, then the overhead of\n",
    "multiprocessing may prevent it from speeding up your program through\n",
    "parallelization. When that happens, multiprocessing provides more advanced\n",
    "facilities for shared memory, cross-process locks, queues, and proxies. But all of these\n",
    "features are very complex. It’s hard enough to reason about such tools in the memory\n",
    "space of a single process shared between Python threads. Extending that complexity to\n",
    "other processes and involving sockets makes this much more difficult to understand.\n",
    "\n",
    "I suggest avoiding all parts of multiprocessing and using these features via the\n",
    "simpler concurrent.futures module. You can start by using the ThreadPoolExecutor class to run isolated, high-leverage functions in threads. Later, you can move to the ProcessPoolExecutor to get a speedup. Finally, once you’ve\n",
    "completely exhausted the other options, you can consider using the multiprocessing\n",
    "module directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProcessPoolExecutor Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/ForkProcess-25] child process calling self.run()\n",
      "[INFO/ForkProcess-26] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112272535095293 is prime: True\n",
      "112582705942171 is prime: True\n",
      "112272535095293 is prime: True\n",
      "115280095190773 is prime: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/ForkProcess-25] process shutting down\n",
      "[INFO/ForkProcess-26] process shutting down\n",
      "[INFO/ForkProcess-25] process exiting with exitcode 0\n",
      "[INFO/ForkProcess-26] process exiting with exitcode 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115797848077099 is prime: True\n",
      "1099726899285419 is prime: False\n"
     ]
    }
   ],
   "source": [
    "# process_pool_executor_ex.py\n",
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "PRIMES = [\n",
    "    112272535095293,\n",
    "    112582705942171,\n",
    "    112272535095293,\n",
    "    115280095190773,\n",
    "    115797848077099,\n",
    "    1099726899285419]\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "\n",
    "    sqrt_n = int(math.floor(math.sqrt(n)))\n",
    "    for i in range(3, sqrt_n + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):\n",
    "            print(f'{number} is prime: {prime}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
