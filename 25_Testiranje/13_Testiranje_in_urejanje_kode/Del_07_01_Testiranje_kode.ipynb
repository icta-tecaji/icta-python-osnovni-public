{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testiranje kode\n",
    "\n",
    "## Getting Started With Testing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated vs. Manual Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is, you’ve probably already created a test without realizing it. Remember when you ran your application and used it for the first time? Did you check the features and experiment using them? That’s known as exploratory testing and is a form of manual testing.\n",
    "\n",
    "Exploratory testing is a form of testing that is done without a plan. In an exploratory test, you’re just exploring the application.\n",
    "\n",
    "To have a complete set of manual tests, all you need to do is make a list of all the features your application has, the different types of input it can accept, and the expected results. Now, every time you make a change to your code, you need to go through every single item on that list and check it.\n",
    "\n",
    "That doesn’t sound like much fun, does it?\n",
    "\n",
    "This is where automated testing comes in. Automated testing is the execution of your test plan (the parts of your application you want to test, the order in which you want to test them, and the expected responses) by a script instead of a human. Python already comes with a set of tools and libraries to help you create automated tests for your application. We’ll explore those tools and libraries in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Tests vs. Integration Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world of testing has no shortage of terminology, and now that you know the difference between automated and manual testing, it’s time to go a level deeper.\n",
    "\n",
    "Think of how you might test the lights on a car. You would turn on the lights (known as the **test step**) and go outside the car or ask a friend to check that the lights are on (known as the **test assertion**). Testing multiple components is known as **integration testing**.\n",
    "\n",
    "Think of all the things that need to work correctly in order for a simple task to give the right result. These components are like the parts to your application, all of those classes, functions, and modules you’ve written.\n",
    "\n",
    "A major challenge with integration testing is when an integration test doesn’t give the right result. It’s very hard to diagnose the issue without being able to isolate which part of the system is failing. If the lights didn’t turn on, then maybe the bulbs are broken. Is the battery dead? What about the alternator? Is the car’s computer failing?\n",
    "\n",
    "If you have a fancy modern car, it will tell you when your light bulbs have gone. It does this using a form of **unit test**.\n",
    "\n",
    "A unit test is a smaller test, one that checks that a single component operates in the right way. A unit test helps you to isolate what is broken in your application and fix it faster.\n",
    "\n",
    "You have just seen two types of tests:\n",
    "- An integration test checks that components in your application operate with each other.\n",
    "- A unit test checks a small component in your application.\n",
    "\n",
    "You can write both integration tests and unit tests in Python. To write a unit test for the built-in function sum(), you would check the output of sum() against a known output.\n",
    "\n",
    "For example, here’s how you check that the sum() of the numbers (1, 2, 3) equals 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([1, 2, 3]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will not output anything on the REPL because the values are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If the result from <code>sum()</code> is incorrect, this will fail with an <code>AssertionError</code> and the message <code>\"Should be 6\"</code>. Try an assertion statement again with the wrong values to see an <code>AssertionError</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa10351dd942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should be 6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "assert sum([1, 1, 1]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the REPL, you are seeing the raised AssertionError because the result of sum() does not match 6.\n",
    "\n",
    "Instead of testing on the REPL, you’ll want to put this into a new Python file called `01_test_sum.py` and execute it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything passed\n"
     ]
    }
   ],
   "source": [
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum()\n",
    "    print(\"Everything passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have written a test case, an assertion, and an entry point (the command line). You can now execute this at the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python test_sum.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the successful result, Everything passed.\n",
    "\n",
    "In Python, sum() accepts any iterable as its first argument. You tested with a list. Now test with a tuple as well. Create a new file called test_sum_2.py with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "def test_sum_tuple():\n",
    "    assert sum((1, 2, 2)) == 6, \"Should be 6\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sum()\n",
    "    test_sum_tuple()\n",
    "    print(\"Everything passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute test_sum_2.py, the script will give an error because the sum() of (1, 2, 2) is 5, not 6. The result of the script gives you the error message, the line of code, and the traceback:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see how a mistake in your code gives an error on the console with some information on where the error was and what the expected result was.\n",
    "\n",
    "Writing tests in this way is okay for a simple check, but what if more than one fails? This is where **test runners come** in. The **test runner is a special application designed for running tests, checking the output, and giving you tools for debugging and diagnosing tests and applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Test Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many test runners available for Python. The one built into the Python standard library is called unittest. The principles of unittest are easily portable to other frameworks. The three most popular test runners are:\n",
    "\n",
    "- unittest\n",
    "- nose or nose2\n",
    "- pytest\n",
    "\n",
    "Choosing the best test runner for your requirements and level of experience is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unittest has been built into the Python standard library since version 2.1. You’ll probably see it in commercial Python applications and open-source projects.\n",
    "\n",
    "unittest contains both a testing framework and a test runner. unittest has some important requirements for writing and executing tests.\n",
    "\n",
    "unittest requires that:\n",
    "- You put your tests into classes as methods\n",
    "- You use a series of special assertion methods in the unittest.TestCase class instead of the built-in assert statement\n",
    "\n",
    "To convert the earlier example to a unittest test case, you would have to:\n",
    "1. Import unittest from the standard library\n",
    "2. Create a class called TestSum that inherits from the TestCase class\n",
    "3. Convert the test functions into methods by adding self as the first argument\n",
    "4. Change the assertions to use the self.assertEqual() method on the TestCase class\n",
    "5. Change the command-line entry point to call unittest.main()\n",
    "\n",
    "Follow those steps by creating a new file `test_sum_unittest.py` with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestSum(unittest.TestCase):\n",
    "\n",
    "    def test_sum(self):\n",
    "        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n",
    "\n",
    "    def test_sum_tuple(self):\n",
    "        self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you execute this at the command line, you’ll see one success (indicated with .) and one failure (indicated with F):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python test_sum_unittest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest supports execution of unittest test cases. The real advantage of pytest comes by writing pytest test cases. pytest test cases are a series of functions in a Python file starting with the name test_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Python standard library comes with a unit testing framework called ùnittest, pytest is the go-to testing framework for testing Python code.\n",
    "\n",
    "pytest makes it easy (and fun!) to write, organize, and run tests. When compared to unittest, from the Python standard library, pytest:\n",
    "\n",
    "1. Requires less boilerplate code so your test suites will be more readable.\n",
    "2. Supports the plain assert statement, which is far more readable and easier to remember compared to the assertSomething methods -- like assertEquals, assertTrue, and assertContains -- in unittest.\n",
    "3. Is updated more frequently since it's not part of the Python standard library.\n",
    "4. Simplifies setting up and tearing down test state with its fixture system.\n",
    "5. Uses a functional approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus, with pytest, you can have a consistent style across all of your Python projects. Say, you have two web applications in your stack -- one built with Django and the other built with Flask. Without pytest, you'd most likely leverage the Django test framework along with a Flask extension like Flask-Testing. So, your test suites would have different styles. With pytest, on the other hand, both test suites would have a consistent style, making it easier to jump from one to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest has some other great features:\n",
    "\n",
    "- Support for the built-in assert statement instead of using special self.assert*() methods\n",
    "- Support for filtering for test cases\n",
    "- Ability to rerun from the last failing test\n",
    "- An ecosystem of hundreds of plugins to extend the functionality\n",
    "\n",
    "Writing the TestSum test case example for pytest would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"\n",
    "\n",
    "def test_sum_tuple():\n",
    "    assert sum((1, 2, 2)) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have dropped the TestCase, any use of classes, and the command-line entry point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Your First Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s bring together what you’ve learned so far and, instead of testing the built-in sum() function, test a simple implementation of the same requirement.\n",
    "\n",
    "Create a new project folder and, inside that, create a new folder called my_sum. Inside my_sum, create an empty file called `__init__.py`. Creating the `__init__.py` file means that the my_sum folder can be imported as a module from the parent directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    project/\n",
    "    │\n",
    "    └── my_sum/\n",
    "        └── __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up `my_sum/__init__.py` and create a new function called sum(), which takes an iterable (a list, tuple, or set) and adds the values together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(arg):\n",
    "    total = 0\n",
    "    for val in arg:\n",
    "        total += val\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example creates a variable called total, iterates over all the values in arg, and adds them to total. It then returns the result once the iterable has been exhausted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to Write the Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started writing tests, you can simply create a file called test.py, which will contain your first test case. Because the file will need to be able to import your application to be able to test it, you want to place test.py above the package folder, so your directory tree will look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll find that, as you add more and more tests, your single file will become cluttered and hard to maintain, so **you can create a folder called `tests/` and split the tests into multiple files**. It is convention to **ensure each file starts with `test_` so all test runners will assume that Python file contains tests** to be executed. Some very large projects split tests into more subdirectories based on their purpose or usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Structure a Simple Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you dive into writing tests, you’ll want to first make a couple of decisions:\n",
    "1. What do you want to test?\n",
    "2. Are you writing a unit test or an integration test?\n",
    "\n",
    "Then the structure of a test should loosely follow this workflow:\n",
    "1. Create your inputs\n",
    "2. Execute the code being tested, capturing the output\n",
    "3. Compare the output with an expected result\n",
    "\n",
    "For this application, you’re testing sum(). There are many behaviors in sum() you could check, such as:\n",
    "1. Can it sum a list of whole numbers (integers)?\n",
    "2. Can it sum a tuple or set?\n",
    "3. Can it sum a list of floats?\n",
    "4. What happens when you provide it with a bad value, such as a single integer or a string?\n",
    "5. What happens when one of the values is negative?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Python standard library comes with a unit testing framework called unittest, pytest is the go-to testing framework for testing Python code.\n",
    "\n",
    "pytest makes it easy (and fun!) to write, organize, and run tests. When compared to unittest, from the Python standard library, pytest:\n",
    "1. Requires less boilerplate code so your test suites will be more readable.\n",
    "2. Supports the plain assert statement, which is far more readable and easier to remember compared to the assertSomething methods - like `assertEquals`, `assertTrue`, and `assertContains` -- in `unittest`. \n",
    "3. Is updated more frequently since it's not part of the Python standard library.\n",
    "4. Simplifies setting up and tearing down test state with its fixture system. \n",
    "5. Uses a functional approach.\n",
    "\n",
    "\n",
    "Plus, with pytest, you can have a consistent style across all of your Python projects. Say, you have two web applications in your stack -- one built with Django and the other built with Flask. Without pytest, you'd most likely leverage the Django test framework along with a Flask extension like Flask-Testing. So, your test suites would have different styles. With pytest, on the other hand, both test suites would have a consistent style, making it easier to jump from one to the other.\n",
    "\n",
    "pytest also has a large, community-maintained plugin ecosystem.\n",
    "\n",
    "Some examples:\n",
    "- pytest-django - provides a set of tools made specifically for testing Django applications\n",
    "- pytest-xdist - is used to run tests in parallel\n",
    "- pytest-cov - adds code coverage support\n",
    "- pytest-instafail - shows failures and errors immediately instead of waiting until the end of a run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of plugins, check out Plugin List from the docs.: https://docs.pytest.org/en/latest/reference/plugin_list.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along with some of the examples in this tutorial, you’ll need to install pytest. As with most Python packages, you can install pytest in a virtual environment from PyPI using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python -m pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytest command will now be available in your installation environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the installation is complete you can confirm it with by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display the help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Pytest Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’ve written unit tests for your Python code before, then you may have used Python’s built-in unittest module. unittest provides a solid base on which to build your test suite, but it has a few shortcomings.\n",
    "\n",
    "A number of third-party testing frameworks attempt to address some of the issues with unittest, and **pytest has proven to be one of the most popular**. pytest is a **feature-rich**, **plugin-based ecosystem for testing your Python code**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most functional tests follow the Arrange-Act-Assert model:\n",
    "- **Arrange**, or set up, the conditions for the test\n",
    "- **Act** by calling some function or method\n",
    "- **Assert** that some end condition is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nardimo novo mapo `project2` -> V njej mapo `helpers` in `tests`. (V obe mape dodamo `__init__.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a file called test_capitalize.py, and inside it we will write a function called capital_case which should take a string as its argument and should return a capitalized version of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers/capitalize.py\n",
    "def capital_case(x):\n",
    "    return x.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a simple test. **Pytest expects our tests to be located in files whose names begin with** `test_` or end with `_test.py.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also write a test, test_capital_case to ensure that the function does what it says. We’ll **prefix our test function names with `test_`**, since this is what **pytest expects our test functions to be named**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_capitalize.py\n",
    "def test_capital_case():\n",
    "    assert capital_case('semaphore') == 'Semaphore'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediately noticeable thing is that **pytest uses a plain assert statement**, which is much easier to remember and use compared to the numerous assertSomething functions found in unittest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the test, execute the pytest command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that our first test passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest presents the test results differently than unittest. The report shows:\n",
    "- The system state, including which versions of Python, pytest, and any plugins you have installed\n",
    "- The rootdir, or the directory to search under for configuration and tests\n",
    "- The number of tests the runner discovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output then indicates the status of each test using a syntax similar to unittest:\n",
    "- A `dot (.)` means that the test passed.\n",
    "- An `F` means that the test has failed.\n",
    "- An `E` means that the test raised an unexpected exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **learning curve for pytest is shallower than it is for unittest** because you don’t need to learn new constructs for most tests. Also, the use of assert, which you may have used before in your implementation code, makes your tests more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A keen reader will notice that our function could lead to a bug. It does not check the type of the argument to ensure that it is a string. Therefore, if we passed in a number as the argument to the function, **it would raise an exception**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our capital_case function, we should **check that the argument passed is a string** or a string subclass before calling the capitalize function. If it is not, we should raise a TypeError with a custom error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to handle this case in our function by raising a custom exception with a friendly error message to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers/capitalize.py\n",
    "def capital_case(x):\n",
    "    if not isinstance(x, str):\n",
    "        raise TypeError('Please provide a string argument')\n",
    "    return x.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_capitalize.py\n",
    "import pytest\n",
    "from helpers.capitalize import capital_case\n",
    "\n",
    "\n",
    "def test_capital_case():\n",
    "    assert capital_case(\"semaphore\") == \"Semaphore\"\n",
    "\n",
    "\n",
    "def test_raises_exception_on_non_string_arguments():\n",
    "    with pytest.raises(TypeError):\n",
    "        capital_case(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major addition here is the `pytest.raises` helper, which **asserts that our function should raise a TypeError** in case the argument passed is not a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monkeypatching **is dynamically changing a piece of software (such as a module, object, method, or function) at runtime**.  Monkeypatching is often used for bug fixes or prototyping software, especially when using external APIs or libraries.  Pytest uses this feature to **allow you to test out interfaces that you don’t want to actually execute**.  For example, you can create a monkeypatched version of the requests module that doesn’t do the actual HTTP transactions during testing, but just returns fixed data that you set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocking is very similar to monkeypatching in the context of testing.  However, I always think of mocking only in terms of testing, whereas monkeypatching has a broader scope beyond just testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated tests should be fast, isolated/independent, and deterministic/repeatable. Thus, if you need to test code that makes an external HTTP request to a third-party API, you should really mock the request. Why? If you don't, then that specific test will be-\n",
    "1. slow since it's making an HTTP request over the network\n",
    "2. dependent on the third-party service and the speed of the network itself\n",
    "3. non-deterministic since the test could yield a different result based on the response from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's also a good idea to mock other long running operations, like database queries and async tasks, since automated tests are generally run frequently, on every commit pushed to source control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocking is the practice of replacing real objects with mocked objects, which mimic their behavior, at runtime. So, instead of a sending a real HTTP request over the network, we just return an expected response when the mocked method is called.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_my_ip():\n",
    "    response = requests.get(\n",
    "        'http://ipinfo.io/json'\n",
    "    )\n",
    "    return response.json()['ip']\n",
    "\n",
    "\n",
    "def test_get_my_ip(monkeypatch):\n",
    "    my_ip = '123.123.123.123'\n",
    "\n",
    "    class MockResponse:\n",
    "\n",
    "        def __init__(self, json_body):\n",
    "            self.json_body = json_body\n",
    "\n",
    "        def json(self):\n",
    "            return self.json_body\n",
    "\n",
    "    monkeypatch.setattr(\n",
    "        requests,\n",
    "        'get',\n",
    "        lambda *args, **kwargs: MockResponse({'ip': my_ip})\n",
    "    )\n",
    "\n",
    "    assert get_my_ip() == my_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening here?\n",
    "\n",
    "We used pytest's `monkeypatch` fixture to replace all calls to the `get` method from the `requests` module with the lambda callback that always returns an instance of `MockedResponse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We used an object because requests returns a Response object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest -v tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monkeypatching with pytest (Example #1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example illustrates how to use monkeypatching with pytest involves changing the behavior of the getcwd() method (Get Current Working Directory) from the os module that is part of the Python standard library.\n",
    "\n",
    "Here’s the source code to be tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers/example_1.py\n",
    "import os\n",
    "\n",
    "def example1():\n",
    "    \"\"\"\n",
    "    Retrieve the current directory\n",
    "\n",
    "    Returns:\n",
    "        Current directory\n",
    "    \"\"\"\n",
    "    current_path = os.getcwd()\n",
    "    return current_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Current Directory: {example1()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing this function, it would be desirable to be able to specify what ‘os.getcwd()’ returns instead of actually calling this function from the Python standard library.  By specifying what ‘os.getcwd()’ returns, you can write predictable tests and you can exercise different aspects of your code by returning off-nominal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the integration test that uses monkeypatching to specify a return value for ‘os.getcwd()’ in pytest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests/test_example_1.py\n",
    "import os\n",
    "from helpers.example_1 import example1\n",
    "\n",
    "\"\"\"\n",
    "This file (test_example1.py) contains the unit tests for the example1.py file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def test_get_current_directory(monkeypatch):\n",
    "    \"\"\"\n",
    "    GIVEN a monkeypatched version of os.getcwd()\n",
    "    WHEN example1() is called\n",
    "    THEN check the current directory returned\n",
    "    \"\"\"\n",
    "\n",
    "    def mock_getcwd():\n",
    "        return \"/data/user/directory123\"\n",
    "\n",
    "    monkeypatch.setattr(os, \"getcwd\", mock_getcwd)\n",
    "    assert example1() == \"/data/user/directory123\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test function utilizes the `monkeypatch` fixture that is part of pytest, which means that the `monkeypatch` fixture is passed into the function as an argument.\n",
    "\n",
    "The test function starts by **creating a mock version of the getcwd()** function (the ‘mock_getcwd()’ function) which returns a specified value.  This mock function **is then set to be called when ‘os.getcwd()’ is called** by using `monkeypatch.setattr()`.  What’s really nice about how pytest does monkeypatching is that this change to ‘os.getcwd()’ is only applicable within the ‘test_get_current_directory()’ function.\n",
    "\n",
    "Finally, the test function does the actual check (ie. the assert call) to check that the value returned from ‘example1()’ matches the specified value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monkeypatching with pytest (Example #2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example illustrates how to use monkeypatching with pytest when working with an external module, which happens to be the ‘requests‘ module in this case.  The ‘requests’ module is an amazing python module that allows for easily working with HTTP requests.\n",
    "\n",
    "Here’s the source code to be tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers/example_2.py\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"http://httpbin.org/\"\n",
    "\n",
    "\n",
    "def example2():\n",
    "    \"\"\"\n",
    "    Call GET for http://httpbin.org/get\n",
    "\n",
    "    Returns:\n",
    "        Status Code of the HTTP Response\n",
    "        URL in the Text of the HTTP Response\n",
    "    \"\"\"\n",
    "    r = requests.get(BASE_URL + \"get\")\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        response_data = r.json()\n",
    "        return r.status_code, response_data[\"url\"]\n",
    "    else:\n",
    "        return r.status_code, \"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    status_code, url = example2()\n",
    "    print(f\"HTTP Response... Status Code: {status_code}, URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs a GET to ‘http://httpbin.org/get’ and then checks that response. As an aside, http://httpbin.org is a great resource for testing API calls and it’s from the same author (Kenneth Reitz) that wrote the ‘requests’ module.\n",
    "\n",
    "In order to test out this function, it would be desirable to be able to test the GET response being both successful and failing.  You can do this with monkeypatching in pytest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the test function that tests the successful GET call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests/test_example2.py\n",
    "import requests\n",
    "from helpers.example_2 import example2\n",
    "\n",
    "\"\"\"\n",
    "This file (test_example2.py) contains the unit tests for the example2.py file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def test_get_response_success(monkeypatch):\n",
    "    \"\"\"\n",
    "    GIVEN a monkeypatched version of requests.get()\n",
    "    WHEN the HTTP response is set to successful\n",
    "    THEN check the HTTP response\n",
    "    \"\"\"\n",
    "\n",
    "    class MockResponse(object):\n",
    "        def __init__(self):\n",
    "            self.status_code = 200\n",
    "            self.url = \"http://httpbin.org/get\"\n",
    "            self.headers = {\"blaa\": \"1234\"}\n",
    "\n",
    "        def json(self):\n",
    "            return {\"account\": \"5678\", \"url\": \"http://www.testurl.com\"}\n",
    "\n",
    "    def mock_get(url):\n",
    "        return MockResponse()\n",
    "\n",
    "    monkeypatch.setattr(requests, \"get\", mock_get)\n",
    "    assert example2() == (200, \"http://www.testurl.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the first example, this test function utilizes the ‘monkeypatch’ fixture that is part of pytest, which means that the ‘monkeypatch’ fixture is passed into the function as an argument.\n",
    "\n",
    "The test function starts by creating a new class (‘MockResponse’) that specifies fixed values to be returned from an HTTP response.  An instance of this class is then returned by the ‘mock_get()’ function.\n",
    "\n",
    "This mock function (‘mock_get()’) is then set to be called when ‘requests.get()’ is called by using ‘monkeypatch.setattr()’.\n",
    "\n",
    "Finally, the actual check (ie. the assert call) is performed to check that the returned values from ‘example2()’ are the expected values.¸m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A failed HTTP GET response can be tested in a similar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_response_failure(monkeypatch):\n",
    "    \"\"\"\n",
    "    GIVEN a monkeypatched version of requests.get()\n",
    "    WHEN the HTTP response is set to failed\n",
    "    THEN check the HTTP response\n",
    "    \"\"\"\n",
    "\n",
    "    class MockResponse(object):\n",
    "        def __init__(self):\n",
    "            self.status_code = 404\n",
    "            self.url = \"http://httpbin.org/get\"\n",
    "            self.headers = {\"blaa\": \"1234\"}\n",
    "\n",
    "        def json(self):\n",
    "            return {\"error\": \"bad\"}\n",
    "\n",
    "    def mock_get(url):\n",
    "        return MockResponse()\n",
    "\n",
    "    monkeypatch.setattr(requests, \"get\", mock_get)\n",
    "    assert example2() == (404, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test function is similar to the success case, except it is now returning a status code of 404 (Internal Server Error) to test that the negative path in ‘example2()’ works as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest -v tests/test_example2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important aspect of tests is code coverage. It's a metric which tells you the ratio between the number of lines executed during test runs and the total number of all lines in your code base. We can use the pytest-cov plugin for this, which integrates Coverage.py with pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip install pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, to run tests with coverage reporting, add the `--cov` option like so:\n",
    "    \n",
    "    python -m pytest -v --cov=tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every file in the project's path you get:\n",
    "- Stmts - number of lines of code\n",
    "- Miss - number of lines that weren't executed by the tests\n",
    "- Cover - coverage percentage for the file\n",
    "\n",
    "At the bottom, there's a line with the totals for the whole project.\n",
    "\n",
    "Keep in mind that although it's encouraged to achieve a high coverage percentage, that doesn't mean your tests are good tests, testing each of the happy and exception paths of your code. For example, tests with assertions like assert sum(3, 2) == 5 can achieve high coverage percentage but your code is still practically untested since exception paths are not being covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures: Managing State and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pytest fixtures** are **a way of providing data**, **test doubles**, or **state setup to your tests**. Fixtures are functions that can return a wide range of values. \n",
    "\n",
    "**Each test that depends on a fixture must explicitly accept that fixture as an argument**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be writing a wallet application that enables its users to add or spend money in the wallet. It will be modeled as a class with two instance methods: spend_cash and add_cash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file called wallet.py, and we will add our Wallet implementation in it. The file should look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers/wallet.py\n",
    "\n",
    "class InsufficientAmount(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Wallet(object):\n",
    "    def __init__(self, initial_amount=0):\n",
    "        self.balance = initial_amount\n",
    "\n",
    "    def spend_cash(self, amount):\n",
    "        if self.balance < amount:\n",
    "            raise InsufficientAmount(f\"Not enough available to spend {amount}\")\n",
    "        self.balance -= amount\n",
    "\n",
    "    def add_cash(self, amount):\n",
    "        self.balance += amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we define our custom exception, InsufficientAmount, which will be raised when we try to spend more money than we have in the wallet. The Wallet class then follows. The constructor accepts an initial amount, which defaults to 0 if not provided. The initial amount is then set as the balance.\n",
    "\n",
    "In the spend_cash method, we first check that we have a sufficient balance. If the balance is lower than the amount we intend to spend, we raise the InsufficientAmount exception with a friendly error message.\n",
    "\n",
    "The implementation of add_cash then follows, which simply adds the provided amount to the current wallet balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file called test_wallet.py in the working directory, and add the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_wallet.py\n",
    "\n",
    "import pytest\n",
    "from helpers.wallet import Wallet, InsufficientAmount\n",
    "\n",
    "\n",
    "def test_default_initial_amount():\n",
    "    wallet = Wallet()\n",
    "    assert wallet.balance == 0\n",
    "\n",
    "def test_setting_initial_amount():\n",
    "    wallet = Wallet(100)\n",
    "    assert wallet.balance == 100\n",
    "\n",
    "def test_wallet_add_cash():\n",
    "    wallet = Wallet(10)\n",
    "    wallet.add_cash(90)\n",
    "    assert wallet.balance == 100\n",
    "\n",
    "def test_wallet_spend_cash():\n",
    "    wallet = Wallet(20)\n",
    "    wallet.spend_cash(10)\n",
    "    assert wallet.balance == 10\n",
    "\n",
    "def test_wallet_spend_cash_raises_exception_on_insufficient_amount():\n",
    "    wallet = Wallet()\n",
    "    with pytest.raises(InsufficientAmount):\n",
    "        wallet.spend_cash(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, we import the Wallet class and the InsufficientAmount exception that we expect to raise when the user tries to spend more cash than they have in their wallet.\n",
    "\n",
    "When we initialize the Wallet class, we expect it to have a default balance of 0. However, when we initialize the class with a value, that value should be set as the wallet’s initial balance.\n",
    "\n",
    "Moving on to the methods we plan to implement, we test that the add_cash method correctly increments the balance with the added amount. On the other hand, we are also ensuring that the spend_cash method reduces the balance by the spent amount and that we can’t spend more cash than we have in the wallet. If we try to do so, an InsufficientAmount exception should be raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this in place, we can rerun our tests, and they should be passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest -q tests/test_wallet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refactoring our Tests with Fixtures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed some **repetition in the way we initialized the class in each test**. This is where pytest fixtures come in. They **help us set up some helper code that should run before any tests are executed**, and are perfect for **setting-up resources that are needed by the tests**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixture functions** are created by marking them with the `@pytest.fixture` decorator. Test **functions that require fixtures should accept them as argument**s. For example, for a test to receive a fixture called wallet, it should have an argument with the fixture name, i.e. wallet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will refactor our previous tests to use test fixtures where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_wallet.py\n",
    "\n",
    "import pytest\n",
    "from helpers.wallet import Wallet, InsufficientAmount\n",
    "\n",
    "@pytest.fixture\n",
    "def empty_wallet():\n",
    "    '''Returns a Wallet instance with a zero balance'''\n",
    "    return Wallet()\n",
    "\n",
    "@pytest.fixture\n",
    "def wallet():\n",
    "    '''Returns a Wallet instance with a balance of 20'''\n",
    "    return Wallet(20)\n",
    "\n",
    "def test_default_initial_amount(empty_wallet):\n",
    "    assert empty_wallet.balance == 0\n",
    "\n",
    "def test_setting_initial_amount(wallet):\n",
    "    assert wallet.balance == 20\n",
    "\n",
    "def test_wallet_add_cash(wallet):\n",
    "    wallet.add_cash(80)\n",
    "    assert wallet.balance == 100\n",
    "\n",
    "def test_wallet_spend_cash(wallet):\n",
    "    wallet.spend_cash(10)\n",
    "    assert wallet.balance == 10\n",
    "\n",
    "def test_wallet_spend_cash_raises_exception_on_insufficient_amount(empty_wallet):\n",
    "    with pytest.raises(InsufficientAmount):\n",
    "        empty_wallet.spend_cash(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our refactored tests, we can see that we have reduced the amount of boilerplate code by making use of fixtures.\n",
    "\n",
    "We define two fixture functions,wallet and empty_wallet, which will be responsible for initializing the Wallet class in tests where it is needed, with different values.\n",
    "\n",
    "For the first test function, we make use of the empty_wallet fixture, which provided a wallet instance with a balance of 0 to the test.\n",
    "\n",
    "The next three tests receive a wallet instance initialized with a balance of 20. Finally, the last test receives the empty_wallet fixture. The tests can then make use of the fixture as if it was created inside the test function, as in the tests we had before.\n",
    "\n",
    "Rerun the tests to confirm that everything works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest -q tests/test_wallet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing **fixtures helps us de-duplicate our code**. If you notice a case where a piece of code is used repeatedly in a number of tests, that might be a good candidate to use as a fixture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some pointers on using test fixtures:\n",
    "- **Each test is provided with a newly-initialized** Wallet instance, and not one that has been used in another test.\n",
    "- It is a good practice to **add docstrings for your fixtures**. To see all the available fixtures, run the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest --fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lists out some inbuilt pytest fixtures, as well as our custom fixtures. The docstrings will appear as the descriptions of the fixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Avoid Fixtures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixtures are great for extracting data or objects that you use across multiple tests. They **aren’t always as good for tests that require slight variations in the data**. Littering your test suite with fixtures is no better than littering it with plain data or objects. It might even be worse because of the added layer of indirection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization: Combining Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You saw earlier in this tutorial how pytest fixtures can be used to reduce code duplication by extracting common dependencies. **Fixtures aren’t quite as useful when you have several tests with slightly different inputs and expected outputs**. In these cases, you can **parametrize a single test definition**, and pytest will create variants of the test for you with the parameters you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you’ve written a function to tell if a string is a palindrome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers/palindrome.py\n",
    "def is_palindrome(s):\n",
    "    return s == s[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial set of tests could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_palindrome.py\n",
    "from helpers.palindrome import is_palindrome\n",
    "\n",
    "\n",
    "def test_is_palindrome_empty_string():\n",
    "    assert is_palindrome(\"\")\n",
    "\n",
    "\n",
    "def test_is_palindrome_single_character():\n",
    "    assert is_palindrome(\"a\")\n",
    "\n",
    "\n",
    "def test_is_palindrome_mixed_casing():\n",
    "    assert is_palindrome(\"Bob\")\n",
    "\n",
    "\n",
    "def test_is_palindrome_with_spaces():\n",
    "    assert is_palindrome(\"Never odd or even\")\n",
    "\n",
    "\n",
    "def test_is_palindrome_with_punctuation():\n",
    "    assert is_palindrome(\"Do geese see God?\")\n",
    "\n",
    "\n",
    "def test_is_palindrome_not_palindrome():\n",
    "    assert not is_palindrome(\"abc\")\n",
    "\n",
    "\n",
    "def test_is_palindrome_not_quite():\n",
    "    assert not is_palindrome(\"abab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest tests/test_palindrome.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidmo, da ni prestalo vseh testov, zato popravimo funkcijo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers/palindrome.py\n",
    "def is_palindrome(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace(\" \", \"\").strip()\n",
    "    s = \"\".join(ch for ch in s if ch.isalnum())\n",
    "    return s == s[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these tests except the last two have the same shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def test_is_palindrome_<in some situation>():\n",
    "        assert is_palindrome(\"<some string>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `@pytest.mark.parametrize()` to fill in this shape with different values, reducing your test code significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_palindrome.py\n",
    "import pytest\n",
    "from helpers.palindrome import is_palindrome\n",
    "\n",
    "@pytest.mark.parametrize(\"palindrome\", [\n",
    "    \"\",\n",
    "    \"a\",\n",
    "    \"Bob\",\n",
    "    \"Never odd or even\",\n",
    "    \"Do geese see God?\",\n",
    "])\n",
    "def test_is_palindrome(palindrome):\n",
    "    assert is_palindrome(palindrome)\n",
    "\n",
    "    \n",
    "@pytest.mark.parametrize(\"non_palindrome\", [\n",
    "    \"abc\",\n",
    "    \"abab\",\n",
    "])\n",
    "def test_is_palindrome_not_palindrome(non_palindrome):\n",
    "    assert not is_palindrome(non_palindrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest tests/test_palindrome.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first argument to parametrize()** is **a comma-delimited string of parameter names**. The **second argument** is **a list of either tuples or single values that represent the parameter value(s)**. You could take your parametrization a step further to combine all your tests into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_palindrome.py\n",
    "import pytest\n",
    "from helpers.palindrome import is_palindrome\n",
    "\n",
    "@pytest.mark.parametrize(\"maybe_palindrome, expected_result\", [\n",
    "    (\"\", True),\n",
    "    (\"a\", True),\n",
    "    (\"Bob\", True),\n",
    "    (\"Never odd or even\", True),\n",
    "    (\"Do geese see God?\", True),\n",
    "    (\"abc\", False),\n",
    "    (\"abab\", False),\n",
    "])\n",
    "def test_is_palindrome(maybe_palindrome, expected_result):\n",
    "    assert is_palindrome(maybe_palindrome) == expected_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this shortened your code, it’s important to note that in this case, it didn’t do much to clarify your test code. Use parametrization to **separate the test data from the test behavior so that it’s clear what the test is testing**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest tests/test_palindrome.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vaja: wallet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our tests less repetitive, we can go further and combine test fixtures and parametrize test functions. To demonstrate this, let’s replace the wallet initialization code with a test fixture as we did before.\n",
    "\n",
    "The end result will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/test_wallet.py\n",
    "\n",
    "# Dodamo:\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"earned,spent,expected\",\n",
    "    [\n",
    "        (30, 10, 20),\n",
    "        (20, 2, 18),\n",
    "    ],\n",
    ")\n",
    "def test_transactions(empty_wallet, earned, spent, expected):\n",
    "    empty_wallet.add_cash(earned)\n",
    "    empty_wallet.spend_cash(spent)\n",
    "    assert empty_wallet.balance == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new fixture called my_wallet that is exactly the same as the empty_wallet fixture we used before. It returns a wallet instance with a balance of 0. To use both the fixture and the parametrized functions in the test, we include the fixture as the first argument and the parameters as the rest of the arguments.\n",
    "\n",
    "The transactions will then be performed on the wallet instance provided by the fixture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durations Reports: Fighting Slow Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time you switch contexts from implementation code to test code, you incur some overhead. If your tests are slow to begin with, then overhead can cause friction and frustration.\n",
    "\n",
    "You read earlier about using marks to filter out slow tests when you run your suite. If you want to improve the speed of your tests, then it’s useful to know which tests might offer the biggest improvements. **pytest can automatically record test durations for you and report the top offenders**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `--durations` option to the pytest command to include a duration report in your test results. `--durations` expects an integer value `n` and will **report the slowest n number of tests**. The output will follow your test results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest tests/test_wallet.py --durations=3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each test that shows up in the durations report is a good candidate to speed up because it takes an above-average amount of the total testing time.\n",
    "\n",
    "Be aware that some tests may have an invisible setup overhead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conftest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `conftest.py` provides configuration for the file tree pytest finds it in. You can **use any fixtures that are defined in a particular conftest.py throughout the file’s parent directory and in any subdirectories**. This is a great place to put your most widely used fixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Naredimo datoteko `conftest.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xfail / Skip tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be some situations where we don't want to execute a test, or a test case is not relevant for a particular time. In those situations, we have the option to xfail the test or skip the tests\n",
    "\n",
    "The **xfailed test** will **be executed**, but it **will not be counted as part failed or passed tests**. There will be no traceback displayed if that test fails. We can xfail tests using `@pytest.mark.xfail`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodamo v test_wallet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.xfail\n",
    "def test_add():\n",
    "    assert 15+13 == 100,\"failed\"\n",
    "    \n",
    "@pytest.mark.xfail\n",
    "def test_add_2():\n",
    "    assert 15+13 == 28,\"failed\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest -v tests/test_wallet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping a test means that the test will not be executed. We can skip tests using `@pytest.mark.skip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodamo v test_wallet.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.skip\n",
    "def test_add_3():\n",
    "    assert 100+200 == 400,\"failed\"\n",
    "    \n",
    "@pytest.mark.skip\n",
    "def test_add_4():\n",
    "    assert 100+200 == 300,\"failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results in XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create test results in XML format which we can feed to Continuous Integration servers for further processing and so. This can be done by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest tests/test_wallet.py -v --junitxml=\"result.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutation Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://mutmut.readthedocs.io/en/latest/\n",
    "- https://hackernoon.com/mutmut-a-python-mutation-testing-system-9b9639356c78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutation Testing helps ensure that your tests actually cover the full behavior of your code. Put another way, it analyzes the effectiveness or robustness of your test suite. During mutation testing, a tool iterates through each line of your source code, making small changes (called mutations) that should break your code. After each mutation, the tool runs your unit tests and checks whether your tests fail or not. If your tests still pass, then your code didn't survive the mutation test.\n",
    "\n",
    "For example, say you have the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if x > y:\n",
    "        z = 50\n",
    "    else:\n",
    "        z = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutation tool may change the operator from > to >= like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if x >= y:\n",
    "        z = 50\n",
    "    else:\n",
    "        z = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mutmut is a mutation testing library for Python. Let's look at it in action.\n",
    "\n",
    "    pip install mutmut\n",
    "\n",
    "Say you have the following Loan class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class LoanStatus(str, Enum):\n",
    "    PENDING = \"PENDING\"\n",
    "    ACCEPTED = \"ACCEPTED\"\n",
    "    REJECTED = \"REJECTED\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Loan:\n",
    "    amount: float\n",
    "    status: LoanStatus = LoanStatus.PENDING\n",
    "\n",
    "    def reject(self):\n",
    "        self.status = LoanStatus.REJECTED\n",
    "\n",
    "    def rejected(self):\n",
    "        return self.status == LoanStatus.REJECTED\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say you want to automatically reject loan requests that are greater than 250,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reject_loan.py\n",
    "def reject_loan(loan):\n",
    "    if loan.amount > 250_000:\n",
    "        loan.reject()\n",
    "\n",
    "    return loan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then wrote the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loan.py\n",
    "\n",
    "from app.loan import Loan\n",
    "from app.reject_loan import reject_loan\n",
    "\n",
    "\n",
    "def test_reject_loan():\n",
    "    loan = Loan(amount=100_000)\n",
    "\n",
    "    assert not reject_loan(loan).rejected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest tests/test_loan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run mutation testing with mutmut, you'll see that you have two surviving mutants:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     mutmut run --paths-to-mutate app/reject_loan.py --tests-dir=tests/test_loan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the surviving mutants by ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    mutmut show 1\n",
    "    mutmut show 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve your test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.loan import Loan\n",
    "from app.reject_loan import reject_loan\n",
    "\n",
    "\n",
    "def test_reject_loan():\n",
    "    loan = Loan(amount=100_000)\n",
    "    assert not reject_loan(loan).rejected()\n",
    "\n",
    "    loan = Loan(amount=250_001)\n",
    "    assert reject_loan(loan).rejected()\n",
    "\n",
    "    loan = Loan(amount=250_000)\n",
    "    assert not reject_loan(loan).rejected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run mutation tests again, you'll see that no mutations survived:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    mutmut run --paths-to-mutate app/reject_loan.py --tests-dir=tests/test_loan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your test is much more robust. Any unintentional change inside of reject_loan.py will produce a failing test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any other approach, mutation testing comes with a tradeoff. While it improves your test suite's ability to catch bugs, it comes at the cost of speed since you have to run your entire test suite hundreds of times. It also forces you to really test everything. This can help uncover exceptions paths, but you will have many more test cases to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "\n",
    "This section describes how to work with mutmut to enhance your test suite.\n",
    "1. Run mutmut with `mutmut run`. A full run is preferred but if you’re just getting started you can exit in the middle and start working with what you have found so far.\n",
    "2. Show the mutants with `mutmut results`\n",
    "3. Apply a surviving mutant to disk running `mutmut apply 3` (replace 3 with the relevant mutant ID from mutmut results)\n",
    "4. Write a new test that fails\n",
    "5. Revert the mutant on disk\n",
    "6. Rerun the new test to see that it now passes\n",
    "7. Go back to point 2.\n",
    "\n",
    "Mutmut keeps a result cache in `.mutmut-cache` so if you want to make sure you run a full mutmut run just delete this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hypothesis.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip install hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis is a library for conducting property-based testing in Python. Rather than having to write different test cases for every argument you want to test, property-based testing generates a wide-range of random test data that's dependent on previous tests runs. This helps increase the robustness of your test suite while decreasing test redundancy. In short, your test code will be cleaner, more DRY, and overall more efficient while still covering a wide range of test data.\n",
    "\n",
    "For example, say you have to write tests for the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment(num: int) -> int:\n",
    "    return num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could write the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    'number, result',\n",
    "    [\n",
    "        (-2, -1),\n",
    "        (0, 1),\n",
    "        (3, 4),\n",
    "        (101234, 101235),\n",
    "    ]\n",
    ")\n",
    "def test_increment(number, result):\n",
    "    assert increment(number) == result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing wrong with this approach. Your code is tested and code coverage is high (100% to be exact). That said, how well is your code tested based on the range of possible inputs? There are quite a lot of integers that could be tested, but only four of them are being used in the test. In some situations this is enough. In other situations four cases isn't enough -- i.e., **non-deterministic machine learning code**. What about really small or large numbers? Or say your function takes a list of integers rather than a single integer -- What if the list was empty or it contained one element, hundreds of elements, or thousands of elements? In **some situations we simply cannot provide (let alone even think of) all the possible cases**. That's where **property-base testing comes into play**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Machine learning algorithms are a great use case for property-based testing since it's difficult to produce (and maintain) test examples for complex sets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks like Hypothesis provide recipes (Hypthesis calls them Strategies) for generating random test data. Hypothesis also stores the results of previous test runs and uses them to create new cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Strategies are algorithms that generate pseudo-random data based on the shape of the input data. It's pseudo-random because the generated data is based on data from previous tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same test using property-based testing via Hypothesis looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis import given\n",
    "import hypothesis.strategies as st\n",
    "\n",
    "\n",
    "@given(st.integers())\n",
    "def test_add_one(num):\n",
    "    assert increment(num) == num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`st.integers()` is a Hypothesis Strategy that generates random integers for testing while the `@given` decorator is used to parameterize the test function. So when the test function is called, the generated integers, from the Strategy, will be passed into the test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
