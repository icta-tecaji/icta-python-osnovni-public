{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viri:\n",
    "- [Increase Test Coverage](https://devguide.python.org/coverage/#increase-test-coverage)\n",
    "- [Coverage.py](https://coverage.readthedocs.io/en/latest/) \n",
    "- [Testing and Code coverage with Python](https://developer.ibm.com/recipes/tutorials/testing-and-code-coverage-with-python/)\n",
    "- [Code Coverage Metrics](https://www.sealights.io/code-quality/code-coverage-metrics/)\n",
    "- [An Intro to coverage.py](http://www.blog.pythonlibrary.org/2016/07/20/an-intro-to-coverage-py/)\n",
    "- [Welcome to pytest-cov’s documentation!](https://pytest-cov.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Code Coverage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measures the number of lines of source code executed during a given test suite for a program. Tools that measure code coverage normally express this metric as a percentage.\n",
    "\n",
    "Many use the terms “code coverage” and “test coverage” interchangeably. However, they are two different things. Test coverage is a measurement of the degree to which a test or testing suite actually checks the full extent of a program’s functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Measuring Code Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>Software with high results is less likely to contain undetected bugs stemming from coding errors, non-adherence to good coding practices, or overly complex code.</li><li>High percentages can imply code that is more maintainable and readable. However, it is possible to achieve high percentages (“for the sake of coverage”) without improving maintainability.</li><li>It provides a measurable value to stakeholders on software quality. Such stakeholders are often not involved in day-to-day software development, and they need a measurable standard to gauge software quality.</li><li>Levels between 70 and 90 percent suggest reliable software, according to a <a href=\"https://gupea.ub.gu.se/bitstream/2077/38588/1/gupea_2077_38588_1.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">review of academic studies</a> that examined the correlation between software quality and code coverage.</li><li>The larger a project team is, the more room for ambiguity when it comes to defining the well-tested code. This measurement can act as an approximation metric that consolidates the team’s definition of well-tested code, and leads to consistent testing practices.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many experts believe that while this metric is valuable, it should not be used as a target for testing or development teams. Targeting a specific percentage or range does not necessarily increase software quality and can lead to problematic testing practices, something we’ll discuss further in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different ways to measure code coverage—it is more like a family of dimensions rather than a single formula metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Coverage (Function Coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method or function coverage measures code by counting the number of functions called by a test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statement coverage measures the percentage of code statements executed during a test suite. Statements are instructions in the code expressing some action that the program should carry out. Therefore, statement coverage gives an accurate measure of the quantity of written code that tests actually execute.\n",
    "\n",
    "However, statement coverage is only useful as a measure of physical code — it says nothing about the quality of tests used to execute the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Branch coverage measures whether a test suite executes the branches from decision points written into the code. Such decision points arise from if and case statements, with two possible outcomes: true and false. Consider the following code:\n",
    "\n",
    "    Read X\n",
    "    Read Y\n",
    "    IF “X > Y”\n",
    "    PRINT “X is greater than Y”\n",
    "    ENDIF\n",
    "\n",
    "There are two outcomes for this if statement: true and false. Branch coverage needs to consider what happens both when X is larger than Y and when Y is larger than X, the latter of which is the FALSE condition for this statement. Two tests can ensure full branch coverage in this code:\n",
    "\n",
    "    TEST CASE 1: X=10, Y=5\n",
    "    TEST CASE 2: X=2, Y=10\n",
    "\n",
    "The aim of measuring branch coverage is to check whether tests execute all reachable branch points (true and false) across a comprehensive set of inputs. This is a good measure of logic coverage, which relates to the quantity of possible code paths tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Coverage\n",
    "\n",
    "Condition coverage measures whether tests execute statements using each of the Boolean expressions contained in the code. For example, consider the basic if statement below:\n",
    "\n",
    "    IF (“X && Y”)\n",
    "\n",
    "A valid condition coverage for this code needs to test what happens when X and Y take on their respective Boolean values of true and false. The tests required are:\n",
    "\n",
    "    TEST 1: X=TRUE, Y=FALSE\n",
    "    TEST 2: X=FALSE, Y=TRUE\n",
    "\n",
    "Condition coverage is another way of ensuring tests hit all possible code paths.\n",
    "\n",
    "### Multiple Condition Decision Coverage (MC/DC)\n",
    "\n",
    "This type of coverage requires extensive tests to ensure that tests execute all combinations of conditions inside each decision statement. For example: testing the results of every combination of the Boolean results for X, Y and Z. This type of coverage metric is used when testing safety-critical applications, such as software used inside aircraft.\n",
    "\n",
    "### Parameter Value Coverage\n",
    "\n",
    "Parameter value coverage aims to cover all possible parameter values for each program procedure/method that uses parameters. For example, a string can take on several values. Parameter value coverage ensures the tests execute the code using all possible string values. Neglecting certain parameter values can lead to software defects.\n",
    "\n",
    "### Cyclomatic Complexity\n",
    "\n",
    "Cyclomatic complexity measures the total number of linearly dependent paths in a program’s source code. For example, source code containing no conditional statements or decision points has a cyclomatic complexity of 1.\n",
    "\n",
    "Cyclomatic complexity is useful in planning test cases to determine coverage for particular code modules. For example, testing teams can use the cyclomatic complexity value as an upper bound for the number of required test cases to achieve full branch coverage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coverage.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular third-party coverage tools is coverage.py which provides very nice HTML output along with advanced features such as branch coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage.py is a tool for measuring code coverage of Python programs. It monitors your program, noting which parts of the code have been executed, then analyzes the source to identify code that could have been executed but was not.\n",
    "\n",
    "Coverage measurement is typically used to gauge the effectiveness of tests. It can show which parts of your code are being exercised by tests, and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install coverage.py in the usual ways. The simplest way is with pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ pip install coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage.py includes a C extension for speed. It is strongly recommended to use this extension: it is much faster, and is needed to support a number of coverage.py features. Most of the time, the C extension will be installed without any special action on your part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can determine if you are using the extension by looking at the output of coverage --version:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ coverage --version\n",
    "    Coverage.py, version 5.0.4 with C extension\n",
    "    Documentation at https://coverage.readthedocs.io/en/coverage-5.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line will either say “with C extension,” or “without C extension.”\n",
    "\n",
    "If you are missing the extension, first make sure you have the latest version of pip in use when installing coverage.\n",
    "\n",
    "If you are installing on Linux, you may need to install the python-dev and gcc support files before installing coverage via pip. The exact commands depend on which package manager you use, which Python version you are using, and the names of the packages for your distribution. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ sudo apt-get install python-dev gcc\n",
    "    $ sudo yum install python-devel gcc\n",
    "\n",
    "    $ sudo apt-get install python3-dev gcc\n",
    "    $ sudo yum install python3-devel gcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few features of coverage.py aren’t supported without the C extension, such as concurrency and plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage.py command line usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you install coverage.py, a command-line script simply called coverage is placed in your Python scripts directory. To help with multi-version installs, it will also create either a coverage2 or coverage3 alias, and a coverage-X.Y alias, depending on the version of Python you’re using. For example, when installing on Python 3.7, you will be able to use coverage, coverage3, or coverage-3.7 on the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage.py has a number of commands which determine the action performed:\n",
    "    \n",
    "<ul class=\"simple\">\n",
    "<li><p><strong>run</strong> – Run a Python program and collect execution data.</p></li>\n",
    "<li><p><strong>report</strong> – Report coverage results.</p></li>\n",
    "<li><p><strong>html</strong> – Produce annotated HTML listings with coverage results.</p></li>\n",
    "<li><p><strong>json</strong> – Produce a JSON report with coverage results.</p></li>\n",
    "<li><p><strong>xml</strong> – Produce an XML report with coverage results.</p></li>\n",
    "<li><p><strong>annotate</strong> – Annotate source files with coverage results.</p></li>\n",
    "<li><p><strong>erase</strong> – Erase previously collected coverage data.</p></li>\n",
    "<li><p><strong>combine</strong> – Combine together a number of data files.</p></li>\n",
    "<li><p><strong>debug</strong> – Get diagnostic information.</p></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Help is available with the <strong>help</strong> command, or with the <code class=\"docutils literal notranslate\"><span class=\"pre\">--help</span></code> switch on\n",
    "any other command:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ coverage help\n",
    "$ coverage help run\n",
    "$ coverage run --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any command can use a configuration file by specifying it with the --rcfile=FILE command-line switch. Any option you can set on the command line can also be set in the configuration file. This can be a better way to control coverage.py since the configuration file can be checked into source control, and can provide options that other invocation techniques (like test runner plugins) may not offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You collect execution data by running your Python program with the run command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ coverage run my_program.py arg1 arg2\n",
    "    blah blah ..your program's output.. blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your program runs just as if it had been invoked with the Python command line. Arguments after your file name are passed to your program as usual in sys.argv. Rather than providing a file name, you can use the -m switch and specify an importable module name instead, just as you can with the Python -m switch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ coverage run -m packagename.modulename arg1 arg2\n",
    "    blah blah ..your program's output.. blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In most cases, the program to use here is a test runner, not your program you are trying to measure. The test runner will run your tests and coverage will measure the coverage of your code along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you decide you want to try to improve branch coverage, simply add the --branch flag to your coverage run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage.py provides a few styles of reporting, with the report, html, annotate, json, and xml commands. They share a number of common options.\n",
    "\n",
    "The command-line arguments are module or file names to report on, if you’d like to report on a subset of the data collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest reporting is a textual summary produced with report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    coverage report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each module executed, the report shows the count of executable statements, the number of those statements missed, and the resulting coverage, expressed as a percentage.\n",
    "\n",
    "The -m flag also shows the line numbers of missing statements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    coverage report -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1) Pripravimo skripto app.py*\n",
    "\n",
    "Below is a small Python script which will perform basic mathematical operations. The method “process_input()” takes the two parameters (which are operands) and the operation name as a string value. The various operations supported here are addition, subtraction, multiplication and division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "def process_input(a, b, operation):\n",
    "    '''Perform operation on *a* and *b* depending on input provided.\n",
    "    :param a: integer value\n",
    "    :param b: integer value\n",
    "    '''\n",
    "    if operation == 'add':\n",
    "        return a + b\n",
    "    if operation == 'subtract':\n",
    "        return a - b\n",
    "    if operation == 'multiple':\n",
    "        return a * b\n",
    "    if operation == 'divide':\n",
    "        if b == 0:\n",
    "            return 'Invalid Input'\n",
    "        return a / b\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    '''Run as a script'''\n",
    "    print(process_input(int(sys.argv[1]), int(sys.argv[2]), sys.argv[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To  run this script, run the following on your shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    python app.py 10 5 add\n",
    "    python app.py 10 5 subtract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2) Write first test case test.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a test suite for the above script, use the small test file below. Here, the “setUp()” method is called before every test case runs. Then there is a test case which checks the addition of two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import unittest\n",
    "from app import process_input\n",
    "\n",
    "class TestApp(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.a = 10\n",
    "        self.b = 5\n",
    "\n",
    "    def test_0010_add(self):\n",
    "        result = process_input(self.a, self.b, 'add')\n",
    "        self.assertEqual(result, 15)\n",
    "\n",
    "\n",
    "def suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTests(\n",
    "        unittest.TestLoader().loadTestsFromTestCase(TestApp)\n",
    "    )\n",
    "    return suite\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.TextTestRunner(verbosity=2).run(suite())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3) Run the test case*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a test case is pretty straightforward. To do so, run the following command from shell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*4) Calculate the code coverage*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following to see coverage report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    coverage run test.py\n",
    "    \n",
    " When you do this, you will see the test’s output as if you ran it yourself. You will also find a new file in the directory that is called .coverage (note the period at the beginning). To get information out of this file, you will need to run the following command:\n",
    " \n",
    "    coverage report\n",
    "    \n",
    "ali\n",
    "\n",
    "    coverage report -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -m flag tells coverage.py that you want it to include the Missing column in the output. If you omit the -m, then you’ll only get the first four columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*5) Increase coverage by adding more tests* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code coverage here is 69% overall and 36% for app.py. This is not good. The code coverage can be increased by writing more test cases, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_0020_subtract(self):\n",
    "    result = process_input(self.a, self.b, 'subtract')\n",
    "    self.assertEqual(result, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the above method, calculate the code coverage again after adding above method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    coverage run test.py  \n",
    "    coverage report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code coverage has increased to 78% on adding another test case. It can be increased further to 100% in a similar fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However it will give us a good idea of basic coverage even if it can’t tell us if we’ve tested every possible argument permutation imaginable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*6) Understanding code coverage metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code coverage is basically a percentage of count of lines that the test cases traversed to the overall lines in the code. This would be called Path Coverage.\n",
    "\n",
    "In this program, we have multiple conditions for each mathematical operation, which means that testing would be done for multiple conditions inside code. This can be referred to as Condition Coverage.\n",
    "\n",
    "When the tests would be written for division, a boolean operation would be checked that would be Branch Coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a visual look at how much of the code is actually traversed, generate a HTML report.\n",
    "\n",
    "    coverage html\n",
    "\n",
    "This will create a folder at the current working directory with the name “htmlcov”. Inside this folder there would be a file named “app_py.html”. Open this file in any browser to have a visual overview of the coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage.py is a popular tool for measuring code coverage in Python-based applications. Now, since we're using Pytest, we'll integrate Coverage.py with Pytest using pytest-cov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install with pip:\n",
    "\n",
    "    pip install pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage:\n",
    "    \n",
    "    pytest --cov=myproj tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file is erased at the beginning of testing to ensure clean data for each test run. If you need to combine the coverage of several test runs you can use the --cov-append option to append this coverage data to coverage data from previous test runs.\n",
    "\n",
    "The data file is left at the end of testing so that it is possible to use normal coverage tools to examine it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can configure the coverage reports in a .coveragerc file. Add this file to the project root, and then add the following config to exclude the tests from the coverage results and enable branch coverage measurement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [run]\n",
    "    omit = project/test*\n",
    "    branch = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests with coverage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pytest \"project\" --cov=\"project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Just keep in mind that while code coverage is a good metric to look at, it does not measure the overall effectiveness of the test suite. In other words, having 100% coverage means that every line of code is being tested; it does not mean that the tests handle every scenario. In other words, just because you have 100% test coverage doesn’t mean you're testing the right things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to view an HTML version?\n",
    "\n",
    "    $  pytest \"project\" -p no:warnings --cov=\"project\" --cov-report html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
